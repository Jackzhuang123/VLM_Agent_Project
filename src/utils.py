"""
VLM-VLA Agent è®­ç»ƒå’Œæ¨ç†çš„å·¥å…·å‡½æ•°åº“

åŒ…å«ä»¥ä¸‹åŠŸèƒ½æ¨¡å—ï¼š
    1. éšæœºç§å­ï¼šè®¾ç½®å¯å¤ç°æ€§çš„ç§å­
    2. è®¾å¤‡ç®¡ç†ï¼šGPU/CPU è®¾å¤‡æ£€æµ‹å’Œé€‰æ‹©
    3. æ¨¡å‹ç»Ÿè®¡ï¼šå‚æ•°è®¡æ•°å’Œæ‘˜è¦æ‰“å°
    4. æ£€æŸ¥ç‚¹ç®¡ç†ï¼šåŠ è½½ã€ä¿å­˜å’Œæ¸…ç†æ£€æŸ¥ç‚¹
    5. å†…å­˜ç›‘æ§ï¼šGPU å†…å­˜ä½¿ç”¨æƒ…å†µè·Ÿè¸ª
    6. è·¯å¾„éªŒè¯ï¼šæ£€æŸ¥å¿…éœ€çš„æ–‡ä»¶å’Œç›®å½•
    7. æ—¥å¿—è®°å½•ï¼šè®¾ç½®å’Œç®¡ç†æ—¥å¿—è¾“å‡º
    8. æ€§èƒ½è®¡ç®—ï¼šç»Ÿè®¡æŒ‡æ ‡å¹³å‡å€¼è®¡ç®—

ä½¿ç”¨ç¤ºä¾‹ï¼š
    >>> from src.utils import set_seed, get_device, print_model_summary
    >>> set_seed(42)
    >>> device = get_device()
    >>> print_model_summary(model)
"""

import json
import os
import random
from pathlib import Path
from typing import Dict

import numpy as np
import torch


def set_seed(seed: int = 42):
    """
    è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯å¤ç°æ€§

    Args:
        seed: éšæœºç§å­å€¼
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)

    # ç¡®ä¿å¯å¤ç°çš„è¡Œä¸º
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    print(f"âœ… éšæœºç§å­å·²è®¾ç½®ä¸º {seed}")


def get_device() -> torch.device:
    """
    è·å–æœ€ä½³å¯ç”¨è®¾å¤‡ (GPU æˆ– CPU)

    Returns:
        torch.device å¯¹è±¡
    """
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"âœ… ä½¿ç”¨ CUDA è®¾å¤‡: {torch.cuda.get_device_name(0)}")
    else:
        device = torch.device("cpu")
        print("âš ï¸  ä½¿ç”¨ CPU (è¾ƒæ…¢ï¼Œä½†ä»å¯å·¥ä½œ)")

    return device


def count_parameters(model: torch.nn.Module) -> Dict[str, int]:
    """
    ç»Ÿè®¡æ¨¡å‹ä¸­å¯è®­ç»ƒå’Œå†»ç»“çš„å‚æ•°æ•°é‡

    Args:
        model: PyTorch æ¨¡å‹

    Returns:
        åŒ…å«å‚æ•°ç»Ÿè®¡çš„å­—å…¸
    """
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    frozen_params = total_params - trainable_params

    return {
        'total': total_params,
        'trainable': trainable_params,
        'frozen': frozen_params,
        'trainable_percentage': 100.0 * trainable_params / total_params if total_params > 0 else 0,
    }


def print_model_summary(model: torch.nn.Module):
    """
    æ‰“å°æ¨¡å‹å‚æ•°æ‘˜è¦

    Args:
        model: PyTorch æ¨¡å‹
    """
    stats = count_parameters(model)

    print("\n" + "="*60)
    print("æ¨¡å‹å‚æ•°æ‘˜è¦")
    print("="*60)
    print(f"æ€»å‚æ•°æ•°:          {stats['total']:>15,}")
    print(f"å¯è®­ç»ƒå‚æ•°æ•°:      {stats['trainable']:>15,}")
    print(f"å†»ç»“å‚æ•°æ•°:        {stats['frozen']:>15,}")
    print(f"å¯è®­ç»ƒæ¯”ä¾‹:        {stats['trainable_percentage']:>14.2f}%")
    print("="*60 + "\n")


def load_checkpoint(
    checkpoint_path: str,
    model: torch.nn.Module,
    optimizer: torch.optim.Optimizer = None,
    device: torch.device = None,
) -> Dict:
    """
    åŠ è½½è®­ç»ƒæ£€æŸ¥ç‚¹

    Args:
        checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
        model: è¦åŠ è½½çŠ¶æ€å­—å…¸çš„æ¨¡å‹
        optimizer: å¯é€‰çš„ä¼˜åŒ–å™¨ï¼Œç”¨äºåŠ è½½çŠ¶æ€å­—å…¸
        device: åŠ è½½æ£€æŸ¥ç‚¹çš„è®¾å¤‡

    Returns:
        æ£€æŸ¥ç‚¹å­—å…¸
    """
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    print(f"ğŸ“¥ æ­£åœ¨ä»ä»¥ä¸‹ä½ç½®åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")

    checkpoint = torch.load(checkpoint_path, map_location=device)

    # åŠ è½½æ¨¡å‹çŠ¶æ€
    model.load_state_dict(checkpoint['model_state_dict'])
    print("âœ… æ¨¡å‹çŠ¶æ€å·²åŠ è½½")

    # å¦‚æœæä¾›åˆ™åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
    if optimizer is not None and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        print("âœ… ä¼˜åŒ–å™¨çŠ¶æ€å·²åŠ è½½")

    # æå–å…ƒæ•°æ®
    epoch = checkpoint.get('epoch', 0)
    global_step = checkpoint.get('global_step', 0)

    print(f"   è½®æ¬¡: {epoch}")
    print(f"   å…¨å±€æ­¥æ•°: {global_step}")

    return checkpoint


def save_config_as_json(config, save_path: str):
    """
    å°†é…ç½®ä¿å­˜ä¸º JSON ä»¥ç¡®ä¿å¯å¤ç°æ€§

    Args:
        config: é…ç½®å¯¹è±¡æˆ–å­—å…¸
        save_path: ä¿å­˜ JSON çš„è·¯å¾„
    """
    config_dict = {}

    if hasattr(config, '__dict__'):
        # å¦‚æœæ˜¯å¯¹è±¡ï¼Œæå–å±æ€§
        for key in dir(config):
            if not key.startswith('_') and not callable(getattr(config, key)):
                value = getattr(config, key)
                # åªä¿å­˜å¯åºåˆ—åŒ–çš„ç±»å‹
                if isinstance(value, (str, int, float, bool, list, dict)):
                    config_dict[key] = value
    else:
        # å¦‚æœå·²ç»æ˜¯å­—å…¸
        config_dict = config

    with open(save_path, 'w') as f:
        json.dump(config_dict, f, indent=2)

    print(f"âœ… é…ç½®å·²ä¿å­˜åˆ° {save_path}")


def create_output_directory(base_path: str = None) -> Path:
    """
    åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„è¾“å‡ºç›®å½•

    Args:
        base_path: è¾“å‡ºç›®å½•çš„åŸºç¡€è·¯å¾„

    Returns:
        è¾“å‡ºç›®å½•çš„ Path å¯¹è±¡
    """
    from datetime import datetime

    if base_path is None:
        base_path = "./output"

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = Path(base_path) / f"run_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)

    print(f"ğŸ“ è¾“å‡ºç›®å½•å·²åˆ›å»º: {output_dir}")
    return output_dir


def format_time(seconds: float) -> str:
    """
    å°†ç§’æ•°æ ¼å¼åŒ–ä¸ºäººç±»å¯è¯»çš„æ—¶é—´

    Args:
        seconds: ä»¥ç§’ä¸ºå•ä½çš„æ—¶é—´

    Returns:
        æ ¼å¼åŒ–çš„æ—¶é—´å­—ç¬¦ä¸²
    """
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)

    if hours > 0:
        return f"{hours}å°æ—¶ {minutes}åˆ†é’Ÿ {secs}ç§’"
    elif minutes > 0:
        return f"{minutes}åˆ†é’Ÿ {secs}ç§’"
    else:
        return f"{secs}ç§’"


def get_gpu_memory_info() -> Dict[str, float]:
    """
    è·å– GPU å†…å­˜ä½¿ç”¨ä¿¡æ¯

    Returns:
        åŒ…å«å†…å­˜ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸ï¼ˆä»¥ GB ä¸ºå•ä½ï¼‰
    """
    if not torch.cuda.is_available():
        return {}

    # æ€»å†…å­˜
    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9

    # å·²åˆ†é…å†…å­˜
    allocated_memory = torch.cuda.memory_allocated(0) / 1e9

    # ä¿ç•™å†…å­˜
    reserved_memory = torch.cuda.memory_reserved(0) / 1e9

    # ç©ºé—²å†…å­˜
    free_memory = total_memory - allocated_memory

    return {
        'total_gb': round(total_memory, 2),
        'allocated_gb': round(allocated_memory, 2),
        'reserved_gb': round(reserved_memory, 2),
        'free_gb': round(free_memory, 2),
        'allocated_percentage': round(100.0 * allocated_memory / total_memory, 2),
    }


def print_gpu_memory():
    """æ‰“å° GPU å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    memory_info = get_gpu_memory_info()

    if not memory_info:
        print("âš ï¸  GPU ä¸å¯ç”¨")
        return

    print("\n" + "="*60)
    print("GPU å†…å­˜ä½¿ç”¨")
    print("="*60)
    print(f"æ€»è®¡:      {memory_info['total_gb']:>6.2f} GB")
    print(f"å·²åˆ†é…:    {memory_info['allocated_gb']:>6.2f} GB ({memory_info['allocated_percentage']:>5.1f}%)")
    print(f"ä¿ç•™:      {memory_info['reserved_gb']:>6.2f} GB")
    print(f"ç©ºé—²:      {memory_info['free_gb']:>6.2f} GB")
    print("="*60 + "\n")


def cleanup_old_checkpoints(checkpoint_dir: Path, keep_best: bool = True, max_keep: int = 3):
    """
    æ¸…ç†æ—§çš„æ£€æŸ¥ç‚¹ï¼Œåªä¿ç•™æœ€è¿‘çš„å‡ ä¸ª

    Args:
        checkpoint_dir: åŒ…å«æ£€æŸ¥ç‚¹çš„ç›®å½•
        keep_best: æ˜¯å¦å§‹ç»ˆä¿ç•™æœ€ä½³æ£€æŸ¥ç‚¹
        max_keep: è¦ä¿ç•™çš„æœ€å¤§æ£€æŸ¥ç‚¹æ•°é‡
    """
    checkpoint_dir = Path(checkpoint_dir)

    if not checkpoint_dir.exists():
        return

    # è·å–æ‰€æœ‰æ£€æŸ¥ç‚¹æ–‡ä»¶
    checkpoint_files = sorted(
        checkpoint_dir.glob("checkpoint_step_*.pt"),
        key=lambda x: x.stat().st_mtime,  # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
    )

    # ä¿ç•™æœ€ä½³æ£€æŸ¥ç‚¹
    keep_files = set()
    if keep_best and (checkpoint_dir / "checkpoint_best.pt").exists():
        keep_files.add("checkpoint_best.pt")

    # ä¿ç•™æœ€è¿‘çš„å‡ ä¸ª
    for cp_file in checkpoint_files[-max_keep:]:
        keep_files.add(cp_file.name)

    # åˆ é™¤å…¶ä»–æ–‡ä»¶
    deleted_count = 0
    for cp_file in checkpoint_files:
        if cp_file.name not in keep_files:
            cp_file.unlink()
            deleted_count += 1

    if deleted_count > 0:
        print(f"ğŸ§¹ å·²åˆ é™¤ {deleted_count} ä¸ªæ—§æ£€æŸ¥ç‚¹")


def validate_paths(paths: Dict[str, str]) -> bool:
    """
    éªŒè¯æ‰€æœ‰å¿…éœ€çš„è·¯å¾„æ˜¯å¦å­˜åœ¨

    Args:
        paths: è·¯å¾„åç§°å’Œè·¯å¾„çš„å­—å…¸

    Returns:
        å¦‚æœæ‰€æœ‰è·¯å¾„éƒ½å­˜åœ¨åˆ™è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
    """
    print("\n" + "="*60)
    print("è·¯å¾„éªŒè¯")
    print("="*60)

    all_valid = True
    for name, path in paths.items():
        exists = os.path.exists(path)
        status = "âœ…" if exists else "âŒ"
        print(f"{status} {name}: {path}")
        if not exists:
            all_valid = False

    print("="*60 + "\n")
    return all_valid


class AverageMeter:
    """
    è®¡ç®—å’Œå­˜å‚¨æŒ‡æ ‡çš„å½“å‰å€¼å’Œå¹³å‡å€¼

    ç”¨äºè®­ç»ƒè¿‡ç¨‹ä¸­è¿½è¸ªæŒ‡æ ‡ï¼Œä¾‹å¦‚æŸå¤±ã€å‡†ç¡®ç‡ç­‰ã€‚

    ç¤ºä¾‹ï¼š
        >>> meter = AverageMeter("æŸå¤±", fmt=":6.4f")
        >>> for batch_loss in losses:
        ...     meter.update(batch_loss, n=batch_size)
        >>> print(meter)  # è¾“å‡º: æŸå¤±  1.2345 (1.2000)
    """

    def __init__(self, name: str, fmt: str = ":.4f"):
        """
        åˆå§‹åŒ– AverageMeter

        Args:
            name (str): æŒ‡æ ‡åç§°ï¼ˆç”¨äºæ‰“å°ï¼‰
            fmt (str): æ•°å­—æ ¼å¼åŒ–å­—ç¬¦ä¸²ï¼ˆé»˜è®¤ 4 ä½å°æ•°ï¼‰
        """
        self.name = name  # æŒ‡æ ‡åç§°
        self.fmt = fmt  # æ ¼å¼åŒ–æ ¼å¼
        self.reset()  # åˆå§‹åŒ–è®¡æ•°å™¨

    def reset(self):
        """é‡ç½®æ‰€æœ‰è®¡æ•°å™¨"""
        self.val = 0  # å½“å‰å€¼
        self.avg = 0  # å¹³å‡å€¼
        self.sum = 0  # ç´¯ç§¯å’Œ
        self.count = 0  # æ ·æœ¬è®¡æ•°

    def update(self, val, n: int = 1):
        """
        ä½¿ç”¨æ–°å€¼æ›´æ–°å¹³å‡å€¼

        Args:
            val (float): æ–°çš„å€¼
            n (int): å€¼çš„æ•°é‡ï¼ˆç”¨äºåŠ æƒå¹³å‡ï¼‰
        """
        self.val = val  # æ›´æ–°å½“å‰å€¼
        self.sum += val * n  # ç´¯åŠ åŠ æƒå€¼
        self.count += n  # æ›´æ–°è®¡æ•°
        self.avg = self.sum / self.count if self.count > 0 else 0  # è®¡ç®—å¹³å‡å€¼

    def __str__(self):
        """è¿”å›æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²è¡¨ç¤º"""
        fmtstr = "{name} {val" + self.fmt + "} ({avg" + self.fmt + "})"
        return fmtstr.format(name=self.name, val=self.val, avg=self.avg)


def setup_logging(log_file: str = None):
    """
    è®¾ç½®æ—¥å¿—è®°å½•é…ç½®

    Args:
        log_file: å¯é€‰çš„æ—¥å¿—æ–‡ä»¶è·¯å¾„
    """
    import logging

    log_format = "%(asctime)s - %(levelname)s - %(message)s"

    # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
    logger = logging.getLogger("VLM_VLA")
    logger.setLevel(logging.DEBUG)

    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter(log_format)
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)

    # æ–‡ä»¶å¤„ç†å™¨
    if log_file:
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.DEBUG)
        file_formatter = logging.Formatter(log_format)
        file_handler.setFormatter(file_formatter)
        logger.addHandler(file_handler)

    return logger


if __name__ == "__main__":
    # æµ‹è¯•å·¥å…·å‡½æ•°
    print("æµ‹è¯•å·¥å…·å‡½æ•°...\n")

    set_seed(42)
    device = get_device()

    print_gpu_memory()

    # æµ‹è¯• AverageMeter
    meter = AverageMeter("æŸå¤±")
    for i in range(10):
        meter.update(1.0 - 0.1 * i, n=1)
    print(f"\n{meter}")

