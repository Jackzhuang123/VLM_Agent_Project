"""
Training script for VLM-VLA Agent
Handles the main training loop with validation, checkpointing, and logging
"""

import json
import os
import time
from datetime import datetime
from pathlib import Path

import torch
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm

from .config import Config
from .dataset import create_dataloaders
from .model import create_model


class Trainer:
    """
    VLM-VLA Agent ËÆ≠ÁªÉÂô®

    ‰∏ªË¶ÅËÅåË¥£Ôºö
        1. Êï∞ÊçÆÂä†ËΩΩÔºö‰ªéÂ§öÁßçÊ†ºÂºèÂä†ËΩΩÊï∞ÊçÆÈõÜ
        2. Ê®°ÂûãÂàùÂßãÂåñÔºöÂàõÂª∫ÂíåÈÖçÁΩÆÊ®°Âûã
        3. ËÆ≠ÁªÉÂæ™ÁéØÔºöÊâßË°å epoch ËÆ≠ÁªÉÂíåÈ™åËØÅ
        4. Ê£ÄÊü•ÁÇπÁÆ°ÁêÜÔºö‰øùÂ≠òÊúÄ‰ºòÊ®°Âûã
        5. ÊåáÊ†áËÆ∞ÂΩïÔºöË∑üË∏™ËÆ≠ÁªÉËøõÂ∫¶

    ÁâπÊÄßÔºö
        - Ëá™Âä®ÈîôËØØÊÅ¢Â§çÔºàË∑≥ËøáÊçüÂùèÁöÑÊâπÊ¨°Ôºâ
        - Ê∑∑ÂêàÁ≤æÂ∫¶ËÆ≠ÁªÉÊîØÊåÅ
        - Ê¢ØÂ∫¶Á¥ØÁßØÊîØÊåÅ
        - Â≠¶‰π†ÁéáË∞ÉÂ∫¶ÊîØÊåÅ
        - ËØ¶ÁªÜÁöÑËøõÂ∫¶ÊòæÁ§∫
    """

    def __init__(self, config: Config = None):
        """
        ÂàùÂßãÂåñËÆ≠ÁªÉÂô®

        ÂàùÂßãÂåñÊ≠•È™§Ôºö
        1. Ê£ÄÊµãËÆæÂ§áÔºàGPU/CPUÔºâ
        2. ÂàõÂª∫ËæìÂá∫ÁõÆÂΩï
        3. ÂàùÂßãÂåñÊåáÊ†áË∑üË∏™

        Args:
            config (Config): ÈÖçÁΩÆÂØπË±°Ôºà‰ΩøÁî® Config Á±ªÁöÑÈªòËÆ§ÂÄºÔºâ
        """
        self.config = config or Config  # ‰ΩøÁî®‰º†ÂÖ•ÁöÑÈÖçÁΩÆÊàñÈªòËÆ§ÈÖçÁΩÆ
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Ê£ÄÊµãÂèØÁî®ËÆæÂ§á

        print(f"\n{'='*60}")
        print("Initializing Trainer")
        print(f"{'='*60}")
        print(f"Device: {self.device}")
        print(f"CUDA available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"CUDA device: {torch.cuda.get_device_name(0)}")
            print(f"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

        # Create output directory
        os.makedirs(self.config.OUTPUT_DIR, exist_ok=True)

        # Initialize training state
        self.global_step = 0
        self.epoch = 0
        self.best_loss = float('inf')

        # Create timestamp for checkpoints
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.checkpoint_dir = Path(self.config.OUTPUT_DIR) / f"checkpoint_{self.timestamp}"
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)

        # Logging
        self.metrics = {
            'train': {
                'loss': [],
                'action_loss': [],
                'learning_rate': [],
            },
            'val': {
                'loss': [],
                'action_loss': [],
            }
        }

        print(f"Checkpoint directory: {self.checkpoint_dir}")
        print(f"{'='*60}\n")

    def load_data(self):
        """Load training and validation dataloaders"""
        print(f"{'='*60}")
        print("Loading Datasets")
        print(f"{'='*60}")

        self.train_loader, self.val_loader = create_dataloaders(
            batch_size=self.config.BATCH_SIZE,
            num_workers=self.config.NUM_WORKERS,
            test_split=0.1,
            seed=self.config.SEED,
        )

        print(f"Train batches: {len(self.train_loader)}")
        print(f"Val batches: {len(self.val_loader)}")
        print(f"{'='*60}\n")

    def setup_model(self):
        """Initialize model and optimizer"""
        print(f"Ê≠£Âú®‰ªé‰ª•‰∏ãË∑ØÂæÑÂä†ËΩΩÊ®°Âûã: {self.config.LLM_PATH}")

        self.model = create_model(
            clip_path=self.config.CLIP_PATH,
            llm_path=self.config.LLM_PATH,
            use_lora=True,
            use_4bit=self.config.USE_4BIT,
        )

        self.model.to(self.device)

        # Setup optimizer - only optimize trainable parameters
        trainable_params = self.model.get_trainable_params()

        self.optimizer = optim.AdamW(
            trainable_params,
            lr=self.config.LEARNING_RATE,
            weight_decay=self.config.WEIGHT_DECAY,
            betas=(0.9, 0.95),
        )

        # Setup learning rate scheduler
        total_steps = len(self.train_loader) * self.config.MAX_EPOCHS
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=total_steps,
            eta_min=self.config.LEARNING_RATE * 0.1
        )

        # Mixed precision training
        self.scaler = GradScaler() if self.config.USE_MIXED_PRECISION else None

        print(f"\n{'='*60}")
        print("Optimizer Configuration")
        print(f"{'='*60}")
        print(f"Learning rate: {self.config.LEARNING_RATE}")
        print(f"Weight decay: {self.config.WEIGHT_DECAY}")
        print(f"Mixed precision: {self.config.USE_MIXED_PRECISION}")
        print(f"Gradient accumulation steps: {self.config.GRADIENT_ACCUMULATION_STEPS}")
        print(f"Total training steps: {total_steps}")
        print(f"{'='*60}\n")

    def train_epoch(self):
        """
        ÊâßË°å‰∏Ä‰∏™ epoch ÁöÑËÆ≠ÁªÉ

        ËÆ≠ÁªÉÊµÅÁ®ãÔºö
        1. ÈÅçÂéÜËÆ≠ÁªÉÊâπÊ¨°
        2. ÂâçÂêë‰º†Êí≠ËÆ°ÁÆóÊçüÂ§±
        3. ÂèçÂêë‰º†Êí≠Êõ¥Êñ∞Ê¢ØÂ∫¶
        4. Ê¢ØÂ∫¶Á¥ØÁßØÂ§ÑÁêÜ
        5. ‰ºòÂåñÂô®Ê≠•È™§ÂíåÂ≠¶‰π†ÁéáÊõ¥Êñ∞

        ÁâπÁÇπÔºö
        - ÊîØÊåÅÊ∑∑ÂêàÁ≤æÂ∫¶ËÆ≠ÁªÉÂä†ÈÄü
        - ÊîØÊåÅÊ¢ØÂ∫¶Á¥ØÁßØÂ¢ûÂä†ÊúâÊïàÊâπÂ§ßÂ∞è
        - Ëá™Âä®ÈîôËØØÂ§ÑÁêÜÂíåÊâπÊ¨°Ë∑≥Ëøá
        - ÂÆöÊúü‰øùÂ≠òÊ£ÄÊü•ÁÇπ

        Returns:
            tuple: (Âπ≥ÂùáËÆ≠ÁªÉÊçüÂ§±, Âπ≥ÂùáÂä®‰ΩúÊçüÂ§±)
        """
        self.model.train()  # ËÆæÁΩÆÊ®°Âûã‰∏∫ËÆ≠ÁªÉÊ®°Âºè

        total_loss = 0.0  # Á¥ØÁßØÊçüÂ§±
        total_action_loss = 0.0  # Á¥ØÁßØÂä®‰ΩúÊçüÂ§±
        skipped_batches = 0  # Ë∑≥ËøáÁöÑÊâπÊ¨°ËÆ°Êï∞ÔºàÈîôËØØÊÅ¢Â§çÔºâ

        pbar = tqdm(
            enumerate(self.train_loader),
            total=len(self.train_loader),
            desc=f"Epoch {self.epoch + 1}/{self.config.MAX_EPOCHS}",
            leave=True,
        )

        for batch_idx, batch in pbar:
            try:
                # Â∞ÜÊâπÊ¨°Êï∞ÊçÆÁßªÂà∞ËÆæÂ§áÔºàGPU/CPUÔºâ
                images_t1 = batch['image_t1'].to(self.device)  # Êó∂Â∫èÂΩ±ÂÉè 1
                images_t2 = batch['image_t2'].to(self.device)  # Êó∂Â∫èÂΩ±ÂÉè 2
                action_targets = batch['action_vector'].to(self.device)  # ÁõÆÊ†áÂä®‰ΩúÂêëÈáè
                captions = batch['caption']  # ÂèòÂåñÊèèËø∞ÊñáÊú¨

                # ÂâçÂêë‰º†Êí≠ÔºàÊîØÊåÅÊ∑∑ÂêàÁ≤æÂ∫¶Âä†ÈÄüÔºâ
                if self.config.USE_MIXED_PRECISION and self.scaler:
                    with autocast(dtype=torch.bfloat16):
                        outputs = self.model(
                            images_t1,
                            images_t2,
                            captions,
                            action_targets=action_targets,
                        )
                        loss = outputs['total_loss']

                        # Normalize loss by gradient accumulation steps
                        loss = loss / self.config.GRADIENT_ACCUMULATION_STEPS

                    # Backward pass with gradient scaling
                    self.scaler.scale(loss).backward()
                else:
                    outputs = self.model(
                        images_t1,
                        images_t2,
                        captions,
                        action_targets=action_targets,
                    )
                    loss = outputs['total_loss']
                    loss = loss / self.config.GRADIENT_ACCUMULATION_STEPS
                    loss.backward()

                # Gradient accumulation
                if (batch_idx + 1) % self.config.GRADIENT_ACCUMULATION_STEPS == 0:
                    # Gradient clipping
                    if self.config.USE_MIXED_PRECISION and self.scaler:
                        self.scaler.unscale_(self.optimizer)
                        torch.nn.utils.clip_grad_norm_(
                            self.model.get_trainable_params(),
                            self.config.MAX_GRAD_NORM
                        )
                        self.scaler.step(self.optimizer)
                        self.scaler.update()
                    else:
                        torch.nn.utils.clip_grad_norm_(
                            self.model.get_trainable_params(),
                            self.config.MAX_GRAD_NORM
                        )
                        self.optimizer.step()

                    self.optimizer.zero_grad()
                    self.scheduler.step()
                    self.global_step += 1

            except Exception as e:
                print(f"\n‚ö†Ô∏è  ÊâπÂ§ÑÁêÜ {batch_idx} Âá∫Áé∞ÈîôËØØ: {e}")
                import traceback
                traceback.print_exc()
                skipped_batches += 1
                self.optimizer.zero_grad()  # Ê∏ÖÁ©∫Ê¢ØÂ∫¶
                continue

            # Accumulate loss (multiply back by accumulation steps for reporting)
            try:
                total_loss += loss.item() * self.config.GRADIENT_ACCUMULATION_STEPS

                if 'action_loss' in outputs:
                    action_loss = outputs['action_loss'].item()
                    total_action_loss += action_loss * self.config.GRADIENT_ACCUMULATION_STEPS

                # Update progress bar
                batch_count = batch_idx + 1 - skipped_batches
                if batch_count > 0:
                    avg_loss = total_loss / batch_count
                else:
                    avg_loss = 0.0

                pbar.set_postfix({
                    'loss': f'{avg_loss:.4f}',
                    'lr': f'{self.optimizer.param_groups[0]["lr"]:.2e}',
                    'skipped': skipped_batches,
                })
            except Exception as e:
                print(f"\n‚ö†Ô∏è  ÊçüÂ§±ËÆ°ÁÆóÂá∫Èîô: {e}")

            # Periodic checkpoint and validation
            if self.global_step % self.config.SAVE_INTERVAL == 0 and self.global_step > 0:
                print(f"\nüìä Global step {self.global_step} - Saving checkpoint...")
                self.save_checkpoint(step=self.global_step)

            if self.config.DEBUG and batch_idx >= 5:
                print("üêõ Debug mode: Stopping after 5 batches")
                break

        # Average loss for epoch
        avg_epoch_loss = total_loss / len(self.train_loader)
        avg_action_loss = total_action_loss / len(self.train_loader) if total_action_loss > 0 else 0.0

        self.metrics['train']['loss'].append(avg_epoch_loss)
        self.metrics['train']['action_loss'].append(avg_action_loss)
        self.metrics['train']['learning_rate'].append(self.optimizer.param_groups[0]['lr'])

        return avg_epoch_loss, avg_action_loss

    def validate(self):
        """Validate model on validation set"""
        self.model.eval()

        total_loss = 0.0
        total_action_loss = 0.0
        skipped_batches = 0

        with torch.no_grad():
            pbar = tqdm(
                enumerate(self.val_loader),
                total=len(self.val_loader),
                desc="Validating",
                leave=True,
            )

            for batch_idx, batch in pbar:
                try:
                    images_t1 = batch['image_t1'].to(self.device)
                    images_t2 = batch['image_t2'].to(self.device)
                    action_targets = batch['action_vector'].to(self.device)
                    captions = batch['caption']

                    if self.config.USE_MIXED_PRECISION:
                        with autocast(dtype=torch.bfloat16):
                            outputs = self.model(
                                images_t1,
                                images_t2,
                                captions,
                                action_targets=action_targets,
                            )
                    else:
                        outputs = self.model(
                            images_t1,
                            images_t2,
                            captions,
                            action_targets=action_targets,
                        )

                    loss = outputs['total_loss'].item()
                    total_loss += loss

                    if 'action_loss' in outputs:
                        total_action_loss += outputs['action_loss'].item()

                    batch_count = batch_idx + 1 - skipped_batches
                    pbar.set_postfix({'loss': f'{loss:.4f}', 'skipped': skipped_batches})

                except Exception as e:
                    print(f"\n‚ö†Ô∏è  È™åËØÅÊâπÂ§ÑÁêÜ {batch_idx} Âá∫Áé∞ÈîôËØØ: {e}")
                    skipped_batches += 1
                    continue

        # ËÆ°ÁÆóÂπ≥ÂùáÈ™åËØÅÊçüÂ§±ÔºåÊéíÈô§Ë∑≥ËøáÁöÑÊâπÊ¨°
        valid_batches = len(self.val_loader) - skipped_batches
        if valid_batches > 0:
            avg_val_loss = total_loss / valid_batches
            avg_val_action_loss = total_action_loss / valid_batches if total_action_loss > 0 else 0.0
        else:
            avg_val_loss = float('inf')
            avg_val_action_loss = 0.0
            print(f"‚ö†Ô∏è  ÊâÄÊúâÈ™åËØÅÊâπÂ§ÑÁêÜÈÉΩË¢´Ë∑≥ËøáÔºÅ")

        self.metrics['val']['loss'].append(avg_val_loss)
        self.metrics['val']['action_loss'].append(avg_val_action_loss)

        return avg_val_loss, avg_val_action_loss

    def save_checkpoint(self, step: int = None, is_best: bool = False):
        """
        Save model checkpoint

        ‰ºòÂåñÔºöÂè™‰øùÂ≠òÊ®°ÂûãÊùÉÈáçÂíåËÆ≠ÁªÉÊåáÊ†áÔºå‰∏ç‰øùÂ≠ò‰ºòÂåñÂô®Áä∂ÊÄÅ‰ª•ËäÇÁúÅÁ£ÅÁõòÁ©∫Èó¥
        Ëøô‰ΩøÂæóÊ£ÄÊü•ÁÇπÂ§ßÂ∞èÂáèÂ∞ëÁ∫¶ 70%ÔºåÈÄÇÂ∫î Kaggle ÁöÑÁ£ÅÁõòÈôêÂà∂

        Args:
            step: Global training step
            is_best: Whether this is the best model
        """
        checkpoint_name = f"checkpoint_step_{step}.pt" if step else "checkpoint_latest.pt"
        checkpoint_path = self.checkpoint_dir / checkpoint_name

        # ËΩªÈáèÁ∫ßÊ£ÄÊü•ÁÇπÔºöÂè™‰øùÂ≠òÂøÖË¶ÅÁöÑ‰ø°ÊÅØ‰ª•ÊÅ¢Â§çÊ®°ÂûãÊé®ÁêÜ
        # ‰∏∫‰∫ÜËäÇÁúÅ Kaggle Á£ÅÁõòÁ©∫Èó¥ÔºåÁßªÈô§‰∫Ü‰ºòÂåñÂô®ÂíåË∞ÉÂ∫¶Âô®ÁöÑÂÆåÊï¥Áä∂ÊÄÅ
        # ËøôÂÖÅËÆ∏ÁªßÁª≠ËÆ≠ÁªÉÈúÄË¶ÅÈáçÊñ∞ÂàõÂª∫‰ºòÂåñÂô®Ôºå‰ΩÜËäÇÁúÅÂ§ßÈáèÁ£ÅÁõòÁ©∫Èó¥
        checkpoint = {
            'epoch': self.epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            # Ê≥®Ôºöoptimizer_state_dict Âíå scheduler_state_dict Â∑≤ÁßªÈô§‰ª•ËäÇÁúÅÁ£ÅÁõòÁ©∫Èó¥
            'metrics': self.metrics,
            'config': {
                'max_epochs': self.config.MAX_EPOCHS,
                'batch_size': self.config.BATCH_SIZE,
                'learning_rate': self.config.LEARNING_RATE,
            }
        }

        try:
            torch.save(checkpoint, checkpoint_path)
            print(f"‚úÖ Checkpoint saved: {checkpoint_path}")
        except RuntimeError as e:
            print(f"‚ö†Ô∏è  Ê£ÄÊü•ÁÇπ‰øùÂ≠òÂ§±Ë¥•: {e}")
            print("‚ö†Ô∏è  ÂèØËÉΩÊòØÁ£ÅÁõòÁ©∫Èó¥‰∏çË∂≥ÔºåÁªßÁª≠ËÆ≠ÁªÉ...")
            return

        # Also save best checkpoint
        if is_best or self.best_loss > (self.metrics['val']['loss'][-1] if self.metrics['val']['loss'] else float('inf')):
            best_path = self.checkpoint_dir / "checkpoint_best.pt"
            try:
                torch.save(checkpoint, best_path)
                self.best_loss = self.metrics['val']['loss'][-1] if self.metrics['val']['loss'] else float('inf')
                print(f"‚úÖ Best checkpoint updated: {best_path}")
            except RuntimeError as e:
                print(f"‚ö†Ô∏è  ÊúÄ‰ºòÊ£ÄÊü•ÁÇπ‰øùÂ≠òÂ§±Ë¥•: {e}")

    def cleanup_old_checkpoints(self, keep_count: int = 3):
        """
        Ê∏ÖÁêÜÊóßÊ£ÄÊü•ÁÇπ‰ª•ËäÇÁúÅÁ£ÅÁõòÁ©∫Èó¥
        Âè™‰øùÁïôÊúÄÊñ∞ÁöÑ keep_count ‰∏™Ê£ÄÊü•ÁÇπ

        Args:
            keep_count: Ë¶Å‰øùÁïôÁöÑÊúÄÊñ∞Ê£ÄÊü•ÁÇπÊï∞ÈáèÔºàÈªòËÆ§ 3Ôºâ
        """
        checkpoint_files = sorted(
            self.checkpoint_dir.glob("checkpoint_step_*.pt"),
            key=lambda x: x.stat().st_mtime,
            reverse=True
        )

        # ‰øùÁïôÊúÄÊñ∞ÁöÑ keep_count ‰∏™Ê£ÄÊü•ÁÇπÔºåÂà†Èô§ÂÖ∂‰ªñÁöÑ
        if len(checkpoint_files) > keep_count:
            for old_checkpoint in checkpoint_files[keep_count:]:
                try:
                    old_checkpoint.unlink()
                    print(f"üóëÔ∏è  Âà†Èô§ÊóßÊ£ÄÊü•ÁÇπ: {old_checkpoint.name}")
                except Exception as e:
                    print(f"‚ö†Ô∏è  Êó†Ê≥ïÂà†Èô§ {old_checkpoint.name}: {e}")

    def save_metrics(self):
        """Save training metrics to JSON"""
        metrics_path = self.checkpoint_dir / "metrics.json"

        # Convert lists to summary stats for readability
        metrics_summary = {
            'train': {
                'final_loss': self.metrics['train']['loss'][-1] if self.metrics['train']['loss'] else None,
                'min_loss': min(self.metrics['train']['loss']) if self.metrics['train']['loss'] else None,
                'max_loss': max(self.metrics['train']['loss']) if self.metrics['train']['loss'] else None,
            },
            'val': {
                'final_loss': self.metrics['val']['loss'][-1] if self.metrics['val']['loss'] else None,
                'min_loss': min(self.metrics['val']['loss']) if self.metrics['val']['loss'] else None,
                'max_loss': max(self.metrics['val']['loss']) if self.metrics['val']['loss'] else None,
            }
        }

        with open(metrics_path, 'w') as f:
            json.dump(metrics_summary, f, indent=2)

        print(f"‚úÖ Metrics saved: {metrics_path}")

    def train(self):
        """Main training loop"""
        print(f"\n{'='*60}")
        print("Starting Training")
        print(f"{'='*60}\n")

        # Load data
        self.load_data()

        # Setup model
        self.setup_model()

        # Training loop
        start_time = time.time()

        for epoch in range(self.config.MAX_EPOCHS):
            self.epoch = epoch

            # Train epoch
            train_loss, train_action_loss = self.train_epoch()
            print(f"Epoch {epoch + 1} - Train Loss: {train_loss:.4f}, Action Loss: {train_action_loss:.4f}")

            # Validate
            val_loss, val_action_loss = self.validate()
            print(f"Epoch {epoch + 1} - Val Loss: {val_loss:.4f}, Action Loss: {val_action_loss:.4f}")

            # Save checkpoint
            is_best = val_loss < self.best_loss
            self.save_checkpoint(step=self.epoch, is_best=is_best)

            # Cleanup old checkpoints to save disk space
            self.cleanup_old_checkpoints(keep_count=3)

            print(f"{'='*60}")

        # Final checkpoint
        self.save_checkpoint(step=self.global_step)
        self.save_metrics()

        elapsed_time = time.time() - start_time
        print(f"\n{'='*60}")
        print("Training Completed")
        print(f"{'='*60}")
        print(f"Total time: {elapsed_time / 3600:.2f} hours")
        print(f"Checkpoints saved in: {self.checkpoint_dir}")
        print(f"{'='*60}\n")


def main():
    """Main entry point"""
    # Print configuration
    Config.print_config()
    Config.verify_paths()

    # Create trainer and start training
    trainer = Trainer(config=Config)
    trainer.train()


if __name__ == "__main__":
    main()

