"""
Training script for VLM-VLA Agent
Handles the main training loop with validation, checkpointing, and logging
"""

import json
import os
import time
from datetime import datetime
from pathlib import Path

import torch
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm

from .config import Config
from .dataset import create_dataloaders
from .model import create_model


class Trainer:
    """
    VLM-VLA Agent è®­ç»ƒå™¨

    ä¸»è¦èŒè´£ï¼š
        1. æ•°æ®åŠ è½½ï¼šä»Žå¤šç§æ ¼å¼åŠ è½½æ•°æ®é›†
        2. æ¨¡åž‹åˆå§‹åŒ–ï¼šåˆ›å»ºå’Œé…ç½®æ¨¡åž‹
        3. è®­ç»ƒå¾ªçŽ¯ï¼šæ‰§è¡Œ epoch è®­ç»ƒå’ŒéªŒè¯
        4. æ£€æŸ¥ç‚¹ç®¡ç†ï¼šä¿å­˜æœ€ä¼˜æ¨¡åž‹
        5. æŒ‡æ ‡è®°å½•ï¼šè·Ÿè¸ªè®­ç»ƒè¿›åº¦

    ç‰¹æ€§ï¼š
        - è‡ªåŠ¨é”™è¯¯æ¢å¤ï¼ˆè·³è¿‡æŸåçš„æ‰¹æ¬¡ï¼‰
        - æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒ
        - æ¢¯åº¦ç´¯ç§¯æ”¯æŒ
        - å­¦ä¹ çŽ‡è°ƒåº¦æ”¯æŒ
        - è¯¦ç»†çš„è¿›åº¦æ˜¾ç¤º
    """

    def __init__(self, config: Config = None):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨

        åˆå§‹åŒ–æ­¥éª¤ï¼š
        1. æ£€æµ‹è®¾å¤‡ï¼ˆGPU/CPUï¼‰
        2. åˆ›å»ºè¾“å‡ºç›®å½•
        3. åˆå§‹åŒ–æŒ‡æ ‡è·Ÿè¸ª

        Args:
            config (Config): é…ç½®å¯¹è±¡ï¼ˆä½¿ç”¨ Config ç±»çš„é»˜è®¤å€¼ï¼‰
        """
        self.config = config or Config  # ä½¿ç”¨ä¼ å…¥çš„é…ç½®æˆ–é»˜è®¤é…ç½®
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # æ£€æµ‹å¯ç”¨è®¾å¤‡

        print(f"\n{'='*60}")
        print("Initializing Trainer")
        print(f"{'='*60}")
        print(f"Device: {self.device}")
        print(f"CUDA available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"CUDA device: {torch.cuda.get_device_name(0)}")
            print(f"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

        # Create output directory
        os.makedirs(self.config.OUTPUT_DIR, exist_ok=True)

        # Initialize training state
        self.global_step = 0
        self.epoch = 0
        self.best_loss = float('inf')

        # Create timestamp for checkpoints
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.checkpoint_dir = Path(self.config.OUTPUT_DIR) / f"checkpoint_{self.timestamp}"
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)

        # Logging
        self.metrics = {
            'train': {
                'loss': [],
                'action_loss': [],
                'learning_rate': [],
            },
            'val': {
                'loss': [],
                'action_loss': [],
            }
        }

        print(f"Checkpoint directory: {self.checkpoint_dir}")
        print(f"{'='*60}\n")

    def load_data(self):
        """Load training and validation dataloaders"""
        print(f"{'='*60}")
        print("Loading Datasets")
        print(f"{'='*60}")

        self.train_loader, self.val_loader = create_dataloaders(
            batch_size=self.config.BATCH_SIZE,
            num_workers=self.config.NUM_WORKERS,
            test_split=0.1,
            seed=self.config.SEED,
        )

        print(f"Train batches: {len(self.train_loader)}")
        print(f"Val batches: {len(self.val_loader)}")
        print(f"{'='*60}\n")

    def setup_model(self):
        """Initialize model and optimizer"""
        print(f"æ­£åœ¨ä»Žä»¥ä¸‹è·¯å¾„åŠ è½½æ¨¡åž‹: {self.config.LLM_PATH}")

        self.model = create_model(
            clip_path=self.config.CLIP_PATH,
            llm_path=self.config.LLM_PATH,
            use_lora=True,
            use_4bit=self.config.USE_4BIT,
        )

        self.model.to(self.device)

        # Setup optimizer - only optimize trainable parameters
        trainable_params = self.model.get_trainable_params()

        self.optimizer = optim.AdamW(
            trainable_params,
            lr=self.config.LEARNING_RATE,
            weight_decay=self.config.WEIGHT_DECAY,
            betas=(0.9, 0.95),
        )

        # Setup learning rate scheduler
        total_steps = len(self.train_loader) * self.config.MAX_EPOCHS
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=total_steps,
            eta_min=self.config.LEARNING_RATE * 0.1
        )

        # Mixed precision training
        self.scaler = GradScaler() if self.config.USE_MIXED_PRECISION else None

        print(f"\n{'='*60}")
        print("Optimizer Configuration")
        print(f"{'='*60}")
        print(f"Learning rate: {self.config.LEARNING_RATE}")
        print(f"Weight decay: {self.config.WEIGHT_DECAY}")
        print(f"Mixed precision: {self.config.USE_MIXED_PRECISION}")
        print(f"Gradient accumulation steps: {self.config.GRADIENT_ACCUMULATION_STEPS}")
        print(f"Total training steps: {total_steps}")
        print(f"{'='*60}\n")

    def train_epoch(self):
        """
        æ‰§è¡Œä¸€ä¸ª epoch çš„è®­ç»ƒ

        è®­ç»ƒæµç¨‹ï¼š
        1. éåŽ†è®­ç»ƒæ‰¹æ¬¡
        2. å‰å‘ä¼ æ’­è®¡ç®—æŸå¤±
        3. åå‘ä¼ æ’­æ›´æ–°æ¢¯åº¦
        4. æ¢¯åº¦ç´¯ç§¯å¤„ç†
        5. ä¼˜åŒ–å™¨æ­¥éª¤å’Œå­¦ä¹ çŽ‡æ›´æ–°

        ç‰¹ç‚¹ï¼š
        - æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒåŠ é€Ÿ
        - æ”¯æŒæ¢¯åº¦ç´¯ç§¯å¢žåŠ æœ‰æ•ˆæ‰¹å¤§å°
        - è‡ªåŠ¨é”™è¯¯å¤„ç†å’Œæ‰¹æ¬¡è·³è¿‡
        - å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹

        Returns:
            tuple: (å¹³å‡è®­ç»ƒæŸå¤±, å¹³å‡åŠ¨ä½œæŸå¤±)
        """
        self.model.train()  # è®¾ç½®æ¨¡åž‹ä¸ºè®­ç»ƒæ¨¡å¼

        total_loss = 0.0  # ç´¯ç§¯æŸå¤±
        total_action_loss = 0.0  # ç´¯ç§¯åŠ¨ä½œæŸå¤±
        skipped_batches = 0  # è·³è¿‡çš„æ‰¹æ¬¡è®¡æ•°ï¼ˆé”™è¯¯æ¢å¤ï¼‰

        pbar = tqdm(
            enumerate(self.train_loader),
            total=len(self.train_loader),
            desc=f"Epoch {self.epoch + 1}/{self.config.MAX_EPOCHS}",
            leave=True,
        )

        for batch_idx, batch in pbar:
            try:
                # å°†æ‰¹æ¬¡æ•°æ®ç§»åˆ°è®¾å¤‡ï¼ˆGPU/CPUï¼‰
                images_t1 = batch['image_t1'].to(self.device)  # æ—¶åºå½±åƒ 1
                images_t2 = batch['image_t2'].to(self.device)  # æ—¶åºå½±åƒ 2
                action_targets = batch['action_vector'].to(self.device)  # ç›®æ ‡åŠ¨ä½œå‘é‡
                captions = batch['caption']  # å˜åŒ–æè¿°æ–‡æœ¬

                # å‰å‘ä¼ æ’­ï¼ˆæ”¯æŒæ··åˆç²¾åº¦åŠ é€Ÿï¼‰
                if self.config.USE_MIXED_PRECISION and self.scaler:
                    with autocast(dtype=torch.bfloat16):
                        outputs = self.model(
                            images_t1,
                            images_t2,
                            captions,
                            action_targets=action_targets,
                        )
                        loss = outputs['total_loss']

                        # Normalize loss by gradient accumulation steps
                        loss = loss / self.config.GRADIENT_ACCUMULATION_STEPS

                    # Backward pass with gradient scaling
                    self.scaler.scale(loss).backward()
                else:
                    outputs = self.model(
                        images_t1,
                        images_t2,
                        captions,
                        action_targets=action_targets,
                    )
                    loss = outputs['total_loss']
                    loss = loss / self.config.GRADIENT_ACCUMULATION_STEPS
                    loss.backward()

                # Gradient accumulation
                if (batch_idx + 1) % self.config.GRADIENT_ACCUMULATION_STEPS == 0:
                    # Gradient clipping
                    if self.config.USE_MIXED_PRECISION and self.scaler:
                        self.scaler.unscale_(self.optimizer)
                        torch.nn.utils.clip_grad_norm_(
                            self.model.get_trainable_params(),
                            self.config.MAX_GRAD_NORM
                        )
                        self.scaler.step(self.optimizer)
                        self.scaler.update()
                    else:
                        torch.nn.utils.clip_grad_norm_(
                            self.model.get_trainable_params(),
                            self.config.MAX_GRAD_NORM
                        )
                        self.optimizer.step()

                    self.optimizer.zero_grad()
                    self.scheduler.step()
                    self.global_step += 1

            except Exception as e:
                print(f"\nâš ï¸  æ‰¹å¤„ç† {batch_idx} å‡ºçŽ°é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                skipped_batches += 1
                self.optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦
                continue

            # Accumulate loss (multiply back by accumulation steps for reporting)
            try:
                total_loss += loss.item() * self.config.GRADIENT_ACCUMULATION_STEPS

                if 'action_loss' in outputs:
                    action_loss = outputs['action_loss'].item()
                    total_action_loss += action_loss * self.config.GRADIENT_ACCUMULATION_STEPS

                # Update progress bar
                batch_count = batch_idx + 1 - skipped_batches
                if batch_count > 0:
                    avg_loss = total_loss / batch_count
                else:
                    avg_loss = 0.0

                pbar.set_postfix({
                    'loss': f'{avg_loss:.4f}',
                    'lr': f'{self.optimizer.param_groups[0]["lr"]:.2e}',
                    'skipped': skipped_batches,
                })
            except Exception as e:
                print(f"\nâš ï¸  æŸå¤±è®¡ç®—å‡ºé”™: {e}")

            # Periodic checkpoint and validation
            if self.global_step % self.config.SAVE_INTERVAL == 0 and self.global_step > 0:
                print(f"\nðŸ“Š Global step {self.global_step} - Saving checkpoint...")
                self.save_checkpoint(step=self.global_step)

            if self.config.DEBUG and batch_idx >= 5:
                print("ðŸ› Debug mode: Stopping after 5 batches")
                break

        # Average loss for epoch
        avg_epoch_loss = total_loss / len(self.train_loader)
        avg_action_loss = total_action_loss / len(self.train_loader) if total_action_loss > 0 else 0.0

        self.metrics['train']['loss'].append(avg_epoch_loss)
        self.metrics['train']['action_loss'].append(avg_action_loss)
        self.metrics['train']['learning_rate'].append(self.optimizer.param_groups[0]['lr'])

        return avg_epoch_loss, avg_action_loss

    def validate(self):
        """Validate model on validation set"""
        self.model.eval()

        total_loss = 0.0
        total_action_loss = 0.0
        skipped_batches = 0

        with torch.no_grad():
            pbar = tqdm(
                enumerate(self.val_loader),
                total=len(self.val_loader),
                desc="Validating",
                leave=True,
            )

            for batch_idx, batch in pbar:
                try:
                    images_t1 = batch['image_t1'].to(self.device)
                    images_t2 = batch['image_t2'].to(self.device)
                    action_targets = batch['action_vector'].to(self.device)
                    captions = batch['caption']

                    if self.config.USE_MIXED_PRECISION:
                        with autocast(dtype=torch.bfloat16):
                            outputs = self.model(
                                images_t1,
                                images_t2,
                                captions,
                                action_targets=action_targets,
                            )
                    else:
                        outputs = self.model(
                            images_t1,
                            images_t2,
                            captions,
                            action_targets=action_targets,
                        )

                    loss = outputs['total_loss'].item()
                    total_loss += loss

                    if 'action_loss' in outputs:
                        total_action_loss += outputs['action_loss'].item()

                    batch_count = batch_idx + 1 - skipped_batches
                    pbar.set_postfix({'loss': f'{loss:.4f}', 'skipped': skipped_batches})

                except Exception as e:
                    print(f"\nâš ï¸  éªŒè¯æ‰¹å¤„ç† {batch_idx} å‡ºçŽ°é”™è¯¯: {e}")
                    skipped_batches += 1
                    continue

        # è®¡ç®—å¹³å‡éªŒè¯æŸå¤±ï¼ŒæŽ’é™¤è·³è¿‡çš„æ‰¹æ¬¡
        valid_batches = len(self.val_loader) - skipped_batches
        if valid_batches > 0:
            avg_val_loss = total_loss / valid_batches
            avg_val_action_loss = total_action_loss / valid_batches if total_action_loss > 0 else 0.0
        else:
            avg_val_loss = float('inf')
            avg_val_action_loss = 0.0
            print(f"âš ï¸  æ‰€æœ‰éªŒè¯æ‰¹å¤„ç†éƒ½è¢«è·³è¿‡ï¼")

        self.metrics['val']['loss'].append(avg_val_loss)
        self.metrics['val']['action_loss'].append(avg_val_action_loss)

        return avg_val_loss, avg_val_action_loss

    def save_checkpoint(self, step: int = None, is_best: bool = False):
        """
        Save model checkpoint

        Args:
            step: Global training step
            is_best: Whether this is the best model
        """
        checkpoint_name = f"checkpoint_step_{step}.pt" if step else "checkpoint_latest.pt"
        checkpoint_path = self.checkpoint_dir / checkpoint_name

        checkpoint = {
            'epoch': self.epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'metrics': self.metrics,
            'config': {
                'max_epochs': self.config.MAX_EPOCHS,
                'batch_size': self.config.BATCH_SIZE,
                'learning_rate': self.config.LEARNING_RATE,
            }
        }

        torch.save(checkpoint, checkpoint_path)
        print(f"âœ… Checkpoint saved: {checkpoint_path}")

        # Also save best checkpoint
        if is_best or self.best_loss > (self.metrics['val']['loss'][-1] if self.metrics['val']['loss'] else float('inf')):
            best_path = self.checkpoint_dir / "checkpoint_best.pt"
            torch.save(checkpoint, best_path)
            self.best_loss = self.metrics['val']['loss'][-1] if self.metrics['val']['loss'] else float('inf')
            print(f"âœ… Best checkpoint updated: {best_path}")

    def save_metrics(self):
        """Save training metrics to JSON"""
        metrics_path = self.checkpoint_dir / "metrics.json"

        # Convert lists to summary stats for readability
        metrics_summary = {
            'train': {
                'final_loss': self.metrics['train']['loss'][-1] if self.metrics['train']['loss'] else None,
                'min_loss': min(self.metrics['train']['loss']) if self.metrics['train']['loss'] else None,
                'max_loss': max(self.metrics['train']['loss']) if self.metrics['train']['loss'] else None,
            },
            'val': {
                'final_loss': self.metrics['val']['loss'][-1] if self.metrics['val']['loss'] else None,
                'min_loss': min(self.metrics['val']['loss']) if self.metrics['val']['loss'] else None,
                'max_loss': max(self.metrics['val']['loss']) if self.metrics['val']['loss'] else None,
            }
        }

        with open(metrics_path, 'w') as f:
            json.dump(metrics_summary, f, indent=2)

        print(f"âœ… Metrics saved: {metrics_path}")

    def train(self):
        """Main training loop"""
        print(f"\n{'='*60}")
        print("Starting Training")
        print(f"{'='*60}\n")

        # Load data
        self.load_data()

        # Setup model
        self.setup_model()

        # Training loop
        start_time = time.time()

        for epoch in range(self.config.MAX_EPOCHS):
            self.epoch = epoch

            # Train epoch
            train_loss, train_action_loss = self.train_epoch()
            print(f"Epoch {epoch + 1} - Train Loss: {train_loss:.4f}, Action Loss: {train_action_loss:.4f}")

            # Validate
            val_loss, val_action_loss = self.validate()
            print(f"Epoch {epoch + 1} - Val Loss: {val_loss:.4f}, Action Loss: {val_action_loss:.4f}")

            # Save checkpoint
            is_best = val_loss < self.best_loss
            self.save_checkpoint(step=self.epoch, is_best=is_best)

            print(f"{'='*60}")

        # Final checkpoint
        self.save_checkpoint(step=self.global_step)
        self.save_metrics()

        elapsed_time = time.time() - start_time
        print(f"\n{'='*60}")
        print("Training Completed")
        print(f"{'='*60}")
        print(f"Total time: {elapsed_time / 3600:.2f} hours")
        print(f"Checkpoints saved in: {self.checkpoint_dir}")
        print(f"{'='*60}\n")


def main():
    """Main entry point"""
    # Print configuration
    Config.print_config()
    Config.verify_paths()

    # Create trainer and start training
    trainer = Trainer(config=Config)
    trainer.train()


if __name__ == "__main__":
    main()

