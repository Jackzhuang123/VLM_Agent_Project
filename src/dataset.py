"""
Dataset module for loading Arrow format data from HuggingFace datasets
Handles LEVIR-CC change detection dataset with bbox and caption
"""
import io
from pathlib import Path
from typing import Dict, Any, Tuple

import numpy as np
import torch
import torchvision.transforms as transforms
from PIL import Image
from torch.utils.data import Dataset

from .config import Config

# Try to import datasets library
try:
    import datasets
    DATASETS_AVAILABLE = True
except ImportError:
    DATASETS_AVAILABLE = False
    print("âš ï¸  Warning: datasets library not found. Install with: pip install datasets")


class LevirCCActionDataset(Dataset):
    """
    LEVIR-CC change detection dataset adapted for action prediction

    Loads data from HuggingFace Arrow format (.arrow files)
    Expected dataset structure:
    - 'image' or 'A': Temporal image 1
    - 'image2' or 'B': Temporal image 2
    - 'caption': Change description text
    - 'bbox': Bounding box [x1, y1, x2, y2] of change region
    """

    def __init__(
        self,
        dataset_split,
        image_size: int = 224,
        max_text_length: int = 128,
        normalize_bbox: bool = True,
    ):
        """
        Initialize LEVIR-CC dataset

        Args:
            dataset_split: HuggingFace dataset split (usually dataset['train'])
            image_size: Target size for image resizing
            max_text_length: Maximum length for tokenized text
            normalize_bbox: Whether to normalize bbox to [0, 1] range
        """
        if not DATASETS_AVAILABLE:
            raise ImportError("datasets library is required. Install with: pip install datasets")

        self.dataset = dataset_split
        self.image_size = image_size
        self.max_text_length = max_text_length
        self.normalize_bbox = normalize_bbox

        # Image preprocessing pipeline
        self.image_transform = transforms.Compose([
            transforms.Resize((image_size, image_size)),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.48145466, 0.4578275, 0.40821073],  # CLIP normalization
                std=[0.26862954, 0.26130258, 0.27577711]
            )
        ])

        # Inspect first sample to understand data structure
        self._inspect_data_structure()

        print(f"âœ… Dataset initialized with {len(self.dataset)} samples")

    def _inspect_data_structure(self):
        """Inspect the first sample to understand data structure and keys"""
        if len(self.dataset) == 0:
            raise ValueError("Dataset is empty!")

        # ç¦ç”¨æ ¼å¼åŒ–ä»¥é¿å…è‡ªåŠ¨è§£ç å›¾åƒ
        # è¿™æ ·å¯ä»¥è®¿é—®åŽŸå§‹çš„bytesæ•°æ®è€Œä¸æ˜¯å°è¯•ä»Žè·¯å¾„åŠ è½½
        with self.dataset.formatted_as(None):
            first_sample = self.dataset[0]

        print("\n" + "="*60)
        print("Dataset Structure Inspection")
        print("="*60)
        print(f"Dataset size: {len(self.dataset)}")
        print(f"Sample keys: {list(first_sample.keys())}")
        print(f"Sample data types:")
        for key, value in first_sample.items():
            if isinstance(value, (list, tuple)):
                print(f"  {key}: {type(value).__name__} of length {len(value)}")
            elif isinstance(value, dict) and 'bytes' in value:
                print(f"  {key}: dict with 'bytes' (image data)")
            else:
                print(f"  {key}: {type(value).__name__}")
        print("="*60 + "\n")

        # Store detected keys for later use
        self.image_key = self._detect_image_key(first_sample)
        self.image2_key = self._detect_image2_key(first_sample)
        self.caption_key = self._detect_caption_key(first_sample)
        self.bbox_key = self._detect_bbox_key(first_sample)

        print(f"Detected keys:")
        print(f"  Image 1 key: {self.image_key}")
        print(f"  Image 2 key: {self.image2_key}")
        print(f"  Caption key: {self.caption_key}")
        print(f"  BBox key: {self.bbox_key}\n")

    @staticmethod
    def _detect_image_key(sample: Dict) -> str:
        """Detect the key for temporal image 1"""
        candidates = ['image', 'A', 'img', 'image1']
        for key in candidates:
            if key in sample:
                return key
        raise KeyError(f"Could not find image key. Available keys: {list(sample.keys())}")

    @staticmethod
    def _detect_image2_key(sample: Dict) -> str:
        """Detect the key for temporal image 2"""
        candidates = ['image2', 'B', 'img2', 'image_2']
        for key in candidates:
            if key in sample:
                return key
        raise KeyError(f"Could not find image2 key. Available keys: {list(sample.keys())}")

    @staticmethod
    def _detect_caption_key(sample: Dict) -> str:
        """Detect the key for caption/description"""
        candidates = ['caption', 'text', 'description', 'change_description']
        for key in candidates:
            if key in sample:
                return key
        # Default to first non-image key if no caption found
        for key in sample.keys():
            if key not in ['image', 'A', 'img', 'image1', 'image2', 'B', 'img2', 'image_2', 'bbox', 'bboxes']:
                return key
        raise KeyError(f"Could not find caption key. Available keys: {list(sample.keys())}")

    @staticmethod
    def _detect_bbox_key(sample: Dict) -> str:
        """Detect the key for bounding box"""
        candidates = ['bbox', 'bboxes', 'bounding_box', 'box']
        for key in candidates:
            if key in sample:
                return key
        raise KeyError(f"Could not find bbox key. Available keys: {list(sample.keys())}")

    def __len__(self) -> int:
        """Return dataset size"""
        return len(self.dataset)

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        """
        Get a sample from the dataset

        Args:
            idx: Sample index

        Returns:
            Dictionary containing:
                - 'image_t1': Tensor of shape (3, H, W) - temporal image 1
                - 'image_t2': Tensor of shape (3, H, W) - temporal image 2
                - 'caption': String - change description
                - 'caption_ids': LongTensor - tokenized caption
                - 'action_vector': Tensor - normalized action vector [cx, cy, scale]
                - 'bbox': List - original bbox [x1, y1, x2, y2]
        """
        # ç¦ç”¨æ ¼å¼åŒ–ä»¥èŽ·å–åŽŸå§‹æ•°æ®ï¼ˆbytesï¼‰è€Œä¸æ˜¯å°è¯•ä»Žè·¯å¾„åŠ è½½
        with self.dataset.formatted_as(None):
            sample = self.dataset[idx]

        # Load images
        try:
            image_t1 = self._load_image(sample[self.image_key])
            image_t2 = self._load_image(sample[self.image2_key])
        except Exception as e:
            print(f"âŒ Error loading images at index {idx}: {e}")
            raise

        # Get caption
        try:
            caption = str(sample[self.caption_key])
            if not caption or caption.lower() in ['none', 'nan', '']:
                caption = "change detected"
        except Exception as e:
            print(f"âš ï¸  Error loading caption at index {idx}: {e}")
            caption = "change detected"

        # Get and process bbox
        try:
            bbox = sample[self.bbox_key]
            action_vector = self._process_bbox(bbox, image_t1.size)
        except Exception as e:
            print(f"âš ï¸  Warning: Could not process bbox at index {idx}: {e}")
            # Default action vector if bbox fails
            action_vector = torch.tensor([0.5, 0.5, 0.5], dtype=torch.float32)
            bbox = [0, 0, 224, 224]

        # Transform images
        image_t1 = self.image_transform(image_t1)
        image_t2 = self.image_transform(image_t2)

        return {
            'image_t1': image_t1,
            'image_t2': image_t2,
            'caption': caption,
            'action_vector': action_vector,
            'bbox': bbox,
            'index': idx,
        }

    @staticmethod
    def _load_image(image_data) -> Image.Image:
        """
        Load image from various formats

        Supports:
        - PIL Image objects
        - Bytes (encoded images)
        - Paths (string or Path)
        - HuggingFace Image dict with 'bytes' key
        """
        if isinstance(image_data, Image.Image):
            return image_data.convert('RGB')
        elif isinstance(image_data, dict) and 'bytes' in image_data:
            # HuggingFace Image feature format
            return Image.open(io.BytesIO(image_data['bytes'])).convert('RGB')
        elif isinstance(image_data, bytes):
            return Image.open(io.BytesIO(image_data)).convert('RGB')
        elif isinstance(image_data, (str, Path)):
            return Image.open(image_data).convert('RGB')
        else:
            raise TypeError(f"Unsupported image format: {type(image_data)}, keys: {image_data.keys() if isinstance(image_data, dict) else 'N/A'}")

    def _process_bbox(self, bbox, image_size: Tuple[int, int]) -> torch.Tensor:
        """
        Process bounding box to action vector

        Converts [x1, y1, x2, y2] to normalized [cx, cy, scale]
        where:
        - cx, cy: center point (normalized to [0, 1])
        - scale: relative size of bbox (normalized to [0, 1])

        Args:
            bbox: List/Tuple of [x1, y1, x2, y2]
            image_size: Tuple of (width, height)

        Returns:
            Tensor of shape (3,) with normalized action vector
        """
        try:
            # Parse bbox
            if isinstance(bbox, (list, tuple)) and len(bbox) == 4:
                x1, y1, x2, y2 = bbox
            else:
                # Fallback for different bbox formats
                raise ValueError(f"Invalid bbox format: {bbox}")

            # Ensure valid bbox
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(image_size[0], x2), min(image_size[1], y2)

            if x2 <= x1 or y2 <= y1:
                # Invalid bbox, return center point with small scale
                return torch.tensor([0.5, 0.5, 0.1], dtype=torch.float32)

            # Calculate center point
            cx = (x1 + x2) / (2.0 * image_size[0])
            cy = (y1 + y2) / (2.0 * image_size[1])

            # Calculate scale (relative bbox size)
            width = (x2 - x1) / image_size[0]
            height = (y2 - y1) / image_size[1]
            scale = np.sqrt(width * height)  # Geometric mean of width and height

            # Clip to valid range
            cx = np.clip(cx, 0.0, 1.0)
            cy = np.clip(cy, 0.0, 1.0)
            scale = np.clip(scale, 0.0, 1.0)

            return torch.tensor([cx, cy, scale], dtype=torch.float32)

        except Exception as e:
            print(f"âš ï¸  Error processing bbox {bbox}: {e}")
            return torch.tensor([0.5, 0.5, 0.5], dtype=torch.float32)


def load_raw_levir_cc_dataset(dataset_path: str):
    """
    ä»ŽåŽŸå§‹LEVIR-CCæ–‡ä»¶ç»“æž„æˆ–å•ä¸ªArrowæ–‡ä»¶åŠ è½½æ•°æ®é›†

    æ”¯æŒçš„ç»“æž„:
    1. å•ä¸ª .arrow æ–‡ä»¶: LEVIR-CC/levir-cc-train.arrow
    2. å›¾åƒç›®å½•ç»“æž„: LEVIR-CC/images/train/A å’Œ B
    3. ç®€åŒ–ç›®å½•ç»“æž„: LEVIR-CC/A å’Œ B

    Args:
        dataset_path: æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„

    Returns:
        åŒ…å«å›¾åƒè·¯å¾„å’Œæ ‡æ³¨çš„æ•°æ®å­—å…¸åˆ—è¡¨
    """
    from pathlib import Path
    import pyarrow as pa

    print(f"\nðŸ”„ å°è¯•ä»ŽåŽŸå§‹ç»“æž„åŠ è½½æ•°æ®é›†: {dataset_path}")

    dataset_path = Path(dataset_path)

    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ .arrow æ–‡ä»¶
    arrow_files = list(dataset_path.glob('*.arrow'))
    if arrow_files:
        print(f"âœ… æ‰¾åˆ° Arrow æ–‡ä»¶: {arrow_files[0].name}")
        try:
            # å°è¯•ç”¨ pyarrow ç›´æŽ¥è¯»å–
            import pyarrow.ipc as ipc
            with pa.memory_map(str(arrow_files[0]), 'r') as source:
                reader = ipc.open_file(source)
                table = reader.read_all()

            print(f"âœ… Arrow æ–‡ä»¶è¯»å–æˆåŠŸ")
            print(f"   åˆ—: {table.column_names}")
            print(f"   è¡Œæ•°: {len(table)}")

            # è½¬æ¢ä¸º HuggingFace Dataset
            dataset = datasets.Dataset(table)
            return dataset

        except Exception as e:
            print(f"âš ï¸  Arrow æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
            print(f"ðŸ”„ å°è¯•ç”¨ datasets.load_dataset...")

            try:
                # å°è¯•ç”¨ datasets åº“åŠ è½½
                dataset = datasets.load_dataset('arrow', data_files=str(arrow_files[0]), split='train')
                print(f"âœ… ä½¿ç”¨ datasets åº“åŠ è½½æˆåŠŸ: {len(dataset)} ä¸ªæ ·æœ¬")
                return dataset
            except Exception as e2:
                print(f"âš ï¸  datasets åº“åŠ è½½ä¹Ÿå¤±è´¥: {e2}")

    # å¦‚æžœæ²¡æœ‰ Arrow æ–‡ä»¶ï¼Œå°è¯•ä»Žå›¾åƒç›®å½•åŠ è½½
    print(f"ðŸ”„ æŸ¥æ‰¾å›¾åƒç›®å½•ç»“æž„...")

    # æ£€æŸ¥å¯èƒ½çš„ç›®å½•ç»“æž„
    possible_structures = [
        # ç»“æž„1: LEVIR-CC/images/train/A, B
        {
            'img_a': dataset_path / 'images' / 'train' / 'A',
            'img_b': dataset_path / 'images' / 'train' / 'B',
        },
        # ç»“æž„2: LEVIR-CC/A, B
        {
            'img_a': dataset_path / 'A',
            'img_b': dataset_path / 'B',
        },
        # ç»“æž„3: LEVIR-CC/train/A, B
        {
            'img_a': dataset_path / 'train' / 'A',
            'img_b': dataset_path / 'train' / 'B',
        },
    ]

    # æ‰¾åˆ°å­˜åœ¨çš„ç»“æž„
    valid_structure = None
    for structure in possible_structures:
        if structure['img_a'].exists() and structure['img_b'].exists():
            valid_structure = structure
            print(f"âœ… æ‰¾åˆ°æœ‰æ•ˆç»“æž„:")
            print(f"   å›¾åƒAç›®å½•: {structure['img_a']}")
            print(f"   å›¾åƒBç›®å½•: {structure['img_b']}")
            break

    if valid_structure is None:
        # åˆ—å‡ºå®žé™…å­˜åœ¨çš„æ–‡ä»¶/ç›®å½•
        print(f"\nðŸ“ å®žé™…ç›®å½•å†…å®¹:")
        if dataset_path.exists():
            for item in dataset_path.iterdir():
                if item.is_dir():
                    print(f"   ðŸ“ {item.name}/")
                else:
                    print(f"   ðŸ“„ {item.name}")

        raise FileNotFoundError(
            f"æ— æ³•åœ¨ {dataset_path} ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„LEVIR-CCæ•°æ®ç»“æž„ã€‚\n"
            f"é¢„æœŸç»“æž„:\n"
            f"  - LEVIR-CC/*.arrow æ–‡ä»¶\n"
            f"  - LEVIR-CC/images/train/A å’Œ B\n"
            f"  - LEVIR-CC/A å’Œ B"
        )

    # èŽ·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
    img_a_files = sorted(list(valid_structure['img_a'].glob('*.png')) +
                        list(valid_structure['img_a'].glob('*.jpg')))
    img_b_files = sorted(list(valid_structure['img_b'].glob('*.png')) +
                        list(valid_structure['img_b'].glob('*.jpg')))

    print(f"âœ… æ‰¾åˆ° {len(img_a_files)} å¯¹å›¾åƒ")

    # æž„å»ºæ•°æ®é›†ï¼ˆä½¿ç”¨é»˜è®¤æ ‡æ³¨ï¼‰
    dataset_list = []
    for img_a_path, img_b_path in zip(img_a_files, img_b_files):
        dataset_list.append({
            'A': str(img_a_path),
            'B': str(img_b_path),
            'caption': 'A change has occurred in the remote sensing image.',
            'bbox': [0, 0, 256, 256],  # é»˜è®¤bboxï¼Œå°†åœ¨åŽç»­è¢«å½’ä¸€åŒ–
        })

    return dataset_list


def create_dataloaders(
    batch_size: int = 4,
    num_workers: int = 4,
    test_split: float = 0.1,
    seed: int = 42,
):
    """
    Create train and validation dataloaders from LEVIR-CC dataset

    Args:
        batch_size: Batch size for dataloaders
        num_workers: Number of workers for data loading
        test_split: Proportion of data to use for validation
        seed: Random seed for train/test split

    Returns:
        Tuple of (train_dataloader, val_dataloader)
    """
    if not DATASETS_AVAILABLE:
        raise ImportError("datasets library is required. Install with: pip install datasets")

    print("\n" + "="*60)
    print("åŠ è½½ LEVIR-CC æ•°æ®é›†")
    print("="*60)
    print(f"æ•°æ®é›†è·¯å¾„: {Config.DATASET_PATH}")

    try:
        # é¦–å…ˆå°è¯•ä»ŽArrowæ ¼å¼åŠ è½½
        full_dataset = datasets.load_from_disk(Config.DATASET_PATH)
        print(f"âœ… ä»ŽArrowæ ¼å¼åŠ è½½æˆåŠŸ")
        print(f"   æ•°æ®é›†ç»“æž„: {full_dataset}")

        # Get the appropriate split
        if 'train' in full_dataset:
            dataset_split = full_dataset['train']
            print(f"âœ… ä½¿ç”¨ 'train' åˆ†å‰²ï¼Œå…± {len(dataset_split)} ä¸ªæ ·æœ¬")
        else:
            # If only one split exists, use it
            dataset_split = full_dataset
            print(f"âš ï¸  æœªæ‰¾åˆ° 'train' åˆ†å‰²ã€‚ä½¿ç”¨æ•´ä¸ªæ•°æ®é›†ï¼Œå…± {len(dataset_split)} ä¸ªæ ·æœ¬")

    except (FileNotFoundError, Exception) as e:
        print(f"âš ï¸  Arrowæ ¼å¼åŠ è½½å¤±è´¥: {e}")
        print(f"ðŸ”„ å°è¯•ä»ŽåŽŸå§‹æ–‡ä»¶ç»“æž„åŠ è½½...")

        try:
            # ä»ŽåŽŸå§‹ç»“æž„åŠ è½½ï¼ˆå¯èƒ½è¿”å›ž Dataset æˆ– listï¼‰
            raw_data = load_raw_levir_cc_dataset(Config.DATASET_PATH)

            # æ£€æŸ¥è¿”å›žç±»åž‹
            if isinstance(raw_data, datasets.Dataset):
                # å·²ç»æ˜¯ Dataset å¯¹è±¡ï¼ˆä»Ž Arrow æ–‡ä»¶åŠ è½½ï¼‰
                dataset_split = raw_data
                print(f"âœ… ä»Ž Arrow æ–‡ä»¶åŠ è½½æˆåŠŸï¼Œå…± {len(dataset_split)} ä¸ªæ ·æœ¬")
            else:
                # æ˜¯åˆ—è¡¨ï¼ˆä»Žå›¾åƒç›®å½•åŠ è½½ï¼‰
                # è½¬æ¢ä¸ºHuggingFace Datasetæ ¼å¼
                dataset_dict = {
                    'A': [item['A'] for item in raw_data],
                    'B': [item['B'] for item in raw_data],
                    'caption': [item['caption'] for item in raw_data],
                    'bbox': [item['bbox'] for item in raw_data],
                }

                dataset_split = datasets.Dataset.from_dict(dataset_dict)
                print(f"âœ… ä»Žå›¾åƒç›®å½•åŠ è½½æˆåŠŸï¼Œå…± {len(dataset_split)} ä¸ªæ ·æœ¬")

        except Exception as raw_e:
            print(f"âŒ åŽŸå§‹ç»“æž„åŠ è½½ä¹Ÿå¤±è´¥: {raw_e}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(
                f"æ— æ³•åŠ è½½æ•°æ®é›†ã€‚å°è¯•äº†:\n"
                f"1. Arrowæ ¼å¼ (load_from_disk): {e}\n"
                f"2. åŽŸå§‹ç»“æž„/å•Arrowæ–‡ä»¶: {raw_e}\n\n"
                f"è¯·æ£€æŸ¥æ•°æ®é›†è·¯å¾„å’Œç»“æž„æ˜¯å¦æ­£ç¡®ã€‚"
            )

    # Create train/val split
    split_dataset = dataset_split.train_test_split(
        test_size=test_split,
        seed=seed
    )

    train_dataset = LevirCCActionDataset(
        split_dataset['train'],
        image_size=Config.IMAGE_SIZE,
        max_text_length=Config.MAX_TEXT_LENGTH,
        normalize_bbox=Config.BBOX_NORMALIZE,
    )

    val_dataset = LevirCCActionDataset(
        split_dataset['test'],
        image_size=Config.IMAGE_SIZE,
        max_text_length=Config.MAX_TEXT_LENGTH,
        normalize_bbox=Config.BBOX_NORMALIZE,
    )

    # Create dataloaders
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True,
    )

    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=False,
    )

    print(f"\nâœ… Dataloaders created:")
    print(f"   Train: {len(train_loader)} batches ({len(train_dataset)} samples)")
    print(f"   Val: {len(val_loader)} batches ({len(val_dataset)} samples)")
    print("="*60 + "\n")

    return train_loader, val_loader


if __name__ == "__main__":
    # Test dataset loading
    print("Testing dataset loading...")
    train_loader, val_loader = create_dataloaders(batch_size=2)

    # Get a sample batch
    for batch_idx, batch in enumerate(train_loader):
        print(f"\nBatch {batch_idx}:")
        for key, value in batch.items():
            if isinstance(value, torch.Tensor):
                print(f"  {key}: {value.shape}")
            else:
                print(f"  {key}: {type(value).__name__}")
        break  # Only show first batch

