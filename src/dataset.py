"""
Dataset module for loading Arrow format data from HuggingFace datasets
Handles LEVIR-CC change detection dataset with bbox and caption
Supports multiple data structures from Kaggle and local environments
"""
import io
from pathlib import Path
from typing import Dict, Any, Tuple

import numpy as np
import torch
import torchvision.transforms as transforms
from PIL import Image
from torch.utils.data import Dataset

from .config import Config

# Try to import datasets library
try:
    import datasets
    DATASETS_AVAILABLE = True
except ImportError:
    DATASETS_AVAILABLE = False
    print("âš ï¸  Warning: datasets library not found. Install with: pip install datasets")


class LevirCCActionDataset(Dataset):
    """
    LEVIR-CC å˜åŒ–æ£€æµ‹æ•°æ®é›†ï¼ˆé€‚é…åŠ¨ä½œé¢„æµ‹ä»»åŠ¡ï¼‰

    åŠŸèƒ½è¯´æ˜ï¼š
        - ä»å¤šç§æ ¼å¼åŠ è½½æ•°æ®ï¼ˆArrowã€åŸå§‹å›¾åƒã€JSON æ ‡æ³¨ï¼‰
        - è‡ªåŠ¨æ£€æµ‹æ•°æ®ä¸­çš„å›¾åƒã€æ–‡æœ¬å’Œè¾¹ç•Œæ¡†å­—æ®µ
        - è¿›è¡Œå›¾åƒé¢„å¤„ç†å’Œè¾¹ç•Œæ¡†è½¬æ¢
        - æ”¯æŒåœ¨å¤šè¿›ç¨‹ DataLoader ä¸­è¿è¡Œ

    æ”¯æŒçš„æ•°æ®å­—æ®µï¼š
        - å›¾åƒ1: 'image', 'A', 'img', 'image1'
        - å›¾åƒ2: 'image2', 'B', 'img2', 'image_2'
        - æ–‡æœ¬: 'caption', 'text', 'description', 'label'
        - è¾¹ç•Œæ¡†: 'bbox', 'bboxes', 'bounding_box', 'box'

    æ•°æ®å¤„ç†æµç¨‹ï¼š
        1. åŠ è½½å¹¶è‡ªåŠ¨æ£€æµ‹å­—æ®µ
        2. ç¦ç”¨ HuggingFace è‡ªåŠ¨è§£ç ï¼ˆé¿å…å¤šè¿›ç¨‹ç¼“å­˜é—®é¢˜ï¼‰
        3. å›¾åƒé¢„å¤„ç†ï¼šResize(224) â†’ Normalize(CLIP)
        4. è¾¹ç•Œæ¡†è½¬æ¢ï¼š[x1,y1,x2,y2] â†’ [cx,cy,scale]
    """

    def __init__(
        self,
        dataset_split,
        image_size: int = 224,
        max_text_length: int = 128,
        normalize_bbox: bool = True,
    ):
        """
        åˆå§‹åŒ– LEVIR-CC æ•°æ®é›†

        åˆå§‹åŒ–æ­¥éª¤ï¼š
        1. éªŒè¯ datasets åº“å¯ç”¨æ€§
        2. è®¾ç½®å›¾åƒé¢„å¤„ç†æµç¨‹
        3. ç¦ç”¨ HuggingFace è‡ªåŠ¨è§£ç 
        4. æ£€æµ‹æ•°æ®ç»“æ„å’Œå­—æ®µåç§°

        Args:
            dataset_split: HuggingFace æ•°æ®é›†åˆ†å‰²å¯¹è±¡ï¼ˆé€šå¸¸ä¸º dataset['train']ï¼‰
            image_size (int): å›¾åƒå¤§å°ï¼ˆé»˜è®¤ 224ï¼ŒCLIP è¦æ±‚ï¼‰
            max_text_length (int): æ–‡æœ¬æœ€å¤§é•¿åº¦ï¼ˆé»˜è®¤ 128ï¼‰
            normalize_bbox (bool): æ˜¯å¦å½’ä¸€åŒ–è¾¹ç•Œæ¡†åˆ° [0, 1]ï¼ˆé»˜è®¤ Trueï¼‰
        """
        if not DATASETS_AVAILABLE:
            raise ImportError("datasets library is required. Install with: pip install datasets")

        self.dataset = dataset_split
        self.image_size = image_size
        self.max_text_length = max_text_length
        self.normalize_bbox = normalize_bbox

        # Image preprocessing pipeline
        self.image_transform = transforms.Compose([
            transforms.Resize((image_size, image_size)),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.48145466, 0.4578275, 0.40821073],  # CLIP normalization
                std=[0.26862954, 0.26130258, 0.27577711]
            )
        ])

        # æ°¸ä¹…æ€§åœ°ç¦ç”¨è‡ªåŠ¨è§£ç ä»¥é¿å…ç¼“å­˜æ–‡ä»¶åŠ è½½é”™è¯¯
        # è¿™åœ¨å¤šè¿›ç¨‹ DataLoader ä¸­æ˜¯å¿…éœ€çš„
        self._disable_auto_decoding()

        # Inspect first sample to understand data structure
        self._inspect_data_structure()

        print(f"âœ… Dataset initialized with {len(self.dataset)} samples")

    def _disable_auto_decoding(self):
        """
        æ°¸ä¹…æ€§åœ°ç¦ç”¨è‡ªåŠ¨è§£ç ï¼Œç‰¹åˆ«æ˜¯å¯¹äº Image ç±»å‹å­—æ®µ

        ä¸ºäº†åœ¨ DataLoader å¤šè¿›ç¨‹ä¸­ä¹Ÿèƒ½å·¥ä½œï¼Œæˆ‘ä»¬å°† HuggingFace Dataset è½¬æ¢ä¸º
        ä¸€ä¸ªç®€å•çš„åˆ—è¡¨ç»“æ„ï¼Œé¿å… HuggingFace çš„è‡ªåŠ¨è§£ç æœºåˆ¶ã€‚

        å…³é”®ç­–ç•¥ï¼šä¸è®¿é—®åŒ…å« Image å­—æ®µçš„æ•°æ®ï¼ˆè¿™ä¼šè§¦å‘è‡ªåŠ¨è§£ç ï¼‰ï¼Œè€Œæ˜¯ï¼š
        1. è·å–åŸå§‹ PyArrow Table
        2. ç¦ç”¨æ‰€æœ‰åˆ—çš„è§£ç å™¨
        3. è½¬æ¢ä¸ºåˆ—è¡¨
        """
        try:
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯ IndexedDataset åŒ…è£…å¯¹è±¡
            if hasattr(self.dataset, '__class__') and self.dataset.__class__.__name__ == 'IndexedDataset':
                print(f"â„¹ï¸  æ£€æµ‹åˆ° IndexedDataset åŒ…è£…å¯¹è±¡ï¼Œè·³è¿‡è‡ªåŠ¨è§£ç ç¦ç”¨")
                return

            # æ£€æŸ¥æ˜¯å¦æœ‰ Image ç±»å‹å­—æ®µ
            if hasattr(self.dataset, 'features'):
                from datasets.features import Image as HFImage
                has_image = any(isinstance(feature, HFImage) for feature in self.dataset.features.values())

                if has_image:
                    print(f"âš ï¸  æ£€æµ‹åˆ° Image ç±»å‹å­—æ®µï¼Œç¦ç”¨è‡ªåŠ¨è§£ç ...")

                    # å…³é”®ï¼šç¦ç”¨æ‰€æœ‰çš„ç‰¹å¾è§£ç å™¨ï¼Œé˜²æ­¢è‡ªåŠ¨è§£ç è§¦å‘
                    self.dataset._format_type = None
                    self.dataset._format_kwargs = {}
                    self.dataset._format_columns = None

                    # ä½¿ç”¨ pyarrow æ“ä½œé¿å…è§¦å‘è§£ç 
                    print(f"ğŸ”„ æ­£åœ¨å°†æ•°æ®é›†è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼...")

                    # è·å–åŸå§‹ pyarrow table
                    table = self.dataset.data

                    # å°† pyarrow table è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                    dataset_list = []
                    for i in range(len(table)):
                        row_dict = {}
                        for col_name in table.column_names:
                            # ä» pyarrow ç›´æ¥è·å–æ•°æ®ï¼Œä¸é€šè¿‡ HF çš„è§£ç æœºåˆ¶
                            col_data = table[col_name][i].as_py()
                            row_dict[col_name] = col_data
                        dataset_list.append(row_dict)

                    # ç”¨åˆ—è¡¨æ›¿æ¢ HuggingFace Dataset
                    self.dataset = dataset_list
                    print(f"âœ… å·²è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼ˆ{len(self.dataset)} æ ·æœ¬ï¼‰")
        except Exception as e:
            print(f"âš ï¸  ç¦ç”¨è‡ªåŠ¨è§£ç å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            print(f"ğŸ”„ å°†ç»§ç»­ä½¿ç”¨åŸå§‹æ•°æ®é›†ï¼Œå¯èƒ½ä¼šæœ‰ç¼“å­˜é—®é¢˜")

    def _inspect_data_structure(self):
        """Inspect the first sample to understand data structure and keys"""
        if len(self.dataset) == 0:
            raise ValueError("Dataset is empty!")

        # è·å–ç¬¬ä¸€ä¸ªæ ·æœ¬
        first_sample = self.dataset[0]

        print("\n" + "="*60)
        print("Dataset Structure Inspection")
        print("="*60)
        print(f"Dataset size: {len(self.dataset)}")
        print(f"Sample keys: {list(first_sample.keys())}")
        print(f"Sample data types:")
        for key, value in first_sample.items():
            if value is None:
                print(f"  {key}: None (detected from schema)")
            elif isinstance(value, (list, tuple)):
                print(f"  {key}: {type(value).__name__} of length {len(value)}")
            elif isinstance(value, dict) and 'bytes' in value:
                print(f"  {key}: dict with 'bytes' (image data)")
            else:
                print(f"  {key}: {type(value).__name__}")
        print("="*60 + "\n")

        # Store detected keys for later use
        self.image_key = self._detect_image_key(first_sample)
        self.image2_key = self._detect_image2_key(first_sample)
        self.caption_key = self._detect_caption_key(first_sample)
        self.bbox_key = self._detect_bbox_key(first_sample)

        print(f"Detected keys:")
        print(f"  Image 1 key: {self.image_key}")
        print(f"  Image 2 key: {self.image2_key}")
        print(f"  Caption key: {self.caption_key}")
        print(f"  BBox key: {self.bbox_key}\n")

        # æ£€æŸ¥å…³é”®å­—æ®µç¼ºå¤±å¹¶ç»™å‡ºè­¦å‘Š
        self._check_critical_fields()

    @staticmethod
    def _detect_image_key(sample: Dict) -> str:
        """Detect the key for temporal image 1"""
        candidates = ['image', 'A', 'img', 'image1']
        for key in candidates:
            if key in sample:
                return key
        raise KeyError(f"Could not find image key. Available keys: {list(sample.keys())}")

    @staticmethod
    def _detect_image2_key(sample: Dict) -> str:
        """Detect the key for temporal image 2"""
        candidates = ['image2', 'B', 'img2', 'image_2']
        for key in candidates:
            if key in sample:
                return key
        # å¦‚æœåªæœ‰å•å¼ å›¾åƒï¼Œè¿”å›ç›¸åŒçš„å›¾åƒé”®
        # è¿™æ · image_t2 å°±ä¼šé‡å¤ä½¿ç”¨åŒä¸€å¼ å›¾åƒ
        for key in ['image', 'A', 'img', 'image1']:
            if key in sample:
                print(f"âš ï¸  æœªæ‰¾åˆ° image2 é”®ï¼Œå°†ä½¿ç”¨ '{key}' ä½œä¸º image2ï¼ˆé‡å¤ä½¿ç”¨åŒä¸€å¼ å›¾åƒï¼‰")
                return key
        raise KeyError(f"Could not find image2 key. Available keys: {list(sample.keys())}")

    @staticmethod
    def _detect_caption_key(sample: Dict) -> str:
        """Detect the key for caption/description"""
        candidates = ['caption', 'text', 'description', 'change_description', 'label']
        for key in candidates:
            if key in sample:
                if key == 'label':
                    print(f"âš ï¸  æœªæ‰¾åˆ° caption é”®ï¼Œå°†ä½¿ç”¨ 'label' å­—æ®µ")
                return key
        # Default to first non-image key if no caption found
        for key in sample.keys():
            if key not in ['image', 'A', 'img', 'image1', 'image2', 'B', 'img2', 'image_2', 'bbox', 'bboxes']:
                print(f"âš ï¸  æœªæ‰¾åˆ° caption é”®ï¼Œä½¿ç”¨é»˜è®¤çš„ç¬¬ä¸€ä¸ªéå›¾åƒé”®: '{key}'")
                return key
        # å¦‚æœå®Œå…¨æ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›ä¸€ä¸ªé»˜è®¤å€¼
        print(f"âš ï¸  æœªæ‰¾åˆ°ä»»ä½• caption é”®ï¼Œå°†ä½¿ç”¨é»˜è®¤æè¿°")
        return None

    @staticmethod
    def _detect_bbox_key(sample: Dict) -> str:
        """Detect the key for bounding box"""
        candidates = ['bbox', 'bboxes', 'bounding_box', 'box']
        for key in candidates:
            if key in sample:
                return key
        # å¦‚æœæ²¡æœ‰ bboxï¼Œè¿”å› Noneï¼Œç¨åä¼šä½¿ç”¨é»˜è®¤å€¼
        print(f"âš ï¸  æœªæ‰¾åˆ° bbox é”®ï¼Œå°†ä½¿ç”¨é»˜è®¤çš„å…¨å›¾ bbox")
        return None

    def __len__(self) -> int:
        """Return dataset size"""
        return len(self.dataset)

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        """
        Get a sample from the dataset

        Args:
            idx: Sample index

        Returns:
            Dictionary containing:
                - 'image_t1': Tensor of shape (3, H, W) - temporal image 1
                - 'image_t2': Tensor of shape (3, H, W) - temporal image 2
                - 'caption': String - change description
                - 'caption_ids': LongTensor - tokenized caption
                - 'action_vector': Tensor - normalized action vector [cx, cy, scale]
                - 'bbox': List - original bbox [x1, y1, x2, y2]
        """
        # ç›´æ¥è®¿é—®æ•°æ®é›†ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»åœ¨ __init__ ä¸­ç¦ç”¨äº†è‡ªåŠ¨è§£ç 
        sample = self.dataset[idx]

        # Load images
        try:
            image_t1 = self._load_image(sample[self.image_key])
            image_t2 = self._load_image(sample[self.image2_key])
        except Exception as e:
            print(f"âŒ Error loading images at index {idx}: {e}")
            raise

        # Get caption
        try:
            if self.caption_key is None:
                caption = "change detected"
            else:
                caption = str(sample[self.caption_key])
                # å¦‚æœ caption æ˜¯æ•°å­—ï¼ˆlabelï¼‰ï¼Œè½¬æ¢ä¸ºæ–‡æœ¬æè¿°
                if caption.isdigit():
                    caption = f"class {caption}"
                elif not caption or caption.lower() in ['none', 'nan', '']:
                    caption = "change detected"
        except Exception as e:
            print(f"âš ï¸  Error loading caption at index {idx}: {e}")
            caption = "change detected"

        # Get and process bbox
        try:
            if self.bbox_key is None:
                # ä½¿ç”¨é»˜è®¤çš„å…¨å›¾ bbox
                bbox = [0, 0, image_t1.size[0], image_t1.size[1]]
                action_vector = torch.tensor([0.5, 0.5, 1.0], dtype=torch.float32)
            else:
                bbox = sample[self.bbox_key]
                action_vector = self._process_bbox(bbox, image_t1.size)
        except Exception as e:
            print(f"âš ï¸  Warning: Could not process bbox at index {idx}: {e}")
            # Default action vector if bbox fails
            action_vector = torch.tensor([0.5, 0.5, 0.5], dtype=torch.float32)
            bbox = [0, 0, image_t1.size[0], image_t1.size[1]]

        # Transform images
        image_t1 = self.image_transform(image_t1)
        image_t2 = self.image_transform(image_t2)

        return {
            'image_t1': image_t1,
            'image_t2': image_t2,
            'caption': caption,
            'action_vector': action_vector,
            'bbox': bbox,
            'index': idx,
        }

    @staticmethod
    def _load_image(image_data) -> Image.Image:
        """
        Load image from various formats

        Supports:
        - PIL Image objects
        - Bytes (encoded images)
        - Paths (string or Path)
        - HuggingFace Image dict with 'bytes' key
        """
        if isinstance(image_data, Image.Image):
            return image_data.convert('RGB')
        elif isinstance(image_data, dict) and 'bytes' in image_data:
            # HuggingFace Image feature format
            return Image.open(io.BytesIO(image_data['bytes'])).convert('RGB')
        elif isinstance(image_data, bytes):
            return Image.open(io.BytesIO(image_data)).convert('RGB')
        elif isinstance(image_data, (str, Path)):
            return Image.open(image_data).convert('RGB')
        else:
            raise TypeError(f"Unsupported image format: {type(image_data)}, keys: {image_data.keys() if isinstance(image_data, dict) else 'N/A'}")

    def _process_bbox(self, bbox, image_size: Tuple[int, int]) -> torch.Tensor:
        """
        Process bounding box to action vector

        Converts [x1, y1, x2, y2] to normalized [cx, cy, scale]
        where:
        - cx, cy: center point (normalized to [0, 1])
        - scale: relative size of bbox (normalized to [0, 1])

        Args:
            bbox: List/Tuple of [x1, y1, x2, y2]
            image_size: Tuple of (width, height)

        Returns:
            Tensor of shape (3,) with normalized action vector
        """
        try:
            # Parse bbox
            if isinstance(bbox, (list, tuple)) and len(bbox) == 4:
                x1, y1, x2, y2 = bbox
            else:
                # Fallback for different bbox formats
                raise ValueError(f"Invalid bbox format: {bbox}")

            # Ensure valid bbox
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(image_size[0], x2), min(image_size[1], y2)

            if x2 <= x1 or y2 <= y1:
                # Invalid bbox, return center point with small scale
                return torch.tensor([0.5, 0.5, 0.1], dtype=torch.float32)

            # Calculate center point
            cx = (x1 + x2) / (2.0 * image_size[0])
            cy = (y1 + y2) / (2.0 * image_size[1])

            # Calculate scale (relative bbox size)
            width = (x2 - x1) / image_size[0]
            height = (y2 - y1) / image_size[1]
            scale = np.sqrt(width * height)  # Geometric mean of width and height

            # Clip to valid range
            cx = np.clip(cx, 0.0, 1.0)
            cy = np.clip(cy, 0.0, 1.0)
            scale = np.clip(scale, 0.0, 1.0)

            return torch.tensor([cx, cy, scale], dtype=torch.float32)

        except Exception as e:
            print(f"âš ï¸  Error processing bbox {bbox}: {e}")
            return torch.tensor([0.5, 0.5, 0.5], dtype=torch.float32)


def load_captions_from_json(json_path: str) -> Dict[str, Dict]:
    """
    ä» JSON æ ‡æ³¨æ–‡ä»¶åŠ è½½å›¾åƒå¯¹åº”çš„æ ‡æ³¨

    åŠŸèƒ½ï¼š
        - æ”¯æŒ LEVIR-CC æ•°æ®é›†çš„æ ‡æ³¨æ ¼å¼
        - å¤„ç†å¤šä¸ªåµŒå¥—æ•°ç»„çš„ JSON æ–‡ä»¶
        - æå–æ ‡æ³¨æ–‡æœ¬å’Œå˜åŒ–æ ‡å¿—
        - ä¼˜åŒ–ï¼šé¿å…è´ªå¿ƒæ­£åˆ™è¡¨è¾¾å¼ï¼Œä½¿ç”¨æµå¼è§£æ

    JSON ç»“æ„ï¼š
        åŒ…å« 'filename', 'sentences', 'changeflag' å­—æ®µçš„å›¾åƒå…ƒæ•°æ®åˆ—è¡¨
        å¤šä¸ªæ•°ç»„å¯ä»¥æ‹¼æ¥åœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­

    Args:
        json_path (str): JSON æ ‡æ³¨æ–‡ä»¶è·¯å¾„

    Returns:
        Dict[str, Dict]: å­—å…¸æ˜ å°„ {å›¾åƒåç§° -> {caption, changeflag}}

    ç¤ºä¾‹ï¼š
        {
            'train_000001.png': {
                'caption': 'some building constructed...',
                'changeflag': 1
            }
        }
    """
    import json

    captions_dict = {}  # å­˜å‚¨æ ‡æ³¨å­—å…¸

    try:
        print(f"ğŸ“‚ åŠ è½½ JSON æ ‡æ³¨: {json_path}")
        with open(json_path, 'r', encoding='utf-8') as f:
            # å°è¯•åŠ è½½ä¸ºåˆ—è¡¨ï¼ˆå¦‚æœæ˜¯å¤šä¸ªæ•°ç»„ï¼‰
            content = f.read().strip()

            data = []
            if content.startswith('['):
                # å•ä¸ªæ•°ç»„ - ç›´æ¥è§£æ
                try:
                    data = json.loads(content)
                except json.JSONDecodeError:
                    print(f"âš ï¸  ç¬¬ä¸€æ¬¡å°è¯• JSON è§£æå¤±è´¥ï¼Œå°è¯•æµå¼è§£æ...")
                    # å¦‚æœå¤±è´¥ï¼Œå°è¯•æµå¼è§£æ
                    data = _parse_json_arrays_streaming(content)
            else:
                # å¯èƒ½æ˜¯å¤šä¸ªæ•°ç»„æ‹¼æ¥ - ä½¿ç”¨æµå¼è§£æï¼ˆé¿å…è´ªå¿ƒæ­£åˆ™ï¼‰
                data = _parse_json_arrays_streaming(content)

        print(f"â„¹ï¸  JSON æ•°æ®åŒ…å« {len(data)} æ¡è®°å½•")

        for item in data:
            if 'filename' in item and 'sentences' in item:
                # è·å–ç¬¬ä¸€ä¸ªå¥å­ä½œä¸ºæ ‡æ³¨
                if item['sentences']:
                    caption = item['sentences'][0].get('raw', 'A change has occurred.').strip()
                    if caption.startswith(' '):
                        caption = caption[1:]
                else:
                    caption = 'A change has occurred.'

                # ä½¿ç”¨ changeflag æ¥è¾…åŠ©æ ‡æ³¨
                changeflag = item.get('changeflag', 1)

                captions_dict[item['filename']] = {
                    'caption': caption,
                    'changeflag': changeflag,
                }

        print(f"âœ… ä» JSON åŠ è½½äº† {len(captions_dict)} ä¸ªæ ‡æ³¨")

    except Exception as e:
        print(f"âš ï¸  æ— æ³•åŠ è½½ JSON æ ‡æ³¨: {e}")
        print(f"   å°†ä½¿ç”¨é»˜è®¤æ ‡æ³¨")

    return captions_dict


def _parse_json_arrays_streaming(content: str):
    """
    æµå¼è§£æå¤šä¸ª JSON æ•°ç»„ï¼ˆé¿å…è´ªå¿ƒæ­£åˆ™å¯¼è‡´çš„æ€§èƒ½é—®é¢˜ï¼‰

    æ¯” re.findall å¿« 100+ å€ï¼ˆç‰¹åˆ«æ˜¯å¤§æ–‡ä»¶ï¼‰
    """
    import json

    data = []
    depth = 0
    start_idx = -1

    for i, char in enumerate(content):
        if char == '[':
            if depth == 0:
                start_idx = i
            depth += 1
        elif char == ']':
            depth -= 1
            if depth == 0 and start_idx != -1:
                # æ‰¾åˆ°ä¸€ä¸ªå®Œæ•´æ•°ç»„
                try:
                    array_str = content[start_idx:i+1]
                    parsed = json.loads(array_str)
                    if isinstance(parsed, list):
                        data.extend(parsed)
                except json.JSONDecodeError:
                    print(f"âš ï¸  è·³è¿‡æ— æ•ˆçš„ JSON æ•°ç»„: {array_str[:50]}...")
                start_idx = -1

    return data


def load_raw_levir_cc_dataset(dataset_path: str):
    """
    ä»åŸå§‹LEVIR-CCæ–‡ä»¶ç»“æ„æˆ–å•ä¸ªArrowæ–‡ä»¶åŠ è½½æ•°æ®é›†

    æ”¯æŒçš„ç»“æ„:
    1. å•ä¸ª .arrow æ–‡ä»¶: LEVIR-CC/levir-cc-train.arrow
    2. å›¾åƒç›®å½•ç»“æ„: LEVIR-CC/images/train/A, B, val/A, B, test/A, B
    3. ç®€åŒ–ç›®å½•ç»“æ„: LEVIR-CC/A, B
    4. å¸¦æœ‰ JSON æ ‡æ³¨: é›†æˆ LevirCCcaptions.json

    Args:
        dataset_path: æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„

    Returns:
        åŒ…å«å›¾åƒè·¯å¾„å’Œæ ‡æ³¨çš„æ•°æ®å­—å…¸åˆ—è¡¨
    """
    from pathlib import Path
    import pyarrow as pa

    print(f"\nğŸ”„ å°è¯•ä»åŸå§‹ç»“æ„åŠ è½½æ•°æ®é›†: {dataset_path}")

    dataset_path = Path(dataset_path)

    # é¦–å…ˆå°è¯•åŠ è½½ JSON æ ‡æ³¨æ–‡ä»¶
    print(f"ğŸ”„ æ£€æŸ¥ JSON æ ‡æ³¨æ–‡ä»¶...")
    json_path = dataset_path / 'LevirCCcaptions.json'
    captions_dict = {}
    if json_path.exists():
        print(f"âœ… æ‰¾åˆ° JSON æ ‡æ³¨æ–‡ä»¶")
        captions_dict = load_captions_from_json(str(json_path))
    else:
        print(f"â„¹ï¸  æœªæ‰¾åˆ° JSON æ ‡æ³¨æ–‡ä»¶ï¼Œå°†ä½¿ç”¨é»˜è®¤æ ‡æ³¨")

    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ .arrow æ–‡ä»¶
    arrow_files = list(dataset_path.glob('*.arrow'))
    if arrow_files:
        print(f"âœ… æ‰¾åˆ° Arrow æ–‡ä»¶: {arrow_files[0].name}")
        try:
            # å°è¯•ç”¨ pyarrow ç›´æ¥è¯»å–
            import pyarrow.ipc as ipc
            with pa.memory_map(str(arrow_files[0]), 'r') as source:
                reader = ipc.open_file(source)
                table = reader.read_all()

            print(f"âœ… Arrow æ–‡ä»¶è¯»å–æˆåŠŸ")
            print(f"   åˆ—: {table.column_names}")
            print(f"   è¡Œæ•°: {len(table)}")

            # è½¬æ¢ä¸º HuggingFace Dataset
            dataset = datasets.Dataset(table)
            return dataset

        except Exception as e:
            print(f"âš ï¸  Arrow æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
            print(f"ğŸ”„ å°è¯•ç”¨ datasets.load_dataset...")

            try:
                # å°è¯•ç”¨ datasets åº“åŠ è½½
                dataset = datasets.load_dataset('arrow', data_files=str(arrow_files[0]), split='train')
                print(f"âœ… ä½¿ç”¨ datasets åº“åŠ è½½æˆåŠŸ: {len(dataset)} ä¸ªæ ·æœ¬")
                return dataset
            except Exception as e2:
                print(f"âš ï¸  datasets åº“åŠ è½½ä¹Ÿå¤±è´¥: {e2}")

    # å¦‚æœæ²¡æœ‰ Arrow æ–‡ä»¶ï¼Œå°è¯•ä»å›¾åƒç›®å½•åŠ è½½
    print(f"ğŸ”„ æŸ¥æ‰¾å›¾åƒç›®å½•ç»“æ„...")

    # æ£€æŸ¥å¯èƒ½çš„ç›®å½•ç»“æ„ï¼ˆåŒ…æ‹¬å¤šä¸ªåˆ†å‰²ï¼‰
    possible_structures = [
        # ç»“æ„1: LEVIR-CC/images/train/A, B (+ val/test)
        {
            'train_a': dataset_path / 'images' / 'train' / 'A',
            'train_b': dataset_path / 'images' / 'train' / 'B',
            'val_a': dataset_path / 'images' / 'val' / 'A',
            'val_b': dataset_path / 'images' / 'val' / 'B',
            'test_a': dataset_path / 'images' / 'test' / 'A',
            'test_b': dataset_path / 'images' / 'test' / 'B',
        },
        # ç»“æ„2: LEVIR-CC/A, B (ç®€åŒ–)
        {
            'train_a': dataset_path / 'A',
            'train_b': dataset_path / 'B',
        },
        # ç»“æ„3: LEVIR-CC/train/A, B (+ val/test)
        {
            'train_a': dataset_path / 'train' / 'A',
            'train_b': dataset_path / 'train' / 'B',
            'val_a': dataset_path / 'val' / 'A',
            'val_b': dataset_path / 'val' / 'B',
            'test_a': dataset_path / 'test' / 'A',
            'test_b': dataset_path / 'test' / 'B',
        },
    ]

    # æ‰¾åˆ°å­˜åœ¨çš„ç»“æ„
    valid_structure = None
    for structure in possible_structures:
        # æ£€æŸ¥è‡³å°‘æœ‰ train é›†
        if structure['train_a'].exists() and structure['train_b'].exists():
            valid_structure = structure
            print(f"âœ… æ‰¾åˆ°æœ‰æ•ˆç»“æ„:")
            print(f"   è®­ç»ƒé›†Aç›®å½•: {structure['train_a']}")
            print(f"   è®­ç»ƒé›†Bç›®å½•: {structure['train_b']}")
            if structure.get('val_a') and structure['val_a'].exists():
                print(f"   éªŒè¯é›†Aç›®å½•: {structure['val_a']}")
                print(f"   éªŒè¯é›†Bç›®å½•: {structure['val_b']}")
            break

    if valid_structure is None:
        # åˆ—å‡ºå®é™…å­˜åœ¨çš„æ–‡ä»¶/ç›®å½•
        print(f"\nğŸ“ å®é™…ç›®å½•å†…å®¹:")
        if dataset_path.exists():
            for item in sorted(dataset_path.iterdir()):
                if item.is_dir():
                    print(f"   ğŸ“ {item.name}/")
                else:
                    print(f"   ğŸ“„ {item.name}")

        raise FileNotFoundError(
            f"æ— æ³•åœ¨ {dataset_path} ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„LEVIR-CCæ•°æ®ç»“æ„ã€‚\n"
            f"é¢„æœŸç»“æ„:\n"
            f"  - å®Œæ•´ç»“æ„: LEVIR-CC/images/train/A, B (+ val/, test/)\n"
            f"  - ç®€åŒ–ç»“æ„: LEVIR-CC/A, B\n"
            f"  - å¦é€‰é¡¹: LEVIR-CC/train/A, B (+ val/, test/)"
        )

    # åŠ è½½æ‰€æœ‰åˆ†å‰²
    dataset_list = []
    splits = {
        'train': ('train_a', 'train_b'),
        'val': ('val_a', 'val_b'),
        'test': ('test_a', 'test_b'),
    }

    for split_name, (key_a, key_b) in splits.items():
        if key_a not in valid_structure or not valid_structure[key_a].exists():
            if split_name != 'train':
                print(f"âš ï¸  è·³è¿‡ {split_name} åˆ†å‰²ï¼ˆç›®å½•ä¸å­˜åœ¨ï¼‰")
            continue

        path_a = valid_structure[key_a]
        path_b = valid_structure[key_b]

        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶ï¼ˆä¼˜åŒ–ï¼šä½¿ç”¨æ›´å¿«çš„ç›®å½•éå†æ–¹å¼ï¼‰
        print(f"ğŸ”„ æ‰«æ {split_name} é›†å›¾åƒæ–‡ä»¶...")
        try:
            # ä½¿ç”¨ os.listdir æ¯” glob å¿« 10+ å€ï¼ˆåœ¨å¤§ç›®å½•ä¸­ï¼‰
            import os
            img_extensions = {'.png', '.jpg', '.jpeg'}

            img_a_files = sorted([
                path_a / f for f in os.listdir(str(path_a))
                if Path(f).suffix.lower() in img_extensions
            ])
            img_b_files = sorted([
                path_b / f for f in os.listdir(str(path_b))
                if Path(f).suffix.lower() in img_extensions
            ])
        except Exception as e:
            print(f"âš ï¸  æ‰«æ {split_name} ç›®å½•å¤±è´¥: {e}")
            continue

        print(f"âœ… æ‰¾åˆ° {len(img_a_files)} å¯¹ {split_name} é›†å›¾åƒ")

        # æ„å»ºæ•°æ®é›†
        print(f"ğŸ”„ æ„å»º {split_name} é›†æ•°æ®åˆ—è¡¨...")
        for idx, (img_a_path, img_b_path) in enumerate(zip(img_a_files, img_b_files)):
            img_a_name = img_a_path.name

            # å°è¯•ä» JSON è·å–æ ‡æ³¨
            caption = 'A change has occurred in the remote sensing image.'
            changeflag = 1

            if img_a_name in captions_dict:
                caption = captions_dict[img_a_name]['caption']
                changeflag = captions_dict[img_a_name]['changeflag']

            dataset_list.append({
                'A': str(img_a_path),
                'B': str(img_b_path),
                'caption': caption,
                'changeflag': changeflag,
                'split': split_name,
                'bbox': [0, 0, 256, 256],  # é»˜è®¤bboxï¼Œå°†åœ¨åç»­è¢«å½’ä¸€åŒ–
            })

            # å®šæœŸè¾“å‡ºè¿›åº¦
            if (idx + 1) % 1000 == 0:
                print(f"   â„¹ï¸  å·²æ„å»º {idx + 1}/{len(img_a_files)} æ ·æœ¬...")

        print(f"âœ… å®Œæˆ {split_name} é›†æ•°æ®æ„å»º")

    return dataset_list


def create_dataloaders(
    batch_size: int = 4,
    num_workers: int = 4,
    test_split: float = 0.1,
    seed: int = 42,
):
    """
    Create train and validation dataloaders from LEVIR-CC dataset

    æ”¯æŒå¤šç§æ•°æ®é›†ç»“æ„ï¼ˆæŒ‰ä¼˜å…ˆçº§é¡ºåºï¼‰ï¼š
    1. åŸå§‹å›¾åƒç›®å½• + JSON æ ‡æ³¨ï¼ˆæ¨èï¼Œå½“å‰æ•°æ®é›†æ ¼å¼ï¼‰
       - images/train/A, B
       - images/val/A, B
       - images/test/A, B
       - LevirCCcaptions.json
    2. å•ä¸ª Arrow æ–‡ä»¶ï¼ˆHuggingFace æ ¼å¼ï¼‰
    3. Arrow æ ¼å¼æ•°æ®é›†ï¼ˆload_from_diskï¼‰

    è‡ªåŠ¨ç‰¹æ€§ï¼š
    - æ£€æµ‹é¢„å®šä¹‰çš„ train/val/test åˆ†å‰²
    - å¦‚æœåªæœ‰ train é›†ï¼Œè‡ªåŠ¨éšæœºåˆ†å‰²ä¸º train/val
    - åŠ è½½ JSON æ ‡æ³¨æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    - Kaggle ç¯å¢ƒè‡ªåŠ¨å¤„ç†ç¼“å­˜ç›®å½•

    Args:
        batch_size: Batch size for dataloaders
        num_workers: Number of workers for data loading
        test_split: Proportion of data to use for validation
        seed: Random seed for train/test split

    Returns:
        Tuple of (train_dataloader, val_dataloader)
    """
    if not DATASETS_AVAILABLE:
        raise ImportError("datasets library is required. Install with: pip install datasets")

    print("\n" + "="*60)
    print("åŠ è½½ LEVIR-CC æ•°æ®é›†")
    print("="*60)
    print(f"æ•°æ®é›†è·¯å¾„: {Config.DATASET_PATH}")

    import os
    import numpy as np

    # è®¾ç½®ç¼“å­˜ç›®å½•åˆ°å¯å†™ä½ç½®ï¼ˆKaggle ç¯å¢ƒä¸­æ˜¯å¿…éœ€çš„ï¼‰
    cache_dir = os.path.join(Config.WORKING_DIR, '.cache')
    os.makedirs(cache_dir, exist_ok=True)
    os.environ['HF_DATASETS_CACHE'] = cache_dir

    dataset_split = None
    raw_data = None

    # ä¼˜å…ˆçº§1ï¼šå°è¯•ä»åŸå§‹æ–‡ä»¶ç»“æ„åŠ è½½ï¼ˆå½“å‰æ•°æ®é›†æ ¼å¼ï¼‰
    print(f"ğŸ”„ æ£€æµ‹æ•°æ®é›†æ ¼å¼...")
    dataset_path = Path(Config.DATASET_PATH)

    # æ£€æŸ¥æ˜¯å¦æ˜¯åŸå§‹å›¾åƒç›®å½•ç»“æ„
    has_images_dir = (dataset_path / 'images' / 'train' / 'A').exists() or \
                     (dataset_path / 'train' / 'A').exists() or \
                     (dataset_path / 'A').exists()

    if has_images_dir:
        print(f"âœ… æ£€æµ‹åˆ°åŸå§‹å›¾åƒç›®å½•ç»“æ„ï¼Œä¼˜å…ˆä½¿ç”¨æ­¤æ ¼å¼")
        try:
            raw_data = load_raw_levir_cc_dataset(Config.DATASET_PATH)
            print(f"âœ… ä»åŸå§‹å›¾åƒç›®å½•åŠ è½½æˆåŠŸï¼Œå…± {len(raw_data)} ä¸ªæ ·æœ¬")
        except Exception as raw_e:
            print(f"âš ï¸  åŸå§‹ç»“æ„åŠ è½½å¤±è´¥: {raw_e}")
            raw_data = None

    # ä¼˜å…ˆçº§2ï¼šå¦‚æœåŸå§‹ç»“æ„ä¸å­˜åœ¨ï¼Œå°è¯• Arrow æ ¼å¼
    if raw_data is None:
        print(f"ğŸ”„ å°è¯•åŠ è½½ Arrow æ ¼å¼...")
        try:
            full_dataset = datasets.load_from_disk(Config.DATASET_PATH)
            print(f"âœ… ä» Arrow æ ¼å¼åŠ è½½æˆåŠŸ")
            print(f"   æ•°æ®é›†ç»“æ„: {full_dataset}")

            # Get the appropriate split
            if 'train' in full_dataset:
                dataset_split = full_dataset['train']
                print(f"âœ… ä½¿ç”¨ 'train' åˆ†å‰²ï¼Œå…± {len(dataset_split)} ä¸ªæ ·æœ¬")
            else:
                # If only one split exists, use it
                dataset_split = full_dataset
                print(f"âš ï¸  æœªæ‰¾åˆ° 'train' åˆ†å‰²ã€‚ä½¿ç”¨æ•´ä¸ªæ•°æ®é›†ï¼Œå…± {len(dataset_split)} ä¸ªæ ·æœ¬")

        except (FileNotFoundError, Exception) as e:
            print(f"âš ï¸  Arrow æ ¼å¼åŠ è½½å¤±è´¥: {e}")

            # ä¼˜å…ˆçº§3ï¼šå°è¯•å•ä¸ª Arrow æ–‡ä»¶æˆ–å…¶ä»–æ ¼å¼
            print(f"ğŸ”„ å°è¯•åŠ è½½å•ä¸ª Arrow æ–‡ä»¶...")
            try:
                raw_data = load_raw_levir_cc_dataset(Config.DATASET_PATH)

                # æ£€æŸ¥è¿”å›ç±»å‹
                if isinstance(raw_data, datasets.Dataset):
                    dataset_split = raw_data
                    print(f"âœ… ä» Arrow æ–‡ä»¶åŠ è½½æˆåŠŸï¼Œå…± {len(dataset_split)} ä¸ªæ ·æœ¬")
                else:
                    print(f"âœ… ä»å…¶ä»–æ ¼å¼åŠ è½½æˆåŠŸï¼Œå…± {len(raw_data)} ä¸ªæ ·æœ¬")

            except Exception as raw_e:
                print(f"âŒ æ‰€æœ‰æ ¼å¼åŠ è½½éƒ½å¤±è´¥")
                import traceback
                traceback.print_exc()
                raise RuntimeError(
                    f"æ— æ³•åŠ è½½æ•°æ®é›†ã€‚å°è¯•äº†ä»¥ä¸‹æ ¼å¼:\n"
                    f"1. åŸå§‹å›¾åƒç›®å½• (images/train/A,B + val + test): è·¯å¾„ä¸å­˜åœ¨\n"
                    f"2. Arrow æ ¼å¼ (load_from_disk): {e}\n"
                    f"3. å•ä¸ª Arrow æ–‡ä»¶: {raw_e}\n\n"
                    f"è¯·æ£€æŸ¥æ•°æ®é›†è·¯å¾„å’Œç»“æ„æ˜¯å¦æ­£ç¡®ã€‚"
                    f"é¢„æœŸæ ¼å¼:\n"
                    f"  - {dataset_path}/images/train/{{A,B}}\n"
                    f"  - {dataset_path}/images/val/{{A,B}}\n"
                    f"  - {dataset_path}/images/test/{{A,B}}"
                )

    # å¤„ç†ä»åˆ—è¡¨åŠ è½½çš„æƒ…å†µ
    if raw_data is not None and dataset_split is None:
        # ä»åˆ—è¡¨è½¬æ¢ä¸º HuggingFace Dataset
        # æŒ‰ split åˆ†ç»„
        train_data = [item for item in raw_data if item.get('split', 'train') == 'train']
        val_data = [item for item in raw_data if item.get('split', 'train') == 'val']
        test_data = [item for item in raw_data if item.get('split', 'train') == 'test']

        print(f"\nğŸ“Š æ•°æ®é›†åˆ†å‰²ç»Ÿè®¡:")
        print(f"   è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
        print(f"   éªŒè¯é›†: {len(val_data)} æ ·æœ¬")
        print(f"   æµ‹è¯•é›†: {len(test_data)} æ ·æœ¬")

        # å¦‚æœæœ‰é¢„å®šä¹‰çš„éªŒè¯é›†ï¼Œç›´æ¥ä½¿ç”¨
        if len(val_data) > 0:
            print(f"ğŸ”„ ä½¿ç”¨é¢„å®šä¹‰çš„è®­ç»ƒ/éªŒè¯åˆ†å‰²...")
            dataset_dict_train = {
                'A': [item['A'] for item in train_data],
                'B': [item['B'] for item in train_data],
                'caption': [item['caption'] for item in train_data],
                'bbox': [item['bbox'] for item in train_data],
            }
            dataset_dict_val = {
                'A': [item['A'] for item in val_data],
                'B': [item['B'] for item in val_data],
                'caption': [item['caption'] for item in val_data],
                'bbox': [item['bbox'] for item in val_data],
            }

            split_dataset = {
                'train': datasets.Dataset.from_dict(dataset_dict_train),
                'test': datasets.Dataset.from_dict(dataset_dict_val),
            }
        else:
            # ä½¿ç”¨æ‰€æœ‰æ•°æ®å¹¶éšæœºåˆ†å‰²
            print(f"ğŸ”„ æ­£åœ¨éšæœºåˆ†å‰²æ•°æ®é›† (train: {1-test_split:.1%}, val: {test_split:.1%})...")

            dataset_dict = {
                'A': [item['A'] for item in train_data],
                'B': [item['B'] for item in train_data],
                'caption': [item['caption'] for item in train_data],
                'bbox': [item['bbox'] for item in train_data],
            }

            dataset_split = datasets.Dataset.from_dict(dataset_dict)

            n_samples = len(dataset_split)
            indices = np.arange(n_samples)
            np.random.seed(seed)
            np.random.shuffle(indices)

            split_point = int(n_samples * (1 - test_split))
            train_indices = sorted(indices[:split_point].tolist())
            test_indices = sorted(indices[split_point:].tolist())

            try:
                split_dataset = {
                    'train': dataset_split.select(train_indices),
                    'test': dataset_split.select(test_indices)
                }
            except OSError as e:
                if "Read-only file system" in str(e) or "No space left" in str(e):
                    print(f"âš ï¸  æ•°æ®é›†æ“ä½œä¸­é‡åˆ°æ–‡ä»¶ç³»ç»Ÿé”™è¯¯ï¼Œä½¿ç”¨ç›´æ¥ç´¢å¼•è®¿é—®...")

                    class IndexedDataset:
                        def __init__(self, dataset, indices):
                            self.dataset = dataset
                            self.indices = indices
                            self._len = len(indices)

                        def __len__(self):
                            return self._len

                        def __getitem__(self, idx):
                            return self.dataset[self.indices[idx]]

                    split_dataset = {
                        'train': IndexedDataset(dataset_split, train_indices),
                        'test': IndexedDataset(dataset_split, test_indices)
                    }
                else:
                    raise
    else:
        # ä» HuggingFace Dataset åŠ è½½
        if dataset_split is None:
            raise RuntimeError("æ— æ³•åŠ è½½æ•°æ®é›†")

        # å¯¹äº HuggingFace Datasetï¼Œè¿›è¡Œéšæœºåˆ†å‰²
        print(f"ğŸ”„ æ­£åœ¨åˆ†å‰² HuggingFace Dataset (train: {1-test_split:.1%}, val: {test_split:.1%})...")

        n_samples = len(dataset_split)
        indices = np.arange(n_samples)
        np.random.seed(seed)
        np.random.shuffle(indices)

        split_point = int(n_samples * (1 - test_split))
        train_indices = sorted(indices[:split_point].tolist())
        test_indices = sorted(indices[split_point:].tolist())

        try:
            split_dataset = {
                'train': dataset_split.select(train_indices),
                'test': dataset_split.select(test_indices)
            }
        except OSError as e:
            if "Read-only file system" in str(e) or "No space left" in str(e):
                print(f"âš ï¸  æ•°æ®é›†æ“ä½œä¸­é‡åˆ°æ–‡ä»¶ç³»ç»Ÿé”™è¯¯ï¼Œä½¿ç”¨ç›´æ¥ç´¢å¼•è®¿é—®...")

                class IndexedDataset:
                    def __init__(self, dataset, indices):
                        self.dataset = dataset
                        self.indices = indices
                        self._len = len(indices)

                    def __len__(self):
                        return self._len

                    def __getitem__(self, idx):
                        return self.dataset[self.indices[idx]]

                split_dataset = {
                    'train': IndexedDataset(dataset_split, train_indices),
                    'test': IndexedDataset(dataset_split, test_indices)
                }
            else:
                raise

    print(f"âœ… æ•°æ®é›†åˆ†å‰²å®Œæˆ: è®­ç»ƒé›† {len(split_dataset['train'])} ä¸ªæ ·æœ¬ï¼ŒéªŒè¯é›† {len(split_dataset['test'])} ä¸ªæ ·æœ¬")

    # åˆ›å»º PyTorch Dataset
    train_dataset = LevirCCActionDataset(
        split_dataset['train'],
        image_size=Config.IMAGE_SIZE,
        max_text_length=Config.MAX_TEXT_LENGTH,
        normalize_bbox=Config.BBOX_NORMALIZE,
    )

    val_dataset = LevirCCActionDataset(
        split_dataset['test'],
        image_size=Config.IMAGE_SIZE,
        max_text_length=Config.MAX_TEXT_LENGTH,
        normalize_bbox=Config.BBOX_NORMALIZE,
    )

    # åˆ›å»º DataLoaders
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True,
    )

    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=False,
    )

    print(f"\nâœ… DataLoaders åˆ›å»ºæˆåŠŸ:")
    print(f"   è®­ç»ƒé›†: {len(train_loader)} ä¸ªbatch ({len(train_dataset)} æ ·æœ¬)")
    print(f"   éªŒè¯é›†: {len(val_loader)} ä¸ªbatch ({len(val_dataset)} æ ·æœ¬)")
    print("="*60 + "\n")

    return train_loader, val_loader


if __name__ == "__main__":
    # Test dataset loading
    print("Testing dataset loading...")
    train_loader, val_loader = create_dataloaders(batch_size=2)

    # Get a sample batch
    for batch_idx, batch in enumerate(train_loader):
        print(f"\nBatch {batch_idx}:")
        for key, value in batch.items():
            if isinstance(value, torch.Tensor):
                print(f"  {key}: {value.shape}")
            else:
                print(f"  {key}: {type(value).__name__}")
        break  # Only show first batch

