"""
Dataset module for loading Arrow format data from HuggingFace datasets
Handles LEVIR-CC change detection dataset with bbox and caption
"""
import io
from pathlib import Path
from typing import Dict, Any, Tuple

import numpy as np
import torch
import torchvision.transforms as transforms
from PIL import Image
from torch.utils.data import Dataset

from .config import Config

# Try to import datasets library
try:
    import datasets
    DATASETS_AVAILABLE = True
except ImportError:
    DATASETS_AVAILABLE = False
    print("âš ï¸  Warning: datasets library not found. Install with: pip install datasets")


class LevirCCActionDataset(Dataset):
    """
    LEVIR-CC change detection dataset adapted for action prediction

    Loads data from HuggingFace Arrow format (.arrow files)
    Expected dataset structure:
    - 'image' or 'A': Temporal image 1
    - 'image2' or 'B': Temporal image 2
    - 'caption': Change description text
    - 'bbox': Bounding box [x1, y1, x2, y2] of change region
    """

    def __init__(
        self,
        dataset_split,
        image_size: int = 224,
        max_text_length: int = 128,
        normalize_bbox: bool = True,
    ):
        """
        Initialize LEVIR-CC dataset

        Args:
            dataset_split: HuggingFace dataset split (usually dataset['train'])
            image_size: Target size for image resizing
            max_text_length: Maximum length for tokenized text
            normalize_bbox: Whether to normalize bbox to [0, 1] range
        """
        if not DATASETS_AVAILABLE:
            raise ImportError("datasets library is required. Install with: pip install datasets")

        self.dataset = dataset_split
        self.image_size = image_size
        self.max_text_length = max_text_length
        self.normalize_bbox = normalize_bbox

        # Image preprocessing pipeline
        self.image_transform = transforms.Compose([
            transforms.Resize((image_size, image_size)),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.48145466, 0.4578275, 0.40821073],  # CLIP normalization
                std=[0.26862954, 0.26130258, 0.27577711]
            )
        ])

        # æ°¸ä¹…æ€§åœ°ç¦ç”¨è‡ªåŠ¨è§£ç ä»¥é¿å…ç¼“å­˜æ–‡ä»¶åŠ è½½é”™è¯¯
        # è¿™åœ¨å¤šè¿›ç¨‹ DataLoader ä¸­æ˜¯å¿…éœ€çš„
        self._disable_auto_decoding()

        # Inspect first sample to understand data structure
        self._inspect_data_structure()

        print(f"âœ… Dataset initialized with {len(self.dataset)} samples")

    def _disable_auto_decoding(self):
        """
        æ°¸ä¹…æ€§åœ°ç¦ç”¨è‡ªåŠ¨è§£ç ï¼Œç‰¹åˆ«æ˜¯å¯¹äºŽ Image ç±»åž‹å­—æ®µ

        ä¸ºäº†åœ¨ DataLoader å¤šè¿›ç¨‹ä¸­ä¹Ÿèƒ½å·¥ä½œï¼Œæˆ‘ä»¬å°† HuggingFace Dataset è½¬æ¢ä¸º
        ä¸€ä¸ªç®€å•çš„åˆ—è¡¨ç»“æž„ï¼Œé¿å… HuggingFace çš„è‡ªåŠ¨è§£ç æœºåˆ¶ã€‚

        å…³é”®ç­–ç•¥ï¼šä¸è®¿é—®åŒ…å« Image å­—æ®µçš„æ•°æ®ï¼ˆè¿™ä¼šè§¦å‘è‡ªåŠ¨è§£ç ï¼‰ï¼Œè€Œæ˜¯ï¼š
        1. èŽ·å–åŽŸå§‹ PyArrow Table
        2. ç¦ç”¨æ‰€æœ‰åˆ—çš„è§£ç å™¨
        3. è½¬æ¢ä¸ºåˆ—è¡¨
        """
        try:
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯ IndexedDataset åŒ…è£…å¯¹è±¡
            if hasattr(self.dataset, '__class__') and self.dataset.__class__.__name__ == 'IndexedDataset':
                print(f"â„¹ï¸  æ£€æµ‹åˆ° IndexedDataset åŒ…è£…å¯¹è±¡ï¼Œè·³è¿‡è‡ªåŠ¨è§£ç ç¦ç”¨")
                return

            # æ£€æŸ¥æ˜¯å¦æœ‰ Image ç±»åž‹å­—æ®µ
            if hasattr(self.dataset, 'features'):
                from datasets.features import Image as HFImage
                has_image = any(isinstance(feature, HFImage) for feature in self.dataset.features.values())

                if has_image:
                    print(f"âš ï¸  æ£€æµ‹åˆ° Image ç±»åž‹å­—æ®µï¼Œç¦ç”¨è‡ªåŠ¨è§£ç ...")

                    # å…³é”®ï¼šç¦ç”¨æ‰€æœ‰çš„ç‰¹å¾è§£ç å™¨ï¼Œé˜²æ­¢è‡ªåŠ¨è§£ç è§¦å‘
                    self.dataset._format_type = None
                    self.dataset._format_kwargs = {}
                    self.dataset._format_columns = None

                    # ä½¿ç”¨ pyarrow æ“ä½œé¿å…è§¦å‘è§£ç 
                    print(f"ðŸ”„ æ­£åœ¨å°†æ•°æ®é›†è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼...")

                    # èŽ·å–åŽŸå§‹ pyarrow table
                    table = self.dataset.data

                    # å°† pyarrow table è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                    dataset_list = []
                    for i in range(len(table)):
                        row_dict = {}
                        for col_name in table.column_names:
                            # ä»Ž pyarrow ç›´æŽ¥èŽ·å–æ•°æ®ï¼Œä¸é€šè¿‡ HF çš„è§£ç æœºåˆ¶
                            col_data = table[col_name][i].as_py()
                            row_dict[col_name] = col_data
                        dataset_list.append(row_dict)

                    # ç”¨åˆ—è¡¨æ›¿æ¢ HuggingFace Dataset
                    self.dataset = dataset_list
                    print(f"âœ… å·²è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼ˆ{len(self.dataset)} æ ·æœ¬ï¼‰")
        except Exception as e:
            print(f"âš ï¸  ç¦ç”¨è‡ªåŠ¨è§£ç å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            print(f"ðŸ”„ å°†ç»§ç»­ä½¿ç”¨åŽŸå§‹æ•°æ®é›†ï¼Œå¯èƒ½ä¼šæœ‰ç¼“å­˜é—®é¢˜")

    def _inspect_data_structure(self):
        """Inspect the first sample to understand data structure and keys"""
        if len(self.dataset) == 0:
            raise ValueError("Dataset is empty!")

        # èŽ·å–ç¬¬ä¸€ä¸ªæ ·æœ¬
        first_sample = self.dataset[0]

        print("\n" + "="*60)
        print("Dataset Structure Inspection")
        print("="*60)
        print(f"Dataset size: {len(self.dataset)}")
        print(f"Sample keys: {list(first_sample.keys())}")
        print(f"Sample data types:")
        for key, value in first_sample.items():
            if value is None:
                print(f"  {key}: None (detected from schema)")
            elif isinstance(value, (list, tuple)):
                print(f"  {key}: {type(value).__name__} of length {len(value)}")
            elif isinstance(value, dict) and 'bytes' in value:
                print(f"  {key}: dict with 'bytes' (image data)")
            else:
                print(f"  {key}: {type(value).__name__}")
        print("="*60 + "\n")

        # Store detected keys for later use
        self.image_key = self._detect_image_key(first_sample)
        self.image2_key = self._detect_image2_key(first_sample)
        self.caption_key = self._detect_caption_key(first_sample)
        self.bbox_key = self._detect_bbox_key(first_sample)

        print(f"Detected keys:")
        print(f"  Image 1 key: {self.image_key}")
        print(f"  Image 2 key: {self.image2_key}")
        print(f"  Caption key: {self.caption_key}")
        print(f"  BBox key: {self.bbox_key}\n")

    @staticmethod
    def _detect_image_key(sample: Dict) -> str:
        """Detect the key for temporal image 1"""
        candidates = ['image', 'A', 'img', 'image1']
        for key in candidates:
            if key in sample:
                return key
        raise KeyError(f"Could not find image key. Available keys: {list(sample.keys())}")

    @staticmethod
    def _detect_image2_key(sample: Dict) -> str:
        """Detect the key for temporal image 2"""
        candidates = ['image2', 'B', 'img2', 'image_2']
        for key in candidates:
            if key in sample:
                return key
        # å¦‚æžœåªæœ‰å•å¼ å›¾åƒï¼Œè¿”å›žç›¸åŒçš„å›¾åƒé”®
        # è¿™æ · image_t2 å°±ä¼šé‡å¤ä½¿ç”¨åŒä¸€å¼ å›¾åƒ
        for key in ['image', 'A', 'img', 'image1']:
            if key in sample:
                print(f"âš ï¸  æœªæ‰¾åˆ° image2 é”®ï¼Œå°†ä½¿ç”¨ '{key}' ä½œä¸º image2ï¼ˆé‡å¤ä½¿ç”¨åŒä¸€å¼ å›¾åƒï¼‰")
                return key
        raise KeyError(f"Could not find image2 key. Available keys: {list(sample.keys())}")

    @staticmethod
    def _detect_caption_key(sample: Dict) -> str:
        """Detect the key for caption/description"""
        candidates = ['caption', 'text', 'description', 'change_description', 'label']
        for key in candidates:
            if key in sample:
                if key == 'label':
                    print(f"âš ï¸  æœªæ‰¾åˆ° caption é”®ï¼Œå°†ä½¿ç”¨ 'label' å­—æ®µ")
                return key
        # Default to first non-image key if no caption found
        for key in sample.keys():
            if key not in ['image', 'A', 'img', 'image1', 'image2', 'B', 'img2', 'image_2', 'bbox', 'bboxes']:
                print(f"âš ï¸  æœªæ‰¾åˆ° caption é”®ï¼Œä½¿ç”¨é»˜è®¤çš„ç¬¬ä¸€ä¸ªéžå›¾åƒé”®: '{key}'")
                return key
        # å¦‚æžœå®Œå…¨æ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›žä¸€ä¸ªé»˜è®¤å€¼
        print(f"âš ï¸  æœªæ‰¾åˆ°ä»»ä½• caption é”®ï¼Œå°†ä½¿ç”¨é»˜è®¤æè¿°")
        return None

    @staticmethod
    def _detect_bbox_key(sample: Dict) -> str:
        """Detect the key for bounding box"""
        candidates = ['bbox', 'bboxes', 'bounding_box', 'box']
        for key in candidates:
            if key in sample:
                return key
        # å¦‚æžœæ²¡æœ‰ bboxï¼Œè¿”å›ž Noneï¼Œç¨åŽä¼šä½¿ç”¨é»˜è®¤å€¼
        print(f"âš ï¸  æœªæ‰¾åˆ° bbox é”®ï¼Œå°†ä½¿ç”¨é»˜è®¤çš„å…¨å›¾ bbox")
        return None

    def __len__(self) -> int:
        """Return dataset size"""
        return len(self.dataset)

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        """
        Get a sample from the dataset

        Args:
            idx: Sample index

        Returns:
            Dictionary containing:
                - 'image_t1': Tensor of shape (3, H, W) - temporal image 1
                - 'image_t2': Tensor of shape (3, H, W) - temporal image 2
                - 'caption': String - change description
                - 'caption_ids': LongTensor - tokenized caption
                - 'action_vector': Tensor - normalized action vector [cx, cy, scale]
                - 'bbox': List - original bbox [x1, y1, x2, y2]
        """
        # ç›´æŽ¥è®¿é—®æ•°æ®é›†ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»åœ¨ __init__ ä¸­ç¦ç”¨äº†è‡ªåŠ¨è§£ç 
        sample = self.dataset[idx]

        # Load images
        try:
            image_t1 = self._load_image(sample[self.image_key])
            image_t2 = self._load_image(sample[self.image2_key])
        except Exception as e:
            print(f"âŒ Error loading images at index {idx}: {e}")
            raise

        # Get caption
        try:
            if self.caption_key is None:
                caption = "change detected"
            else:
                caption = str(sample[self.caption_key])
                # å¦‚æžœ caption æ˜¯æ•°å­—ï¼ˆlabelï¼‰ï¼Œè½¬æ¢ä¸ºæ–‡æœ¬æè¿°
                if caption.isdigit():
                    caption = f"class {caption}"
                elif not caption or caption.lower() in ['none', 'nan', '']:
                    caption = "change detected"
        except Exception as e:
            print(f"âš ï¸  Error loading caption at index {idx}: {e}")
            caption = "change detected"

        # Get and process bbox
        try:
            if self.bbox_key is None:
                # ä½¿ç”¨é»˜è®¤çš„å…¨å›¾ bbox
                bbox = [0, 0, image_t1.size[0], image_t1.size[1]]
                action_vector = torch.tensor([0.5, 0.5, 1.0], dtype=torch.float32)
            else:
                bbox = sample[self.bbox_key]
                action_vector = self._process_bbox(bbox, image_t1.size)
        except Exception as e:
            print(f"âš ï¸  Warning: Could not process bbox at index {idx}: {e}")
            # Default action vector if bbox fails
            action_vector = torch.tensor([0.5, 0.5, 0.5], dtype=torch.float32)
            bbox = [0, 0, image_t1.size[0], image_t1.size[1]]

        # Transform images
        image_t1 = self.image_transform(image_t1)
        image_t2 = self.image_transform(image_t2)

        return {
            'image_t1': image_t1,
            'image_t2': image_t2,
            'caption': caption,
            'action_vector': action_vector,
            'bbox': bbox,
            'index': idx,
        }

    @staticmethod
    def _load_image(image_data) -> Image.Image:
        """
        Load image from various formats

        Supports:
        - PIL Image objects
        - Bytes (encoded images)
        - Paths (string or Path)
        - HuggingFace Image dict with 'bytes' key
        """
        if isinstance(image_data, Image.Image):
            return image_data.convert('RGB')
        elif isinstance(image_data, dict) and 'bytes' in image_data:
            # HuggingFace Image feature format
            return Image.open(io.BytesIO(image_data['bytes'])).convert('RGB')
        elif isinstance(image_data, bytes):
            return Image.open(io.BytesIO(image_data)).convert('RGB')
        elif isinstance(image_data, (str, Path)):
            return Image.open(image_data).convert('RGB')
        else:
            raise TypeError(f"Unsupported image format: {type(image_data)}, keys: {image_data.keys() if isinstance(image_data, dict) else 'N/A'}")

    def _process_bbox(self, bbox, image_size: Tuple[int, int]) -> torch.Tensor:
        """
        Process bounding box to action vector

        Converts [x1, y1, x2, y2] to normalized [cx, cy, scale]
        where:
        - cx, cy: center point (normalized to [0, 1])
        - scale: relative size of bbox (normalized to [0, 1])

        Args:
            bbox: List/Tuple of [x1, y1, x2, y2]
            image_size: Tuple of (width, height)

        Returns:
            Tensor of shape (3,) with normalized action vector
        """
        try:
            # Parse bbox
            if isinstance(bbox, (list, tuple)) and len(bbox) == 4:
                x1, y1, x2, y2 = bbox
            else:
                # Fallback for different bbox formats
                raise ValueError(f"Invalid bbox format: {bbox}")

            # Ensure valid bbox
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(image_size[0], x2), min(image_size[1], y2)

            if x2 <= x1 or y2 <= y1:
                # Invalid bbox, return center point with small scale
                return torch.tensor([0.5, 0.5, 0.1], dtype=torch.float32)

            # Calculate center point
            cx = (x1 + x2) / (2.0 * image_size[0])
            cy = (y1 + y2) / (2.0 * image_size[1])

            # Calculate scale (relative bbox size)
            width = (x2 - x1) / image_size[0]
            height = (y2 - y1) / image_size[1]
            scale = np.sqrt(width * height)  # Geometric mean of width and height

            # Clip to valid range
            cx = np.clip(cx, 0.0, 1.0)
            cy = np.clip(cy, 0.0, 1.0)
            scale = np.clip(scale, 0.0, 1.0)

            return torch.tensor([cx, cy, scale], dtype=torch.float32)

        except Exception as e:
            print(f"âš ï¸  Error processing bbox {bbox}: {e}")
            return torch.tensor([0.5, 0.5, 0.5], dtype=torch.float32)


def load_raw_levir_cc_dataset(dataset_path: str):
    """
    ä»ŽåŽŸå§‹LEVIR-CCæ–‡ä»¶ç»“æž„æˆ–å•ä¸ªArrowæ–‡ä»¶åŠ è½½æ•°æ®é›†

    æ”¯æŒçš„ç»“æž„:
    1. å•ä¸ª .arrow æ–‡ä»¶: LEVIR-CC/levir-cc-train.arrow
    2. å›¾åƒç›®å½•ç»“æž„: LEVIR-CC/images/train/A å’Œ B
    3. ç®€åŒ–ç›®å½•ç»“æž„: LEVIR-CC/A å’Œ B

    Args:
        dataset_path: æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„

    Returns:
        åŒ…å«å›¾åƒè·¯å¾„å’Œæ ‡æ³¨çš„æ•°æ®å­—å…¸åˆ—è¡¨
    """
    from pathlib import Path
    import pyarrow as pa

    print(f"\nðŸ”„ å°è¯•ä»ŽåŽŸå§‹ç»“æž„åŠ è½½æ•°æ®é›†: {dataset_path}")

    dataset_path = Path(dataset_path)

    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ .arrow æ–‡ä»¶
    arrow_files = list(dataset_path.glob('*.arrow'))
    if arrow_files:
        print(f"âœ… æ‰¾åˆ° Arrow æ–‡ä»¶: {arrow_files[0].name}")
        try:
            # å°è¯•ç”¨ pyarrow ç›´æŽ¥è¯»å–
            import pyarrow.ipc as ipc
            with pa.memory_map(str(arrow_files[0]), 'r') as source:
                reader = ipc.open_file(source)
                table = reader.read_all()

            print(f"âœ… Arrow æ–‡ä»¶è¯»å–æˆåŠŸ")
            print(f"   åˆ—: {table.column_names}")
            print(f"   è¡Œæ•°: {len(table)}")

            # è½¬æ¢ä¸º HuggingFace Dataset
            dataset = datasets.Dataset(table)
            return dataset

        except Exception as e:
            print(f"âš ï¸  Arrow æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
            print(f"ðŸ”„ å°è¯•ç”¨ datasets.load_dataset...")

            try:
                # å°è¯•ç”¨ datasets åº“åŠ è½½
                dataset = datasets.load_dataset('arrow', data_files=str(arrow_files[0]), split='train')
                print(f"âœ… ä½¿ç”¨ datasets åº“åŠ è½½æˆåŠŸ: {len(dataset)} ä¸ªæ ·æœ¬")
                return dataset
            except Exception as e2:
                print(f"âš ï¸  datasets åº“åŠ è½½ä¹Ÿå¤±è´¥: {e2}")

    # å¦‚æžœæ²¡æœ‰ Arrow æ–‡ä»¶ï¼Œå°è¯•ä»Žå›¾åƒç›®å½•åŠ è½½
    print(f"ðŸ”„ æŸ¥æ‰¾å›¾åƒç›®å½•ç»“æž„...")

    # æ£€æŸ¥å¯èƒ½çš„ç›®å½•ç»“æž„
    possible_structures = [
        # ç»“æž„1: LEVIR-CC/images/train/A, B
        {
            'img_a': dataset_path / 'images' / 'train' / 'A',
            'img_b': dataset_path / 'images' / 'train' / 'B',
        },
        # ç»“æž„2: LEVIR-CC/A, B
        {
            'img_a': dataset_path / 'A',
            'img_b': dataset_path / 'B',
        },
        # ç»“æž„3: LEVIR-CC/train/A, B
        {
            'img_a': dataset_path / 'train' / 'A',
            'img_b': dataset_path / 'train' / 'B',
        },
    ]

    # æ‰¾åˆ°å­˜åœ¨çš„ç»“æž„
    valid_structure = None
    for structure in possible_structures:
        if structure['img_a'].exists() and structure['img_b'].exists():
            valid_structure = structure
            print(f"âœ… æ‰¾åˆ°æœ‰æ•ˆç»“æž„:")
            print(f"   å›¾åƒAç›®å½•: {structure['img_a']}")
            print(f"   å›¾åƒBç›®å½•: {structure['img_b']}")
            break

    if valid_structure is None:
        # åˆ—å‡ºå®žé™…å­˜åœ¨çš„æ–‡ä»¶/ç›®å½•
        print(f"\nðŸ“ å®žé™…ç›®å½•å†…å®¹:")
        if dataset_path.exists():
            for item in dataset_path.iterdir():
                if item.is_dir():
                    print(f"   ðŸ“ {item.name}/")
                else:
                    print(f"   ðŸ“„ {item.name}")

        raise FileNotFoundError(
            f"æ— æ³•åœ¨ {dataset_path} ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„LEVIR-CCæ•°æ®ç»“æž„ã€‚\n"
            f"é¢„æœŸç»“æž„:\n"
            f"  - LEVIR-CC/*.arrow æ–‡ä»¶\n"
            f"  - LEVIR-CC/images/train/A å’Œ B\n"
            f"  - LEVIR-CC/A å’Œ B"
        )

    # èŽ·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
    img_a_files = sorted(list(valid_structure['img_a'].glob('*.png')) +
                        list(valid_structure['img_a'].glob('*.jpg')))
    img_b_files = sorted(list(valid_structure['img_b'].glob('*.png')) +
                        list(valid_structure['img_b'].glob('*.jpg')))

    print(f"âœ… æ‰¾åˆ° {len(img_a_files)} å¯¹å›¾åƒ")

    # æž„å»ºæ•°æ®é›†ï¼ˆä½¿ç”¨é»˜è®¤æ ‡æ³¨ï¼‰
    dataset_list = []
    for img_a_path, img_b_path in zip(img_a_files, img_b_files):
        dataset_list.append({
            'A': str(img_a_path),
            'B': str(img_b_path),
            'caption': 'A change has occurred in the remote sensing image.',
            'bbox': [0, 0, 256, 256],  # é»˜è®¤bboxï¼Œå°†åœ¨åŽç»­è¢«å½’ä¸€åŒ–
        })

    return dataset_list


def create_dataloaders(
    batch_size: int = 4,
    num_workers: int = 4,
    test_split: float = 0.1,
    seed: int = 42,
):
    """
    Create train and validation dataloaders from LEVIR-CC dataset

    Args:
        batch_size: Batch size for dataloaders
        num_workers: Number of workers for data loading
        test_split: Proportion of data to use for validation
        seed: Random seed for train/test split

    Returns:
        Tuple of (train_dataloader, val_dataloader)
    """
    if not DATASETS_AVAILABLE:
        raise ImportError("datasets library is required. Install with: pip install datasets")

    print("\n" + "="*60)
    print("åŠ è½½ LEVIR-CC æ•°æ®é›†")
    print("="*60)
    print(f"æ•°æ®é›†è·¯å¾„: {Config.DATASET_PATH}")

    try:
        # é¦–å…ˆå°è¯•ä»ŽArrowæ ¼å¼åŠ è½½
        full_dataset = datasets.load_from_disk(Config.DATASET_PATH)
        print(f"âœ… ä»ŽArrowæ ¼å¼åŠ è½½æˆåŠŸ")
        print(f"   æ•°æ®é›†ç»“æž„: {full_dataset}")

        # Get the appropriate split
        if 'train' in full_dataset:
            dataset_split = full_dataset['train']
            print(f"âœ… ä½¿ç”¨ 'train' åˆ†å‰²ï¼Œå…± {len(dataset_split)} ä¸ªæ ·æœ¬")
        else:
            # If only one split exists, use it
            dataset_split = full_dataset
            print(f"âš ï¸  æœªæ‰¾åˆ° 'train' åˆ†å‰²ã€‚ä½¿ç”¨æ•´ä¸ªæ•°æ®é›†ï¼Œå…± {len(dataset_split)} ä¸ªæ ·æœ¬")


    except (FileNotFoundError, Exception) as e:
        print(f"âš ï¸  Arrowæ ¼å¼åŠ è½½å¤±è´¥: {e}")
        print(f"ðŸ”„ å°è¯•ä»ŽåŽŸå§‹æ–‡ä»¶ç»“æž„åŠ è½½...")

        try:
            # ä»ŽåŽŸå§‹ç»“æž„åŠ è½½ï¼ˆå¯èƒ½è¿”å›ž Dataset æˆ– listï¼‰
            raw_data = load_raw_levir_cc_dataset(Config.DATASET_PATH)

            # æ£€æŸ¥è¿”å›žç±»åž‹
            if isinstance(raw_data, datasets.Dataset):
                # å·²ç»æ˜¯ Dataset å¯¹è±¡ï¼ˆä»Ž Arrow æ–‡ä»¶åŠ è½½ï¼‰
                dataset_split = raw_data
                print(f"âœ… ä»Ž Arrow æ–‡ä»¶åŠ è½½æˆåŠŸï¼Œå…± {len(dataset_split)} ä¸ªæ ·æœ¬")
            else:
                # æ˜¯åˆ—è¡¨ï¼ˆä»Žå›¾åƒç›®å½•åŠ è½½ï¼‰
                # è½¬æ¢ä¸ºHuggingFace Datasetæ ¼å¼
                dataset_dict = {
                    'A': [item['A'] for item in raw_data],
                    'B': [item['B'] for item in raw_data],
                    'caption': [item['caption'] for item in raw_data],
                    'bbox': [item['bbox'] for item in raw_data],
                }

                dataset_split = datasets.Dataset.from_dict(dataset_dict)
                print(f"âœ… ä»Žå›¾åƒç›®å½•åŠ è½½æˆåŠŸï¼Œå…± {len(dataset_split)} ä¸ªæ ·æœ¬")

        except Exception as raw_e:
            print(f"âŒ åŽŸå§‹ç»“æž„åŠ è½½ä¹Ÿå¤±è´¥: {raw_e}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(
                f"æ— æ³•åŠ è½½æ•°æ®é›†ã€‚å°è¯•äº†:\n"
                f"1. Arrowæ ¼å¼ (load_from_disk): {e}\n"
                f"2. åŽŸå§‹ç»“æž„/å•Arrowæ–‡ä»¶: {raw_e}\n\n"
                f"è¯·æ£€æŸ¥æ•°æ®é›†è·¯å¾„å’Œç»“æž„æ˜¯å¦æ­£ç¡®ã€‚"
            )

    # Create train/val split
    # æ³¨æ„ï¼šåœ¨ Kaggle çŽ¯å¢ƒä¸­ï¼Œ/kaggle/input/ æ˜¯åªè¯»çš„
    # æˆ‘ä»¬éœ€è¦è®¾ç½® HF_DATASETS_CACHE åˆ°å¯å†™ç›®å½•
    import os
    import numpy as np

    # ç¡®ä¿ç¼“å­˜ç›®å½•æŒ‡å‘å¯å†™ä½ç½®
    cache_dir = os.path.join(Config.WORKING_DIR, '.cache')
    os.makedirs(cache_dir, exist_ok=True)
    os.environ['HF_DATASETS_CACHE'] = cache_dir

    # å¯¹äºŽåªè¯»æ–‡ä»¶ç³»ç»Ÿï¼ˆKaggleï¼‰ï¼Œä½¿ç”¨æ‰‹åŠ¨åˆ†å‰²è€Œä¸æ˜¯ HF çš„ train_test_split
    # è¿™é¿å…äº†åœ¨è¾“å…¥ç›®å½•ä¸­åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    print(f"ðŸ”„ æ­£åœ¨åˆ†å‰²æ•°æ®é›† (train: {1-test_split:.1%}, test: {test_split:.1%})...")

    n_samples = len(dataset_split)
    indices = np.arange(n_samples)

    # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯é‡çŽ°æ€§
    np.random.seed(seed)

    # éšæœºæ‰“ä¹±ç´¢å¼•
    np.random.shuffle(indices)

    # åˆ†å‰²
    split_point = int(n_samples * (1 - test_split))
    train_indices = indices[:split_point]
    test_indices = indices[split_point:]

    # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŽ’åºï¼ˆä½¿æ•°æ®æ›´è¿žè´¯ï¼‰
    train_indices = sorted(train_indices.tolist())
    test_indices = sorted(test_indices.tolist())

    try:
        # å°è¯•ä½¿ç”¨ select æ–¹æ³•ï¼ˆåœ¨å¯å†™ç³»ç»Ÿä¸­å¯èƒ½ä¼šç¼“å­˜ï¼‰
        split_dataset = {
            'train': dataset_split.select(train_indices),
            'test': dataset_split.select(test_indices)
        }
    except OSError as e:
        if "Read-only file system" in str(e) or "No space left" in str(e):
            print(f"âš ï¸  æ•°æ®é›†æ“ä½œä¸­é‡åˆ°æ–‡ä»¶ç³»ç»Ÿé”™è¯¯ï¼Œä½¿ç”¨ç›´æŽ¥ç´¢å¼•è®¿é—®...")
            # é™çº§æ–¹æ¡ˆï¼šåˆ›å»ºä¸€ä¸ªåŒ…è£…å¯¹è±¡ï¼Œåœ¨è®¿é—®æ—¶è¿›è¡Œè¿‡æ»¤
            class IndexedDataset:
                def __init__(self, dataset, indices):
                    self.dataset = dataset
                    self.indices = indices
                    self._len = len(indices)

                def __len__(self):
                    return self._len

                def __getitem__(self, idx):
                    return self.dataset[self.indices[idx]]

                def train_test_split(self, *args, **kwargs):
                    # é˜²æ­¢å†æ¬¡è°ƒç”¨ train_test_split
                    raise RuntimeError("Cannot split an already indexed dataset")

            split_dataset = {
                'train': IndexedDataset(dataset_split, train_indices),
                'test': IndexedDataset(dataset_split, test_indices)
            }
        else:
            raise

    print(f"âœ… æ•°æ®é›†åˆ†å‰²å®Œæˆ: è®­ç»ƒé›† {len(split_dataset['train'])} ä¸ªæ ·æœ¬ï¼Œæµ‹è¯•é›† {len(split_dataset['test'])} ä¸ªæ ·æœ¬")

    train_dataset = LevirCCActionDataset(
        split_dataset['train'],
        image_size=Config.IMAGE_SIZE,
        max_text_length=Config.MAX_TEXT_LENGTH,
        normalize_bbox=Config.BBOX_NORMALIZE,
    )

    val_dataset = LevirCCActionDataset(
        split_dataset['test'],
        image_size=Config.IMAGE_SIZE,
        max_text_length=Config.MAX_TEXT_LENGTH,
        normalize_bbox=Config.BBOX_NORMALIZE,
    )

    # Create dataloaders
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True,
    )

    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=False,
    )

    print(f"\nâœ… Dataloaders created:")
    print(f"   Train: {len(train_loader)} batches ({len(train_dataset)} samples)")
    print(f"   Val: {len(val_loader)} batches ({len(val_dataset)} samples)")
    print("="*60 + "\n")

    return train_loader, val_loader


if __name__ == "__main__":
    # Test dataset loading
    print("Testing dataset loading...")
    train_loader, val_loader = create_dataloaders(batch_size=2)

    # Get a sample batch
    for batch_idx, batch in enumerate(train_loader):
        print(f"\nBatch {batch_idx}:")
        for key, value in batch.items():
            if isinstance(value, torch.Tensor):
                print(f"  {key}: {value.shape}")
            else:
                print(f"  {key}: {type(value).__name__}")
        break  # Only show first batch

