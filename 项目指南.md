# ğŸ“– é¡¹ç›®æŒ‡å—

> VLM-VLA Agent æŠ€æœ¯æ–‡æ¡£ - æ¶æ„ã€æ¨¡å—å’Œå®ç°ç»†èŠ‚

---

## ğŸ“‹ ç›®å½•

1. [é¡¹ç›®æ¶æ„](#é¡¹ç›®æ¶æ„)
2. [æ ¸å¿ƒæ¨¡å—](#æ ¸å¿ƒæ¨¡å—)
3. [æ•°æ®æµç¨‹](#æ•°æ®æµç¨‹)
4. [è®­ç»ƒæµç¨‹](#è®­ç»ƒæµç¨‹)
5. [é…ç½®è¯¦è§£](#é…ç½®è¯¦è§£)
6. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
7. [æ‰©å±•å¼€å‘](#æ‰©å±•å¼€å‘)

---

## é¡¹ç›®æ¶æ„

### ç³»ç»Ÿæ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VLM-VLA Agent                        â”‚
â”‚                                                         â”‚
â”‚  è¾“å…¥: åŒæ—¶ç›¸å½±åƒ + å˜åŒ–æè¿°æ–‡æœ¬                          â”‚
â”‚                       â†“                                 â”‚
â”‚  [CLIP è§†è§‰ç¼–ç å™¨] â”€â†’ è§†è§‰ç‰¹å¾ (768ç»´)                   â”‚
â”‚                       â†“                                 â”‚
â”‚  [ç‰¹å¾æŠ•å½±å±‚] â”€â†’ LLM åµŒå…¥ç©ºé—´ (1024ç»´)                   â”‚
â”‚                       â†“                                 â”‚
â”‚  [Qwen LLM + LoRA] â”€â†’ è¯­è¨€ç†è§£                          â”‚
â”‚                       â†“                                 â”‚
â”‚  [å¤šæ¨¡æ€èåˆ] â”€â†’ èåˆè¡¨ç¤º                                â”‚
â”‚                       â†“                                 â”‚
â”‚  [åŠ¨ä½œé¢„æµ‹å¤´] â”€â†’ [ä¸­å¿ƒx, ä¸­å¿ƒy, å°ºåº¦]                    â”‚
â”‚                                                         â”‚
â”‚  è¾“å‡º: å½’ä¸€åŒ–åŠ¨ä½œå‘é‡ [0, 1]Â³                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å…³é”®æŠ€æœ¯æ ˆ

| ç»„ä»¶ | æŠ€æœ¯ | ç‰ˆæœ¬ |
|------|------|------|
| æ·±åº¦å­¦ä¹ æ¡†æ¶ | PyTorch | 2.0+ |
| è§†è§‰ç¼–ç å™¨ | CLIP ViT-B/32 | OpenAI |
| è¯­è¨€æ¨¡å‹ | Qwen2.5-0.5B | é˜¿é‡Œäº‘ |
| å¾®è°ƒæ–¹æ³• | LoRA | PEFT |
| é‡åŒ– | 4-bit | BitsAndBytes |
| æ•°æ®é›† | LEVIR-CC | Arrow æ ¼å¼ |

---

## æ ¸å¿ƒæ¨¡å—

### 1. config.py - é…ç½®ç®¡ç†

**åŠŸèƒ½**:
- è‡ªåŠ¨ç¯å¢ƒæ£€æµ‹ï¼ˆKaggle vs æœ¬åœ°ï¼‰
- è·¯å¾„æ˜ å°„å’Œç®¡ç†
- è¶…å‚æ•°é…ç½®

**å…³é”®ç±»**:
```python
class Config:
    # ç¯å¢ƒæ£€æµ‹
    IS_KAGGLE = os.path.exists("/kaggle/input")

    # è·¯å¾„é…ç½®
    if IS_KAGGLE:
        DATASET_PATH = "/kaggle/input/levir-cc-dateset/LEVIR-CC"
        CLIP_PATH = "/kaggle/input/clip-vit-b32"
        LLM_PATH = "/kaggle/input/qwen2.5-0.5b"
    else:
        # æœ¬åœ°è·¯å¾„
        DATASET_PATH = "data/"
        CLIP_PATH = "models/clip-vit-b32"
        LLM_PATH = "models/qwen2.5-0.5b"

    # è®­ç»ƒè¶…å‚æ•°
    MAX_EPOCHS = 10
    BATCH_SIZE = 4
    LEARNING_RATE = 1e-4

    # LoRA é…ç½®
    LORA_R = 8
    LORA_ALPHA = 16
    LORA_DROPOUT = 0.05
```

**ä½¿ç”¨æ–¹å¼**:
```python
from src.config import Config

# æ‰“å°é…ç½®
Config.print_config()

# éªŒè¯è·¯å¾„
Config.verify_paths()
```

---

### 2. dataset.py - æ•°æ®åŠ è½½

**åŠŸèƒ½**:
- æ”¯æŒå¤šç§æ•°æ®æ ¼å¼ï¼šArrowã€åŸå§‹å›¾åƒç›®å½•ã€JSON æ ‡æ³¨
- è‡ªåŠ¨æ£€æµ‹æ•°æ®ç»“æ„å’Œåˆ†å‰²ï¼ˆtrain/val/testï¼‰
- å›¾åƒé¢„å¤„ç†å’Œå¢å¼º
- è¾¹ç•Œæ¡†è½¬æ¢ä¸ºåŠ¨ä½œå‘é‡
- é”™è¯¯å¤„ç†å’Œæ•°æ®åŠ è½½å®¹é”™

**å…³é”®ç±»**:
```python
class LevirCCActionDataset(Dataset):
    def __init__(self, dataset_split, image_size=224):
        # åˆå§‹åŒ–æ•°æ®é›†
        self.dataset = dataset_split
        self.image_transform = transforms.Compose([
            transforms.Resize((image_size, image_size)),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.48145466, 0.4578275, 0.40821073],  # CLIP æ ‡å‡†åŒ–
                std=[0.26862954, 0.26130258, 0.27577711]
            )
        ])

    def __getitem__(self, idx):
        # è¿”å›æ•°æ®æ ·æœ¬
        return {
            'image_t1': Tensor(3, 224, 224),      # æ—¶é—´1å½±åƒ
            'image_t2': Tensor(3, 224, 224),      # æ—¶é—´2å½±åƒ
            'caption': str,                        # å˜åŒ–æè¿°
            'action_vector': Tensor(3,),           # [cx, cy, scale]
            'bbox': [x1, y1, x2, y2],            # åŸå§‹è¾¹ç•Œæ¡†
        }
```

**æ•°æ®è½¬æ¢æµç¨‹**:
```
åŸå§‹æ•°æ®
  â”œâ”€ å›¾åƒ â†’ Resize(224, 224) â†’ ToTensor â†’ Normalize
  â”œâ”€ æ–‡æœ¬ â†’ ç›´æ¥ä¼ é€’ï¼ˆåœ¨æ¨¡å‹ä¸­ tokenizeï¼‰
  â””â”€ BBox [x1,y1,x2,y2] â†’ åŠ¨ä½œå‘é‡ [cx,cy,scale]
                           cx = (x1 + x2) / 2 / width
                           cy = (y1 + y2) / 2 / height
                           scale = sqrt(area) / sqrt(image_area)
```

**ä½¿ç”¨æ–¹å¼**:
```python
from src.dataset import create_dataloaders

train_loader, val_loader = create_dataloaders(
    batch_size=4,
    num_workers=4,
    test_split=0.1,
)

for batch in train_loader:
    images_t1 = batch['image_t1']  # (B, 3, 224, 224)
    images_t2 = batch['image_t2']  # (B, 3, 224, 224)
    captions = batch['caption']    # List[str]
    actions = batch['action_vector']  # (B, 3)
```

---

### 3. model.py - æ¨¡å‹æ¶æ„

**åŠŸèƒ½**:
- CLIP è§†è§‰ç¼–ç å™¨ï¼ˆå†»ç»“ï¼‰
- Qwen LLMï¼ˆLoRA å¾®è°ƒï¼‰
- ç‰¹å¾èåˆ
- åŠ¨ä½œé¢„æµ‹å¤´

**æ¨¡å‹æ¶æ„**:
```python
class VLM_ActionAgent(nn.Module):
    def __init__(self):
        # 1. CLIP è§†è§‰ç¼–ç å™¨ï¼ˆå†»ç»“ï¼‰
        self.clip_model = CLIPModel.from_pretrained(...)
        for param in self.clip_model.vision_model.parameters():
            param.requires_grad = False

        # 2. LLMï¼ˆ4ä½é‡åŒ– + LoRAï¼‰
        bnb_config = BitsAndBytesConfig(load_in_4bit=True, ...)
        self.llm_model = AutoModelForCausalLM.from_pretrained(
            ..., quantization_config=bnb_config
        )

        # åº”ç”¨ LoRA
        lora_config = LoraConfig(r=8, lora_alpha=16, ...)
        self.llm_model = get_peft_model(self.llm_model, lora_config)

        # 3. æŠ•å½±å±‚
        self.projector = nn.Linear(512, 1024)

        # 4. åŠ¨ä½œé¢„æµ‹å¤´
        self.action_head = nn.Sequential(
            nn.Linear(1024, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 3),
            nn.Sigmoid()  # å½’ä¸€åŒ–åˆ° [0, 1]
        )
```

**å‰å‘ä¼ æ’­æµç¨‹**:
```
è¾“å…¥: images_t1, images_t2, captions
  â†“
è§†è§‰ç¼–ç :
  vision_feat_t1 = clip_model.vision_model(images_t1)  # (B, 512)
  vision_feat_t2 = clip_model.vision_model(images_t2)  # (B, 512)
  temporal_feat = concat([vision_feat_t1, vision_feat_t2])  # (B, 1024)
  projected_feat = projector(temporal_feat)  # (B, 1024)
  â†“
è¯­è¨€ç¼–ç :
  tokens = tokenizer(captions)
  llm_out = llm_model(tokens)
  llm_feat = llm_out.hidden_states[-1][:, 0, :]  # (B, 1024)
  â†“
ç‰¹å¾èåˆ:
  fused_feat = projected_feat + llm_feat  # (B, 1024)
  â†“
åŠ¨ä½œé¢„æµ‹:
  action_pred = action_head(fused_feat)  # (B, 3)
  â†“
è¾“å‡º: action_pred = [cx, cy, scale] âˆˆ [0, 1]Â³
```

**å‚æ•°ç»Ÿè®¡**:
```python
æ€»å‚æ•°: ~500M
å¯è®­ç»ƒå‚æ•°: ~2M (LoRA + æŠ•å½±å±‚ + åŠ¨ä½œå¤´)
å†»ç»“å‚æ•°: ~498M (CLIP è§†è§‰ç¼–ç å™¨)
è®­ç»ƒæ¯”ä¾‹: ~0.4%
```

---

### 4. train.py - è®­ç»ƒæµç¨‹

**åŠŸèƒ½**:
- å®Œæ•´è®­ç»ƒå¾ªç¯
- æ··åˆç²¾åº¦è®­ç»ƒ
- æ¢¯åº¦ç´¯ç§¯å’Œè£å‰ª
- æ£€æŸ¥ç‚¹ä¿å­˜
- æŒ‡æ ‡è®°å½•

**Trainer ç±»**:
```python
class Trainer:
    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.global_step = 0
        self.epoch = 0
        self.best_loss = float('inf')

    def load_data(self):
        # åŠ è½½è®­ç»ƒå’ŒéªŒè¯æ•°æ®
        self.train_loader, self.val_loader = create_dataloaders(...)

    def setup_model(self):
        # åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨
        self.model = create_model(...)
        self.optimizer = optim.AdamW(...)
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(...)
        self.scaler = GradScaler()  # æ··åˆç²¾åº¦

    def train_epoch(self):
        # ä¸€ä¸ª epoch çš„è®­ç»ƒ
        for batch in self.train_loader:
            # å‰å‘ä¼ æ’­ï¼ˆæ··åˆç²¾åº¦ï¼‰
            with autocast(dtype=torch.bfloat16):
                outputs = self.model(...)
                loss = outputs['total_loss']

            # åå‘ä¼ æ’­ï¼ˆæ¢¯åº¦ç¼©æ”¾ï¼‰
            self.scaler.scale(loss).backward()

            # æ¢¯åº¦è£å‰ªå’Œä¼˜åŒ–å™¨æ­¥è¿›
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(..., max_norm=1.0)
            self.scaler.step(self.optimizer)
            self.scaler.update()

    def validate(self):
        # éªŒè¯å¾ªç¯
        with torch.no_grad():
            for batch in self.val_loader:
                outputs = self.model(...)
                # è®¡ç®—éªŒè¯æŸå¤±

    def save_checkpoint(self, step, is_best=False):
        # ä¿å­˜æ£€æŸ¥ç‚¹
        checkpoint = {
            'epoch': self.epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'metrics': self.metrics,
        }
        torch.save(checkpoint, f"checkpoint_step_{step}.pt")
```

**è®­ç»ƒå¾ªç¯**:
```
for epoch in range(MAX_EPOCHS):
    â”œâ”€ train_epoch():
    â”‚   â”œâ”€ å‰å‘ä¼ æ’­ (æ··åˆç²¾åº¦)
    â”‚   â”œâ”€ è®¡ç®—æŸå¤±
    â”‚   â”œâ”€ åå‘ä¼ æ’­ (æ¢¯åº¦ç´¯ç§¯)
    â”‚   â”œâ”€ æ¢¯åº¦è£å‰ª
    â”‚   â”œâ”€ ä¼˜åŒ–å™¨æ­¥è¿›
    â”‚   â””â”€ å­¦ä¹ ç‡è°ƒåº¦
    â”œâ”€ validate():
    â”‚   â””â”€ è¯„ä¼°éªŒè¯é›†
    â””â”€ save_checkpoint():
        â”œâ”€ å®šæœŸä¿å­˜ (æ¯ 500 æ­¥)
        â””â”€ ä¿å­˜æœ€ä½³æ¨¡å‹
```

---

### 5. utils.py - å·¥å…·å‡½æ•°

**ä¸»è¦å‡½æ•°**:

| å‡½æ•° | åŠŸèƒ½ |
|------|------|
| `set_seed(seed)` | è®¾ç½®éšæœºç§å­ |
| `get_device()` | è·å– GPU/CPU è®¾å¤‡ |
| `count_parameters(model)` | ç»Ÿè®¡æ¨¡å‹å‚æ•° |
| `print_gpu_memory()` | æ˜¾ç¤º GPU å†…å­˜ |
| `load_checkpoint(path, model)` | åŠ è½½æ£€æŸ¥ç‚¹ |
| `format_time(seconds)` | æ ¼å¼åŒ–æ—¶é—´ |
| `AverageMeter` | è®¡ç®—å¹³å‡å€¼ |

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from src.utils import (
    set_seed,
    get_device,
    print_model_summary,
    print_gpu_memory
)

# è®¾ç½®éšæœºç§å­
set_seed(42)

# è·å–è®¾å¤‡
device = get_device()

# æ‰“å°æ¨¡å‹æ‘˜è¦
print_model_summary(model)

# ç›‘æ§ GPU å†…å­˜
print_gpu_memory()
```

---

## æ•°æ®æµç¨‹

### Arrow æ•°æ®é›†ç»“æ„

```
LEVIR-CC/
â”œâ”€â”€ dataset_info.json           # æ•°æ®é›†å…ƒä¿¡æ¯
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ data-00000.arrow       # è®­ç»ƒæ•°æ®
â”‚   â””â”€â”€ state.json
â””â”€â”€ validation/
    â”œâ”€â”€ data-00000.arrow       # éªŒè¯æ•°æ®
    â””â”€â”€ state.json
```

### æ•°æ®æ ·æœ¬æ ¼å¼

```python
{
    'image': PIL.Image (H, W, 3),      # æ—¶é—´1å½±åƒ
    'image2': PIL.Image (H, W, 3),     # æ—¶é—´2å½±åƒ
    'caption': str,                     # "building area increased"
    'bbox': [x1, y1, x2, y2],          # [100, 150, 300, 350]
}
```

### æ•°æ®é¢„å¤„ç†æµç¨‹

```
1. åŠ è½½ Arrow æ•°æ®
   â”œâ”€ datasets.load_from_disk(path)
   â””â”€ è‡ªåŠ¨æ£€æµ‹æ•°æ®ç»“æ„

2. å›¾åƒé¢„å¤„ç†
   â”œâ”€ Resize â†’ (224, 224)
   â”œâ”€ ToTensor â†’ (3, 224, 224)
   â””â”€ Normalize â†’ CLIP æ ‡å‡†åŒ–

3. BBox è½¬æ¢
   â”œâ”€ è®¡ç®—ä¸­å¿ƒç‚¹: cx = (x1+x2)/(2*W)
   â”œâ”€ è®¡ç®—ä¸­å¿ƒç‚¹: cy = (y1+y2)/(2*H)
   â””â”€ è®¡ç®—å°ºåº¦: scale = sqrt(area/total_area)

4. æ‰¹æ¬¡ç»„è£…
   â””â”€ DataLoader â†’ (B, ...)
```

---

## è®­ç»ƒæµç¨‹

### å®Œæ•´è®­ç»ƒç®¡é“

```
1. åˆå§‹åŒ–é˜¶æ®µ
   â”œâ”€ åŠ è½½é…ç½®
   â”œâ”€ éªŒè¯è·¯å¾„
   â”œâ”€ åˆ›å»ºè¾“å‡ºç›®å½•
   â””â”€ è®¾ç½®éšæœºç§å­

2. æ•°æ®åŠ è½½
   â”œâ”€ åŠ è½½ Arrow æ•°æ®é›†
   â”œâ”€ åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›† (9:1)
   â””â”€ åˆ›å»º DataLoader

3. æ¨¡å‹åˆå§‹åŒ–
   â”œâ”€ åŠ è½½ CLIPï¼ˆå†»ç»“ï¼‰
   â”œâ”€ åŠ è½½ Qwenï¼ˆ4ä½é‡åŒ–ï¼‰
   â”œâ”€ åº”ç”¨ LoRA
   â””â”€ åˆå§‹åŒ–åŠ¨ä½œé¢„æµ‹å¤´

4. ä¼˜åŒ–å™¨è®¾ç½®
   â”œâ”€ AdamW ä¼˜åŒ–å™¨
   â”œâ”€ ä½™å¼¦é€€ç«è°ƒåº¦å™¨
   â””â”€ æ··åˆç²¾åº¦ Scaler

5. è®­ç»ƒå¾ªç¯ (æ¯ä¸ª epoch)
   â”œâ”€ è®­ç»ƒé˜¶æ®µ
   â”‚   â”œâ”€ å‰å‘ä¼ æ’­
   â”‚   â”œâ”€ è®¡ç®—æŸå¤±
   â”‚   â”œâ”€ åå‘ä¼ æ’­
   â”‚   â”œâ”€ æ¢¯åº¦è£å‰ª
   â”‚   â””â”€ ä¼˜åŒ–å™¨æ­¥è¿›
   â”œâ”€ éªŒè¯é˜¶æ®µ
   â”‚   â”œâ”€ è¯„ä¼°éªŒè¯é›†
   â”‚   â””â”€ è®¡ç®—éªŒè¯æŸå¤±
   â””â”€ æ£€æŸ¥ç‚¹ä¿å­˜
       â”œâ”€ å®šæœŸä¿å­˜ (æ¯ 500 æ­¥)
       â””â”€ æœ€ä½³æ¨¡å‹ä¿å­˜

6. è®­ç»ƒå®Œæˆ
   â”œâ”€ ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹
   â”œâ”€ ä¿å­˜è®­ç»ƒæŒ‡æ ‡
   â””â”€ æ‰“å°è®­ç»ƒæ€»ç»“
```

### æŸå¤±å‡½æ•°

```python
# åŠ¨ä½œé¢„æµ‹æŸå¤±ï¼ˆMSEï¼‰
action_loss = MSELoss(action_pred, action_target)

# æ€»æŸå¤±
total_loss = action_loss
```

### å­¦ä¹ ç‡è°ƒåº¦

```python
# ä½™å¼¦é€€ç«
scheduler = CosineAnnealingLR(
    optimizer,
    T_max=total_steps,
    eta_min=lr * 0.1
)

# å­¦ä¹ ç‡å˜åŒ–æ›²çº¿
lr_start = 1e-4
lr_min = 1e-5
lr(t) = lr_min + (lr_start - lr_min) * (1 + cos(Ï€t/T_max)) / 2
```

---

## é…ç½®è¯¦è§£

### è®­ç»ƒè¶…å‚æ•°

```python
# åŸºæœ¬è®¾ç½®
MAX_EPOCHS = 10              # è®­ç»ƒè½®æ•°
BATCH_SIZE = 4               # æ‰¹æ¬¡å¤§å°
LEARNING_RATE = 1e-4         # å­¦ä¹ ç‡
WEIGHT_DECAY = 0.01          # æƒé‡è¡°å‡

# ä¼˜åŒ–è®¾ç½®
MAX_GRAD_NORM = 1.0          # æ¢¯åº¦è£å‰ª
WARMUP_STEPS = 500           # é¢„çƒ­æ­¥æ•°
GRADIENT_ACCUMULATION_STEPS = 1  # æ¢¯åº¦ç´¯ç§¯

# ä¿å­˜å’Œè¯„ä¼°
SAVE_INTERVAL = 500          # ä¿å­˜é—´éš”
EVAL_INTERVAL = 1000         # è¯„ä¼°é—´éš”
LOG_INTERVAL = 100           # æ—¥å¿—é—´éš”
```

### æ¨¡å‹è¶…å‚æ•°

```python
# CLIP è®¾ç½®
VISION_MODEL_NAME = "openai/clip-vit-base-patch32"
VISION_HIDDEN_DIM = 512  # CLIP ViT-B/32 å®é™…è¾“å‡ºç»´åº¦
VISION_OUTPUT_DIM = 512  # æŠ•å½±å±‚è¾“å…¥ç»´åº¦ï¼ˆåŒæ—¶ç›¸ 512*2=1024ï¼‰

# Qwen è®¾ç½®
LLM_MODEL_NAME = "Qwen/Qwen2.5-0.5B"
LLM_HIDDEN_DIM = 1024  # LLM éšè—ç»´åº¦å’ŒæŠ•å½±å±‚è¾“å‡ºç»´åº¦
LLM_NUM_LAYERS = 24

# LoRA è®¾ç½®
LORA_R = 8                   # LoRA ç§©
LORA_ALPHA = 16              # ç¼©æ”¾å› å­
LORA_DROPOUT = 0.05          # Dropout
LORA_TARGET_MODULES = ["q_proj", "v_proj"]  # ç›®æ ‡å±‚

# é‡åŒ–è®¾ç½®
USE_4BIT = True              # ä½¿ç”¨ 4 ä½é‡åŒ–
COMPUTE_DTYPE = "bfloat16"   # è®¡ç®—ç²¾åº¦
```

### æ•°æ®è¶…å‚æ•°

```python
IMAGE_SIZE = 224             # CLIP è¾“å…¥å°ºå¯¸
MAX_TEXT_LENGTH = 128        # æ–‡æœ¬æœ€å¤§é•¿åº¦
BBOX_NORMALIZE = True        # è¾¹ç•Œæ¡†å½’ä¸€åŒ–
NUM_WORKERS = 4              # DataLoader å·¥ä½œè¿›ç¨‹æ•°
```

---

## æ€§èƒ½ä¼˜åŒ–

### å†…å­˜ä¼˜åŒ–æŠ€æœ¯

1. **4ä½é‡åŒ–**
```python
# å‡å°‘ LLM å†…å­˜å ç”¨ 75%
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)
```

2. **LoRA å¾®è°ƒ**
```python
# åªè®­ç»ƒ 0.4% çš„å‚æ•°
lora_config = LoraConfig(r=8, lora_alpha=16)
model = get_peft_model(model, lora_config)
```

3. **æ··åˆç²¾åº¦è®­ç»ƒ**
```python
# ä½¿ç”¨ bfloat16 å‡å°‘å†…å­˜å’ŒåŠ é€Ÿ
with autocast(dtype=torch.bfloat16):
    outputs = model(...)
```

4. **æ¢¯åº¦æ£€æŸ¥ç‚¹**
```python
# ç”¨è®¡ç®—æ¢å†…å­˜
model.gradient_checkpointing_enable()
```

### è®­ç»ƒåŠ é€ŸæŠ€æœ¯

1. **æ¢¯åº¦ç´¯ç§¯**
```python
# æ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹æ¬¡
GRADIENT_ACCUMULATION_STEPS = 4
effective_batch_size = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS
```

2. **æ•°æ®åŠ è½½ä¼˜åŒ–**
```python
# å¤šè¿›ç¨‹æ•°æ®åŠ è½½
DataLoader(..., num_workers=8, pin_memory=True)
```

3. **ç¼–è¯‘ä¼˜åŒ–**
```python
# PyTorch 2.0 ç¼–è¯‘
model = torch.compile(model)
```

---

## æ‰©å±•å¼€å‘

### æ·»åŠ æ–°çš„æ•°æ®é›†

1. **å®ç°è‡ªå®šä¹‰ Dataset**:
```python
class CustomDataset(Dataset):
    def __init__(self, data_path):
        # åŠ è½½æ•°æ®
        pass

    def __getitem__(self, idx):
        return {
            'image_t1': ...,
            'image_t2': ...,
            'caption': ...,
            'action_vector': ...,
        }
```

2. **ä¿®æ”¹ dataset.py**:
```python
def create_custom_dataloaders(...):
    dataset = CustomDataset(...)
    loader = DataLoader(dataset, ...)
    return loader
```

### ä¿®æ”¹æ¨¡å‹æ¶æ„

1. **æ›´æ¢è§†è§‰ç¼–ç å™¨**:
```python
# æ›¿æ¢ CLIP
from transformers import ViTModel
self.vision_model = ViTModel.from_pretrained("google/vit-base-patch16-224")
```

2. **æ›´æ¢è¯­è¨€æ¨¡å‹**:
```python
# æ›¿æ¢ Qwen
from transformers import AutoModelForCausalLM
self.llm_model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b")
```

3. **ä¿®æ”¹èåˆæ–¹å¼**:
```python
# ä»ç›¸åŠ æ”¹ä¸ºæ‹¼æ¥ + MLP
fused_feat = torch.cat([vision_feat, llm_feat], dim=-1)  # (B, 2048)
fused_feat = self.fusion_mlp(fused_feat)  # (B, 1024)
```

### æ·»åŠ æ–°çš„æŸå¤±å‡½æ•°

```python
# åœ¨ model.py ä¸­æ·»åŠ 
class VLM_ActionAgent(nn.Module):
    def __init__(self):
        # ...
        self.focal_loss = FocalLoss()

    def forward(self, ...):
        # ...
        if action_targets is not None:
            action_loss = self.focal_loss(action_pred, action_targets)
            # æˆ–ç»„åˆå¤šä¸ªæŸå¤±
            total_loss = action_loss + 0.1 * auxiliary_loss
```

---

## ğŸ“ æœ€ä½³å®è·µ

### è®­ç»ƒå»ºè®®

1. **å…ˆå°è§„æ¨¡æµ‹è¯•**
```python
# åœ¨ config.py ä¸­
DEBUG = True  # åªè®­ç»ƒ 5 ä¸ªæ‰¹æ¬¡
BATCH_SIZE = 2  # å°æ‰¹æ¬¡æµ‹è¯•
```

2. **ç›‘æ§ GPU å†…å­˜**
```python
from src.utils import print_gpu_memory
print_gpu_memory()  # å®šæœŸæ£€æŸ¥
```

3. **ä¿å­˜é…ç½®**
```python
from src.utils import save_config_as_json
save_config_as_json(Config, "config.json")
```

4. **ç‰ˆæœ¬æ§åˆ¶**
```bash
git commit -m "Training run with lr=1e-4"
```

### è°ƒè¯•æŠ€å·§

1. **ä½¿ç”¨ DEBUG æ¨¡å¼**
```python
Config.DEBUG = True  # å¿«é€Ÿè¿­ä»£
```

2. **æ‰“å°æ¨¡å‹ç»“æ„**
```python
from src.utils import print_model_summary
print_model_summary(model)
```

3. **éªŒè¯æ•°æ®**
```python
# å¯è§†åŒ–ä¸€ä¸ªæ‰¹æ¬¡
batch = next(iter(train_loader))
print(batch['image_t1'].shape)
print(batch['caption'][0])
```

---

## ğŸ“š å‚è€ƒèµ„æº

### æ ¸å¿ƒè®ºæ–‡
- CLIP: https://arxiv.org/abs/2103.14030
- Qwen: https://arxiv.org/abs/2309.16609
- LoRA: https://arxiv.org/abs/2106.09685

### å·¥å…·æ–‡æ¡£
- PyTorch: https://pytorch.org/docs
- Transformers: https://huggingface.co/docs/transformers
- PEFT: https://huggingface.co/docs/peft

---

[è¿”å› README](./README.md) | [æŸ¥çœ‹å¿«é€Ÿå¼€å§‹](./å¿«é€Ÿå¼€å§‹.md)

