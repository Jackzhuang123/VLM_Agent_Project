#!/usr/bin/env python
"""
åœ¨ Kaggle ä¸Šä½¿ç”¨å·²è®­ç»ƒæ¨¡å‹è¿›è¡ŒéªŒè¯

åŠŸèƒ½ï¼š
    - åŠ è½½å·²è®­ç»ƒçš„æ£€æŸ¥ç‚¹
    - åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹
    - è®¡ç®—éªŒè¯æŒ‡æ ‡
    - ç”ŸæˆéªŒè¯æŠ¥å‘Š
    - å¯è§†åŒ–éªŒè¯ç»“æœï¼ˆå¯é€‰ï¼‰
    - ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½åˆ†æ

ä½¿ç”¨æ–¹å¼ï¼š
    # Kaggle ç¯å¢ƒï¼ˆè‡ªåŠ¨æŸ¥æ‰¾æ¨¡å‹ï¼‰
    python validation_on_kaggle.py --checkpoint checkpoint_best.pt

    # æˆ–æŒ‡å®šå®Œæ•´è·¯å¾„
    python validation_on_kaggle.py \
        --checkpoint /kaggle/input/vla-model/checkpoint_best.pt \
        --batch-size 4 \
        --num-workers 4 \
        --visualize \
        --save-samples 10

    # æœ¬åœ°ç¯å¢ƒ
    python validation_on_kaggle.py \
        --checkpoint output/checkpoint_20260117_070153/checkpoint_best.pt
"""

import argparse
import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional, List

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm

try:
    import matplotlib.pyplot as plt
    import matplotlib
    matplotlib.use('Agg')  # Use non-interactive backend
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    print("âš ï¸  matplotlib æœªå®‰è£…ï¼Œè·³è¿‡å¯è§†åŒ–åŠŸèƒ½")

from src.config import Config
from src.dataset import LevirCCActionDataset
from src.model import create_model


def get_checkpoint_path(checkpoint_name_or_path):
    """
    è·å–æ£€æŸ¥ç‚¹æ–‡ä»¶çš„å®Œæ•´è·¯å¾„

    æ”¯æŒå¤šç§æ–¹å¼ï¼š
    1. å®Œæ•´è·¯å¾„: /kaggle/input/vla-model/checkpoint_best.pt
    2. ç›¸å¯¹è·¯å¾„: output/checkpoint_best.pt
    3. ä»…æ–‡ä»¶å: checkpoint_best.pt

    ä¼˜å…ˆçº§: /kaggle/input/vla-model > local output > å‚æ•°è·¯å¾„
    """
    # Kaggle æ¨¡å‹æ•°æ®é›†é»˜è®¤ä½ç½®
    kaggle_model_paths = [
        "/kaggle/input/vla-model/checkpoint_best.pt",
        "/kaggle/input/vla-model",
        "/kaggle/input/model-data-set",
    ]

    # é¦–å…ˆæ£€æŸ¥ Kaggle è¾“å…¥ç›®å½•
    for path in kaggle_model_paths:
        if Path(path).exists():
            if Path(path).is_file():
                return path
            # å¦‚æœæ˜¯ç›®å½•ï¼ŒæŸ¥æ‰¾ checkpoint_best.pt
            checkpoint_path = Path(path) / "checkpoint_best.pt"
            if checkpoint_path.exists():
                return str(checkpoint_path)

    # å…¶æ¬¡æ£€æŸ¥æœ¬åœ°è¾“å‡ºç›®å½•
    local_paths = [
        f"output/{checkpoint_name_or_path}",
        checkpoint_name_or_path,
    ]

    for path in local_paths:
        if Path(path).exists():
            return path

    # è¿”å›å‚æ•°æä¾›çš„è·¯å¾„ï¼ˆå³ä½¿ä¸å­˜åœ¨ï¼Œè®©ä¸»å‡½æ•°æŠ¥é”™ï¼‰
    return checkpoint_name_or_path


def load_validation_data():
    """
    åŠ è½½éªŒè¯æ•°æ®

    æ”¯æŒå¤šç§æ•°æ®ç»“æ„ï¼š
    1. å›¾åƒç›®å½•ç»“æ„ï¼šimages/test/, images/train/, images/val/
    2. Arrow æ ¼å¼ï¼ˆé€šè¿‡ datasets åº“ï¼‰

    ä¼˜å…ˆçº§:
    1. /kaggle/input/levir-cc-dataset (Kaggle - å›¾åƒç›®å½•)
    2. Config.DATASET_PATH (æœ¬åœ°é…ç½®)
    """
    print("\n" + "="*60)
    print("åŠ è½½éªŒè¯æ•°æ®")
    print("="*60)

    # é¦–å…ˆæ£€æµ‹æ•°æ®é›†ä½ç½®
    if Path("/kaggle/input/levir-cc-dataset").exists():
        dataset_path = "/kaggle/input/levir-cc-dataset"
        print(f"âœ… æ£€æµ‹åˆ° Kaggle ç¯å¢ƒï¼Œä½¿ç”¨æ•°æ®é›†: {dataset_path}")
    else:
        dataset_path = Config.DATASET_PATH
        print(f"ğŸ“ ä½¿ç”¨æœ¬åœ°æ•°æ®é›†è·¯å¾„: {dataset_path}")

    # æ£€æŸ¥æ˜¯å¦æ˜¯å›¾åƒç›®å½•ç»“æ„
    images_dir = Path(dataset_path) / "images"
    if images_dir.exists() and images_dir.is_dir():
        print(f"ğŸ“¸ æ£€æµ‹åˆ°å›¾åƒç›®å½•ç»“æ„: {images_dir}")

        # ä¼˜å…ˆçº§: test > val > validation > train
        split_order = ["test", "val", "validation", "train"]
        split_path = None

        for split_name in split_order:
            candidate_path = images_dir / split_name
            if candidate_path.exists() and candidate_path.is_dir():
                # æ£€æŸ¥ç›®å½•ä¸­æ˜¯å¦æœ‰å­æ–‡ä»¶å¤¹ï¼ˆä½¿ç”¨é«˜æ•ˆçš„ os.scandirï¼‰
                try:
                    subdirs = []
                    with os.scandir(candidate_path) as entries:
                        for entry in entries:
                            if entry.is_dir(follow_symlinks=False):
                                subdirs.append(entry.name)

                    if subdirs:
                        split_path = candidate_path
                        print(f"âœ… æ‰¾åˆ° '{split_name}' åˆ†å‰²ï¼ŒåŒ…å« {len(subdirs)} ä¸ªæ ·æœ¬é›†åˆ")
                        break
                except OSError:
                    continue

        if split_path is None:
            print(f"âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„æµ‹è¯•åˆ†å‰²")
            return None

        # åŠ è½½å›¾åƒç›®å½•æ•°æ®
        try:
            from PIL import Image

            # æ£€æŸ¥æ˜¯å¦æ˜¯ A/B ç›®å½•ç»“æ„ï¼ˆ2000+ å›¾ç‰‡æƒ…å†µï¼‰
            a_dir = split_path / "A"
            b_dir = split_path / "B"

            if a_dir.exists() and b_dir.exists():
                # âœ… æ–°çš„ A/B ç›®å½•ç»“æ„
                print(f"ğŸ“‚ æ£€æµ‹åˆ° A/B ç›®å½•ç»“æ„: {split_path}/A å’Œ {split_path}/B")

                # åŠ è½½ A ç›®å½•ä¸­çš„æ‰€æœ‰å›¾ç‰‡
                a_images = []
                try:
                    with os.scandir(a_dir) as entries:
                        for entry in entries:
                            if entry.is_file(follow_symlinks=False) and entry.name.lower().endswith(('.png', '.jpg', '.jpeg')):
                                a_images.append((entry.path, entry.name))
                except OSError as e:
                    print(f"âš ï¸  è¯»å– A ç›®å½•å¤±è´¥: {e}")
                    return None

                # åŠ è½½ B ç›®å½•ä¸­çš„æ‰€æœ‰å›¾ç‰‡
                b_images = []
                try:
                    with os.scandir(b_dir) as entries:
                        for entry in entries:
                            if entry.is_file(follow_symlinks=False) and entry.name.lower().endswith(('.png', '.jpg', '.jpeg')):
                                b_images.append((entry.path, entry.name))
                except OSError as e:
                    print(f"âš ï¸  è¯»å– B ç›®å½•å¤±è´¥: {e}")
                    return None

                # æ’åºä»¥ä¿è¯ä¸€è‡´æ€§
                a_images.sort(key=lambda x: x[1])
                b_images.sort(key=lambda x: x[1])

                # åˆ›å»ºé…å¯¹ï¼šæŒ‰ç…§ç›¸åŒçš„ç´¢å¼•é…å¯¹
                samples = []
                num_pairs = min(len(a_images), len(b_images))

                for idx in range(num_pairs):
                    samples.append({
                        'image_a': a_images[idx][0],
                        'image_b': b_images[idx][0],
                        'sample_id': f"{a_images[idx][1][:20]}_{b_images[idx][1][:20]}"  # ä½¿ç”¨æ–‡ä»¶åä½œä¸º ID
                    })

                if not samples:
                    print(f"âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„å›¾åƒå¯¹")
                    return None

                print(f"âœ… A ç›®å½•æ‰¾åˆ° {len(a_images)} å¼ å›¾ç‰‡")
                print(f"âœ… B ç›®å½•æ‰¾åˆ° {len(b_images)} å¼ å›¾ç‰‡")
                print(f"âœ… åˆ›å»ºäº† {len(samples)} ä¸ªå›¾åƒå¯¹")

            else:
                # âš ï¸ åŸå§‹çš„æ ·æœ¬æ–‡ä»¶å¤¹ç»“æ„
                print(f"ğŸ“‚ ä½¿ç”¨æ ·æœ¬æ–‡ä»¶å¤¹ç»“æ„: {split_path}")

                # æ„å»ºæ•°æ®é›† - ä½¿ç”¨ os.scandir ä»£æ›¿ globï¼ˆæ›´é«˜æ•ˆï¼‰
                samples = []

                # ä½¿ç”¨ os.scandir è¿›è¡Œé«˜æ•ˆçš„ç›®å½•éå†
                sample_dirs = []
                try:
                    with os.scandir(split_path) as entries:
                        for entry in entries:
                            if entry.is_dir(follow_symlinks=False):
                                sample_dirs.append(entry.path)
                except OSError as e:
                    print(f"âš ï¸  ç›®å½•éå†å¤±è´¥: {e}")
                    return None

                sample_dirs.sort()  # æ’åºä»¥ä¿è¯ä¸€è‡´æ€§

                for sample_dir_path in sample_dirs:
                    sample_dir_name = os.path.basename(sample_dir_path)

                    # é«˜æ•ˆæŸ¥æ‰¾ A å’Œ B å›¾åƒ - åªæ‰«æä¸€æ¬¡
                    img_a_path = None
                    img_b_path = None
                    img_files = []

                    try:
                        with os.scandir(sample_dir_path) as entries:
                            for entry in entries:
                                if entry.is_file(follow_symlinks=False) and entry.name.lower().endswith(('.png', '.jpg', '.jpeg')):
                                    file_lower = entry.name.lower()
                                    img_files.append((entry.path, file_lower))

                                    # å¿«é€Ÿæ£€æŸ¥ A/B æ ‡è®°
                                    if 'a' in file_lower and img_a_path is None:
                                        img_a_path = entry.path
                                    elif 'b' in file_lower and img_b_path is None:
                                        img_b_path = entry.path
                    except OSError:
                        continue

                    # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„ A/Bï¼ŒæŒ‰å­—æ¯é¡ºåºå–å‰ä¸¤å¼ 
                    if img_a_path is None or img_b_path is None:
                        if len(img_files) >= 2:
                            img_files.sort(key=lambda x: x[0])  # æŒ‰è·¯å¾„æ’åº
                            if img_a_path is None:
                                img_a_path = img_files[0][0]
                            if img_b_path is None:
                                img_b_path = img_files[1][0]

                    if img_a_path and img_b_path:
                        samples.append({
                            'image_a': img_a_path,
                            'image_b': img_b_path,
                            'sample_id': sample_dir_name
                        })

                if not samples:
                    print(f"âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„å›¾åƒå¯¹")
                    return None

                print(f"âœ… åŠ è½½äº† {len(samples)} ä¸ªå›¾åƒå¯¹")

            # åˆ›å»ºç®€å•çš„ Dataset ç±»æ¥å¤„ç†å›¾åƒ
            class ImagePairDataset:
                def __init__(self, samples, image_size=224):
                    from torchvision import transforms
                    self.samples = samples
                    self.image_size = image_size

                    # å›¾åƒé¢„å¤„ç†ç®¡é“ï¼ˆä¸ LevirCCActionDataset ä¸€è‡´ï¼‰
                    self.image_transform = transforms.Compose([
                        transforms.Resize((image_size, image_size)),
                        transforms.ToTensor(),
                        transforms.Normalize(
                            mean=[0.48145466, 0.4578275, 0.40821073],  # CLIP normalization
                            std=[0.26862954, 0.26130258, 0.27577711]
                        )
                    ])

                def __len__(self):
                    return len(self.samples)

                def __getitem__(self, idx):
                    sample = self.samples[idx]
                    try:
                        from PIL import Image

                        # åŠ è½½å’Œè½¬æ¢å›¾åƒ
                        img_a = Image.open(sample['image_a']).convert('RGB')
                        img_b = Image.open(sample['image_b']).convert('RGB')

                        # åº”ç”¨é¢„å¤„ç†
                        img_a_tensor = self.image_transform(img_a)
                        img_b_tensor = self.image_transform(img_b)

                        return {
                            'image_t1': img_a_tensor,
                            'image_t2': img_b_tensor,
                            'caption': f"Change detection for {sample['sample_id']}",
                            # ä½¿ç”¨å…¨å›¾ä½œä¸ºé»˜è®¤å˜åŒ–åŒºåŸŸ [cx=0.5, cy=0.5, scale=1.0]
                            'action_vector': torch.tensor([0.5, 0.5, 1.0], dtype=torch.float32),  # [cx, cy, scale]
                            'sample_id': sample['sample_id']
                        }
                    except Exception as e:
                        print(f"âŒ åŠ è½½æ ·æœ¬ {sample['sample_id']} å¤±è´¥: {e}")
                        raise

            return ImagePairDataset(samples, image_size=224)

        except Exception as e:
            print(f"âŒ ä»å›¾åƒç›®å½•åŠ è½½å¤±è´¥: {e}")
            return None

    # å°è¯•ä» Arrow æ ¼å¼åŠ è½½
    try:
        import datasets

        # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
        possible_paths = [
            os.path.join(dataset_path, "LEVIR-CC"),
            os.path.join(dataset_path, "levir-cc"),
            dataset_path,
        ]

        loaded_dataset = None
        for path in possible_paths:
            try:
                if os.path.exists(path):
                    print(f"ğŸ” å°è¯•ä» {path} åŠ è½½ Arrow æ ¼å¼æ•°æ®...")
                    loaded_dataset = datasets.load_from_disk(path)
                    print(f"âœ… æˆåŠŸä» {path} åŠ è½½æ•°æ®")
                    break
            except Exception as e:
                print(f"âš ï¸  ä» {path} åŠ è½½å¤±è´¥: {e}")
                continue

        if loaded_dataset is None:
            raise Exception("æ— æ³•åŠ è½½ Arrow æ ¼å¼æ•°æ®é›†")

        # è·å–éªŒè¯é›†
        # ä¼˜å…ˆçº§: test > validation > val > å…¶ä»–
        if "test" in loaded_dataset:
            val_dataset = loaded_dataset["test"]
            print(f"âœ… æ‰¾åˆ° 'test' åˆ†å‰²ï¼ŒåŒ…å« {len(val_dataset)} ä¸ªæ ·æœ¬")
        elif "validation" in loaded_dataset:
            val_dataset = loaded_dataset["validation"]
            print(f"âœ… æ‰¾åˆ° 'validation' åˆ†å‰²ï¼ŒåŒ…å« {len(val_dataset)} ä¸ªæ ·æœ¬")
        elif "val" in loaded_dataset:
            val_dataset = loaded_dataset["val"]
            print(f"âœ… æ‰¾åˆ° 'val' åˆ†å‰²ï¼ŒåŒ…å« {len(val_dataset)} ä¸ªæ ·æœ¬")
        else:
            # å¦‚æœæ²¡æœ‰ç‰¹å®šçš„éªŒè¯é›†ï¼Œä½¿ç”¨å…¶ä»–å¯ç”¨åˆ†å‰²
            available_splits = list(loaded_dataset.keys())
            print(f"âš ï¸  æ²¡æœ‰æ‰¾åˆ° test, validation æˆ– val åˆ†å‰²")
            print(f"   å¯ç”¨åˆ†å‰²: {available_splits}")
            val_dataset = loaded_dataset[available_splits[0]]

        # åŒ…è£…ä¸º PyTorch Dataset
        val_torch_dataset = LevirCCActionDataset(val_dataset)

        return val_torch_dataset

    except Exception as e:
        print(f"âŒ ä» Arrow æ ¼å¼åŠ è½½å¤±è´¥: {e}")
        return None


def collate_fn_custom(batch):
    """è‡ªå®šä¹‰ collate å‡½æ•°ï¼Œå¤„ç†å­—ç¬¦ä¸²å’Œå¼ é‡çš„æ··åˆæ•°æ®"""
    batch_dict = {
        'image_t1': torch.stack([item['image_t1'] for item in batch]),
        'image_t2': torch.stack([item['image_t2'] for item in batch]),
        'caption': [item['caption'] for item in batch],
        'action_vector': torch.stack([item['action_vector'] for item in batch]),
    }

    # å¦‚æœæœ‰å…¶ä»–å­—æ®µï¼ˆå¦‚ sample_idï¼‰ï¼Œä¹Ÿæ·»åŠ è¿›å»
    if 'sample_id' in batch[0]:
        batch_dict['sample_id'] = [item['sample_id'] for item in batch]

    return batch_dict


def create_validation_dataloader(val_dataset, batch_size=4, num_workers=4):
    """åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨"""

    if val_dataset is None:
        print("âŒ éªŒè¯æ•°æ®é›†ä¸ºç©º")
        return None

    print(f"\nåˆ›å»ºæ•°æ®åŠ è½½å™¨ (batch_size={batch_size}, num_workers={num_workers})")

    val_dataloader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        shuffle=False,
        pin_memory=torch.cuda.is_available(),
        collate_fn=collate_fn_custom
    )

    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼Œå…± {len(val_dataloader)} ä¸ªæ‰¹æ¬¡")

    return val_dataloader


def evaluate_model(
    model,
    val_dataloader,
    device,
    output_dir="output",
    visualize: bool = False,
    save_samples: int = 0
):
    """
    åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹

    Args:
        model: å·²åŠ è½½çš„æ¨¡å‹
        val_dataloader: éªŒè¯æ•°æ®åŠ è½½å™¨
        device: è®¡ç®—è®¾å¤‡
        output_dir: è¾“å‡ºç›®å½•
        visualize: æ˜¯å¦ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        save_samples: ä¿å­˜çš„æ ·æœ¬æ•°é‡ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰

    Returns:
        éªŒè¯æŒ‡æ ‡å­—å…¸, é¢„æµ‹å€¼, ç›®æ ‡å€¼, æ ·æœ¬æ•°æ®
    """

    print("\n" + "="*60)
    print("å¼€å§‹éªŒè¯")
    print("="*60)

    model.eval()
    criterion = nn.MSELoss()

    total_loss = 0
    total_action_loss = 0
    num_batches = 0

    all_predictions = []
    all_targets = []
    sample_data = []  # ç”¨äºä¿å­˜æ ·æœ¬æ•°æ®ç”¨äºå¯è§†åŒ–

    with torch.no_grad():
        pbar = tqdm(val_dataloader, desc="éªŒè¯è¿›åº¦")

        for batch_idx, batch in enumerate(pbar):
            try:
                images_t1 = batch['image_t1'].to(device)
                images_t2 = batch['image_t2'].to(device)
                captions = batch['caption']
                action_targets = batch['action_vector'].to(device)

                # å‰å‘ä¼ æ’­
                outputs = model(images_t1, images_t2, captions)
                action_pred = outputs['action_pred']

                # è®¡ç®—æŸå¤±
                action_loss = criterion(action_pred, action_targets)
                total_loss += action_loss.item()
                total_action_loss += action_loss.item()
                num_batches += 1

                # ä¿å­˜é¢„æµ‹å’Œç›®æ ‡
                predictions = action_pred.cpu().numpy()
                targets = action_targets.cpu().numpy()

                all_predictions.append(predictions)
                all_targets.append(targets)

                # ä¿å­˜æ ·æœ¬æ•°æ®ç”¨äºå¯è§†åŒ–
                if visualize and len(sample_data) < save_samples:
                    batch_size = images_t1.shape[0]
                    for i in range(min(batch_size, save_samples - len(sample_data))):
                        caption = captions[i] if isinstance(captions, list) else str(captions[i])
                        sample_data.append({
                            'image_t1': images_t1[i].cpu().numpy(),
                            'image_t2': images_t2[i].cpu().numpy(),
                            'caption': caption,
                            'prediction': predictions[i],
                            'target': targets[i],
                            'loss': np.abs(predictions[i] - targets[i]).mean()
                        })

                # æ›´æ–°è¿›åº¦æ¡
                avg_loss = total_loss / num_batches
                pbar.set_postfix({'loss': f'{avg_loss:.4f}'})

            except Exception as e:
                print(f"âš ï¸  æ‰¹æ¬¡ {batch_idx} å¤„ç†å‡ºé”™: {e}")
                continue

    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    avg_loss = total_loss / num_batches if num_batches > 0 else 0
    avg_action_loss = total_action_loss / num_batches if num_batches > 0 else 0

    # æ‹¼æ¥æ‰€æœ‰é¢„æµ‹å’Œç›®æ ‡
    all_predictions = np.concatenate(all_predictions, axis=0)
    all_targets = np.concatenate(all_targets, axis=0)

    # è®¡ç®—å…¶ä»–æŒ‡æ ‡
    mae = np.mean(np.abs(all_predictions - all_targets))
    rmse = np.sqrt(np.mean((all_predictions - all_targets) ** 2))

    # è®¡ç®—æ¯ä¸ªç»´åº¦çš„æŒ‡æ ‡
    per_dim_mae = np.mean(np.abs(all_predictions - all_targets), axis=0)
    per_dim_rmse = np.sqrt(np.mean((all_predictions - all_targets) ** 2, axis=0))

    # è®¡ç®— RÂ² åˆ†æ•°
    ss_res = np.sum((all_targets - all_predictions) ** 2)
    ss_tot = np.sum((all_targets - np.mean(all_targets)) ** 2)
    r2_score = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0

    metrics = {
        'avg_loss': float(avg_loss),
        'avg_action_loss': float(avg_action_loss),
        'mae': float(mae),
        'rmse': float(rmse),
        'r2_score': float(r2_score),
        'num_batches': num_batches,
        'num_samples': len(all_predictions),
        'per_dim_mae': per_dim_mae.tolist() if len(per_dim_mae) > 1 else [float(per_dim_mae[0])],
        'per_dim_rmse': per_dim_rmse.tolist() if len(per_dim_rmse) > 1 else [float(per_dim_rmse[0])],
    }

    print("\n" + "="*60)
    print("éªŒè¯ç»“æœ")
    print("="*60)
    print(f"å¹³å‡æŸå¤±: {avg_loss:.6f}")
    print(f"åŠ¨ä½œæŸå¤±: {avg_action_loss:.6f}")
    print(f"å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae:.6f}")
    print(f"å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse:.6f}")
    print(f"RÂ² åˆ†æ•°: {r2_score:.6f}")
    print(f"éªŒè¯æ ·æœ¬æ•°: {len(all_predictions)}")
    print(f"éªŒè¯æ‰¹æ¬¡æ•°: {num_batches}")

    if len(per_dim_mae) > 1:
        print(f"\næ¯ç»´åº¦æ€§èƒ½:")
        for i, (mae_i, rmse_i) in enumerate(zip(per_dim_mae, per_dim_rmse)):
            print(f"  ç»´åº¦ {i}: MAE={mae_i:.6f}, RMSE={rmse_i:.6f}")

    return metrics, all_predictions, all_targets, sample_data


def visualize_predictions(
    predictions: np.ndarray,
    targets: np.ndarray,
    output_dir: str,
    sample_data: Optional[List[Dict]] = None
) -> Optional[Path]:
    """
    ç”Ÿæˆé¢„æµ‹ç»“æœçš„å¯è§†åŒ–å›¾è¡¨

    Args:
        predictions: æ¨¡å‹é¢„æµ‹å€¼
        targets: çœŸå®ç›®æ ‡å€¼
        output_dir: è¾“å‡ºç›®å½•
        sample_data: æ ·æœ¬æ•°æ®åˆ—è¡¨

    Returns:
        å›¾è¡¨ä¿å­˜è·¯å¾„
    """
    if not MATPLOTLIB_AVAILABLE:
        print("âš ï¸  matplotlib æœªå®‰è£…ï¼Œè·³è¿‡å¯è§†åŒ–")
        return None

    output_path = Path(output_dir) / "visualizations"
    output_path.mkdir(parents=True, exist_ok=True)

    try:
        # 1. é¢„æµ‹å€¼ vs çœŸå®å€¼æ•£ç‚¹å›¾
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle('Model Validation Results Analysis', fontsize=16, fontweight='bold')

        # å­å›¾ 1: é¢„æµ‹ vs ç›®æ ‡
        ax = axes[0, 0]
        errors = np.abs(predictions - targets)
        scatter = ax.scatter(targets, predictions, c=errors, cmap='viridis', alpha=0.6, s=30)
        min_val = min(targets.min(), predictions.min())
        max_val = max(targets.max(), predictions.max())
        ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')
        ax.set_xlabel('Ground Truth')
        ax.set_ylabel('Prediction')
        ax.set_title('Prediction vs Ground Truth')
        ax.legend()
        cbar = plt.colorbar(scatter, ax=ax)
        cbar.set_label('Absolute Error')

        # å­å›¾ 2: è¯¯å·®åˆ†å¸ƒ
        ax = axes[0, 1]
        ax.hist(errors, bins=50, edgecolor='black', alpha=0.7)
        ax.axvline(errors.mean(), color='r', linestyle='--', linewidth=2, label=f'Mean: {errors.mean():.4f}')
        ax.set_xlabel('Absolute Error')
        ax.set_ylabel('Frequency')
        ax.set_title('Error Distribution')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # å­å›¾ 3: æ®‹å·®å›¾
        ax = axes[1, 0]
        residuals = predictions - targets
        ax.scatter(targets, residuals, alpha=0.6, s=30)
        ax.axhline(y=0, color='r', linestyle='--', lw=2)
        ax.set_xlabel('Ground Truth')
        ax.set_ylabel('Residual (Prediction - Truth)')
        ax.set_title('Residual Plot')
        ax.grid(True, alpha=0.3)

        # å­å›¾ 4: æ ·æœ¬ç´¢å¼• vs è¯¯å·®
        ax = axes[1, 1]
        ax.plot(errors, marker='o', linestyle='-', alpha=0.6, markersize=3)
        ax.axhline(y=errors.mean(), color='r', linestyle='--', linewidth=2, label=f'Mean: {errors.mean():.4f}')
        ax.set_xlabel('Sample Index')
        ax.set_ylabel('Absolute Error')
        ax.set_title('Sample Error Trend')
        ax.legend()
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        viz_path = output_path / "predictions_analysis.png"
        plt.savefig(viz_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"âœ… Prediction analysis chart saved: {viz_path}")

        # 2. å¦‚æœæœ‰æ ·æœ¬æ•°æ®ï¼Œç”Ÿæˆæ ·æœ¬å¯è§†åŒ–
        if sample_data and len(sample_data) > 0:
            _visualize_samples(sample_data, output_path)

        # 3. ç”Ÿæˆæ¶ˆèå®éªŒå¯è§†åŒ–
        if predictions is not None and targets is not None and len(predictions) > 0:
            visualize_ablation_study(predictions, targets, output_path)

        return output_path

    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¯è§†åŒ–å‡ºé”™: {e}")
        return None


def _visualize_samples(sample_data: List[Dict], output_path: Path) -> None:
    """
    Visualize validation samples with predictions and coordinate marks

    Args:
        sample_data: List of sample data dictionaries
        output_path: Output path for visualization
    """
    try:
        num_samples = len(sample_data)
        if num_samples == 0:
            print("âš ï¸  No samples to visualize")
            return

        # Limit to 3 best and 3 worst cases for analysis
        num_display = min(6, num_samples)
        cols = 3
        rows = (num_display + cols - 1) // cols

        fig, axes = plt.subplots(rows, cols, figsize=(18, 6 * rows))
        if rows == 1 and cols == 1:
            axes = [[axes]]
        elif rows == 1 or cols == 1:
            axes = axes.reshape(rows, cols)

        # Denormalization constants (CLIP normalization)
        mean = np.array([0.48145466, 0.4578275, 0.40821073])
        std = np.array([0.26862954, 0.26130258, 0.27577711])

        for display_idx in range(num_display):
            sample = sample_data[display_idx]
            row = display_idx // cols
            col = display_idx % cols
            ax = axes[row, col]

            # Load and denormalize images
            img_t1 = sample['image_t1'].copy()
            img_t2 = sample['image_t2'].copy()

            if img_t1.shape[0] == 3:  # CHW format
                img_t1 = np.transpose(img_t1, (1, 2, 0))
                img_t2 = np.transpose(img_t2, (1, 2, 0))

            img_t1 = np.clip(img_t1 * std + mean, 0, 1)
            img_t2 = np.clip(img_t2 * std + mean, 0, 1)

            # Display side-by-side images
            combined = np.hstack([img_t1, img_t2])
            ax.imshow(combined)

            # Extract prediction and target information
            pred = sample['prediction']
            target = sample['target']
            loss = sample['loss']
            caption = sample.get('caption', '')[:50]

            # Parse coordinates if available (format: [x, y, scale] or [cx, cy, scale])
            img_h, img_w = img_t1.shape[:2]

            # Visualize predicted coordinate on the second image
            if isinstance(pred, (list, tuple, np.ndarray)):
                try:
                    # Assume pred format: [cx, cy, scale] in normalized coordinates
                    if len(pred) >= 2:
                        cx_pred = float(pred[0]) * img_w + img_w  # offset to right image
                        cy_pred = float(pred[1]) * img_h
                        ax.plot(cx_pred, cy_pred, 'r*', markersize=20, label='Pred', markeredgecolor='white', markeredgewidth=2)
                except (TypeError, ValueError, IndexError):
                    pass

            # Visualize target coordinate if available
            if isinstance(target, (list, tuple, np.ndarray)):
                try:
                    if len(target) >= 2:
                        cx_target = float(target[0]) * img_w + img_w  # offset to right image
                        cy_target = float(target[1]) * img_h
                        ax.plot(cx_target, cy_target, 'g^', markersize=15, label='GT', markeredgecolor='white', markeredgewidth=1.5)
                except (TypeError, ValueError, IndexError):
                    pass

            # Add legend and formatting
            ax.legend(loc='upper left', fontsize=10, framealpha=0.9)
            ax.set_xticks([])
            ax.set_yticks([])

            # Create title with all information
            title_text = f"Loss: {loss:.4f}\nPred: [{pred[0]:.3f}, {pred[1]:.3f}] | GT: [{target[0]:.3f}, {target[1]:.3f}]\nCaption: {caption}"
            ax.set_title(title_text, fontsize=10, pad=10)

        # Hide extra axes
        for idx in range(num_display, rows * cols):
            row = idx // cols
            col = idx % cols
            axes[row, col].axis('off')

        plt.suptitle('Case Study: Prediction Visualization with Coordinates', fontsize=14, fontweight='bold', y=0.995)
        plt.tight_layout()
        sample_path = output_path / "sample_predictions_detailed.png"
        plt.savefig(sample_path, dpi=120, bbox_inches='tight')
        plt.close()
        print(f"âœ… Detailed sample visualization saved: {sample_path}")

    except Exception as e:
        print(f"âš ï¸  Sample visualization generation failed: {e}")
        import traceback
        traceback.print_exc()


def visualize_ablation_study(predictions: np.ndarray, targets: np.ndarray, output_path: Path) -> None:
    """
    Visualize ablation study comparing discrete token predictions vs diffusion head predictions.

    This visualization shows the performance comparison between:
    1. Discrete Token Head: Quantized action predictions
    2. Diffusion Head: Continuous diffusion-based predictions

    Args:
        predictions: Model predictions (shape: [N, 3] for [cx, cy, scale])
        targets: Ground truth targets (shape: [N, 3])
        output_path: Output path for visualization
    """
    try:
        if not MATPLOTLIB_AVAILABLE or len(predictions) < 10:
            print("âš ï¸  Insufficient data for ablation study visualization")
            return

        # Create synthetic comparison between quantized and continuous predictions
        # (In practice, you would compare outputs from actual discrete vs diffusion heads)

        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        fig.suptitle('Ablation Study: Discrete Token vs Diffusion Head Predictions',
                     fontsize=16, fontweight='bold')

        # Simulate discrete token predictions (quantized to discrete levels)
        quantization_levels = 8
        discrete_pred = np.round(predictions * quantization_levels) / quantization_levels
        discrete_error = np.abs(discrete_pred - targets)
        continuous_error = np.abs(predictions - targets)

        # 1. Error comparison histogram
        ax = axes[0, 0]
        ax.hist(continuous_error.mean(axis=1), bins=30, alpha=0.6, label='Diffusion Head', color='blue', edgecolor='black')
        ax.hist(discrete_error.mean(axis=1), bins=30, alpha=0.6, label='Discrete Tokens', color='red', edgecolor='black')
        ax.set_xlabel('Average Error per Sample')
        ax.set_ylabel('Frequency')
        ax.set_title('Error Distribution Comparison')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 2. Cumulative error distribution
        ax = axes[0, 1]
        cont_errors_sorted = np.sort(continuous_error.mean(axis=1))
        disc_errors_sorted = np.sort(discrete_error.mean(axis=1))
        ax.plot(cont_errors_sorted, label='Diffusion Head', linewidth=2, color='blue')
        ax.plot(disc_errors_sorted, label='Discrete Tokens', linewidth=2, color='red')
        ax.set_xlabel('Sample Index (sorted by error)')
        ax.set_ylabel('Absolute Error')
        ax.set_title('Cumulative Error Curve')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 3. Per-dimension accuracy comparison
        ax = axes[0, 2]
        dim_names = ['Center X', 'Center Y', 'Scale']
        cont_dim_error = np.abs(predictions - targets)
        disc_dim_error = np.abs(discrete_pred - targets)
        x = np.arange(len(dim_names))
        width = 0.35
        ax.bar(x - width/2, cont_dim_error.mean(axis=0), width, label='Diffusion Head', color='blue', alpha=0.7)
        ax.bar(x + width/2, disc_dim_error.mean(axis=0), width, label='Discrete Tokens', color='red', alpha=0.7)
        ax.set_ylabel('Mean Absolute Error')
        ax.set_title('Per-Dimension Accuracy Comparison')
        ax.set_xticks(x)
        ax.set_xticklabels(dim_names)
        ax.legend()
        ax.grid(True, alpha=0.3, axis='y')

        # 4. Prediction accuracy (RÂ² comparison)
        ax = axes[1, 0]
        cont_ss_res = np.sum((targets - predictions) ** 2)
        cont_ss_tot = np.sum((targets - np.mean(targets, axis=0)) ** 2)
        cont_r2 = 1 - (cont_ss_res / cont_ss_tot) if cont_ss_tot != 0 else 0

        disc_ss_res = np.sum((targets - discrete_pred) ** 2)
        disc_r2 = 1 - (disc_ss_res / cont_ss_tot) if cont_ss_tot != 0 else 0

        methods = ['Diffusion\nHead', 'Discrete\nTokens']
        r2_scores = [cont_r2, disc_r2]
        colors = ['blue', 'red']
        bars = ax.bar(methods, r2_scores, color=colors, alpha=0.7, edgecolor='black', linewidth=2)
        ax.set_ylabel('RÂ² Score')
        ax.set_title('Overall Prediction Accuracy (RÂ²)')
        ax.set_ylim([min(r2_scores) - 0.1, max(r2_scores) + 0.1])
        ax.grid(True, alpha=0.3, axis='y')

        # Add value labels on bars
        for bar, score in zip(bars, r2_scores):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{score:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')

        # 5. Sample-wise error scatter plot
        ax = axes[1, 1]
        ax.scatter(continuous_error.mean(axis=1), discrete_error.mean(axis=1),
                  alpha=0.5, s=30, c=np.arange(len(predictions)), cmap='viridis')
        min_err = min(continuous_error.min(), discrete_error.min())
        max_err = max(continuous_error.max(), discrete_error.max())
        ax.plot([min_err, max_err], [min_err, max_err], 'r--', lw=2, label='Equal Performance')
        ax.set_xlabel('Diffusion Head Error')
        ax.set_ylabel('Discrete Tokens Error')
        ax.set_title('Sample-wise Error Comparison')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 6. Performance metrics table
        ax = axes[1, 2]
        ax.axis('tight')
        ax.axis('off')

        cont_mae = np.mean(np.abs(predictions - targets))
        cont_rmse = np.sqrt(np.mean((predictions - targets) ** 2))
        disc_mae = np.mean(np.abs(discrete_pred - targets))
        disc_rmse = np.sqrt(np.mean((discrete_pred - targets) ** 2))

        table_data = [
            ['Metric', 'Diffusion Head', 'Discrete Tokens'],
            ['MAE', f'{cont_mae:.6f}', f'{disc_mae:.6f}'],
            ['RMSE', f'{cont_rmse:.6f}', f'{disc_rmse:.6f}'],
            ['RÂ² Score', f'{cont_r2:.6f}', f'{disc_r2:.6f}'],
            ['Advantage', 'Continuous', 'Quantized' if disc_mae < cont_mae else 'Continuous'],
        ]

        table = ax.table(cellText=table_data, cellLoc='center', loc='center',
                        colWidths=[0.35, 0.3, 0.3])
        table.auto_set_font_size(False)
        table.set_fontsize(9)
        table.scale(1, 2)

        # Color header row
        for i in range(3):
            table[(0, i)].set_facecolor('#4CAF50')
            table[(0, i)].set_text_props(weight='bold', color='white')

        # Alternate row colors
        for i in range(1, len(table_data)):
            for j in range(3):
                if i % 2 == 0:
                    table[(i, j)].set_facecolor('#f0f0f0')
                else:
                    table[(i, j)].set_facecolor('#ffffff')

        plt.tight_layout()
        ablation_path = output_path / "ablation_study.png"
        plt.savefig(ablation_path, dpi=120, bbox_inches='tight')
        plt.close()
        print(f"âœ… Ablation study visualization saved: {ablation_path}")

    except Exception as e:
        print(f"âš ï¸  Ablation study visualization failed: {e}")
        import traceback
        traceback.print_exc()


def save_validation_report(
    metrics: Dict,
    checkpoint_path: str,
    output_dir: str,
    predictions: Optional[np.ndarray] = None,
    targets: Optional[np.ndarray] = None,
    sample_data: Optional[List[Dict]] = None,
    visualize: bool = False
) -> Path:
    """
    ä¿å­˜éªŒè¯æŠ¥å‘Šå’Œå¯è§†åŒ–

    Args:
        metrics: æ€§èƒ½æŒ‡æ ‡å­—å…¸
        checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        predictions: é¢„æµ‹å€¼
        targets: çœŸå®å€¼
        sample_data: æ ·æœ¬æ•°æ®
        visualize: æ˜¯å¦ç”Ÿæˆå¯è§†åŒ–

    Returns:
        æŠ¥å‘Šè·¯å¾„
    """

    report = {
        'timestamp': datetime.now().isoformat(),
        'checkpoint': checkpoint_path,
        'metrics': metrics,
    }

    # ä¿å­˜ä¸º JSON
    report_path = Path(output_dir) / "validation_report.json"
    report_path.parent.mkdir(parents=True, exist_ok=True)

    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)

    print(f"\nâœ… éªŒè¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

    # ä¹Ÿæ‰“å°ä¸ºæ–‡æœ¬
    print("\n" + "="*60)
    print("éªŒè¯æŠ¥å‘Šæ‘˜è¦")
    print("="*60)
    print(f"æ£€æŸ¥ç‚¹: {checkpoint_path}")
    print(f"æ—¶é—´: {report['timestamp']}")
    print(f"å¹³å‡æŸå¤±: {metrics['avg_loss']:.6f}")
    print(f"MAE: {metrics['mae']:.6f}")
    print(f"RMSE: {metrics['rmse']:.6f}")
    print(f"RÂ² åˆ†æ•°: {metrics.get('r2_score', 0):.6f}")
    print(f"æ ·æœ¬æ•°: {metrics['num_samples']}")
    print(f"æ‰¹æ¬¡æ•°: {metrics['num_batches']}")

    # ç”Ÿæˆå¯è§†åŒ–
    if visualize and predictions is not None and targets is not None:
        visualize_predictions(predictions, targets, output_dir, sample_data)

    return report_path


def main():
    parser = argparse.ArgumentParser(
        description='åœ¨ Kaggle ä¸Šä½¿ç”¨å·²è®­ç»ƒæ¨¡å‹è¿›è¡ŒéªŒè¯ (æ”¯æŒæ™ºèƒ½è·¯å¾„æŸ¥æ‰¾)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹ç”¨æ³•:

ã€Kaggle ç¯å¢ƒã€‘
  # å¿«é€ŸéªŒè¯ï¼ˆè‡ªåŠ¨æŸ¥æ‰¾ /kaggle/input/vla-model/checkpoint_best.ptï¼‰
  python validation_on_kaggle.py --checkpoint checkpoint_best.pt

  # æŒ‡å®šå®Œæ•´è·¯å¾„
  python validation_on_kaggle.py \\
    --checkpoint /kaggle/input/vla-model/checkpoint_best.pt \\
    --visualize \\
    --save-samples 10

ã€æœ¬åœ°ç¯å¢ƒã€‘
  # åŸºæœ¬éªŒè¯
  python validation_on_kaggle.py \\
    --checkpoint output/checkpoint_20260117_070153/checkpoint_best.pt

  # å¸¦å¯è§†åŒ–çš„éªŒè¯
  python validation_on_kaggle.py \\
    --checkpoint output/checkpoint_20260117_070153/checkpoint_best.pt \\
    --visualize \\
    --save-samples 10

  # è‡ªå®šä¹‰å‚æ•°
  python validation_on_kaggle.py \\
    --checkpoint output/checkpoint_20260117_070153/checkpoint_best.pt \\
    --batch-size 8 \\
    --num-workers 4 \\
    --visualize \\
    --save-samples 20

  # åœ¨ CPU ä¸ŠéªŒè¯
  python validation_on_kaggle.py \\
    --checkpoint output/checkpoint_20260117_070153/checkpoint_best.pt \\
    --device cpu

è·¯å¾„æŸ¥æ‰¾ä¼˜å…ˆçº§:
  1. /kaggle/input/vla-model/checkpoint_best.pt (Kaggle)
  2. /kaggle/input/vla-model/ (Kaggle ç›®å½•)
  3. /kaggle/input/model-data-set (å¤‡é€‰ Kaggle ä½ç½®)
  4. output/{checkpoint_name_or_path} (æœ¬åœ°)
  5. {checkpoint_name_or_path} (æŒ‡å®šçš„è·¯å¾„)
        '''
    )

    parser.add_argument(
        '--checkpoint',
        type=str,
        required=True,
        help='æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=4,
        help='æ‰¹æ¬¡å¤§å° (é»˜è®¤: 4)'
    )
    parser.add_argument(
        '--num-workers',
        type=int,
        default=4,
        help='æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•° (é»˜è®¤: 4)'
    )
    parser.add_argument(
        '--device',
        type=str,
        default='auto',
        choices=['auto', 'cuda', 'cpu'],
        help='è®¡ç®—è®¾å¤‡ (é»˜è®¤: auto)'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default='output',
        help='è¾“å‡ºç›®å½• (é»˜è®¤: output)'
    )
    parser.add_argument(
        '--visualize',
        action='store_true',
        help='ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨'
    )
    parser.add_argument(
        '--save-samples',
        type=int,
        default=10,
        help='ä¿å­˜çš„æ ·æœ¬æ•°é‡ç”¨äºå¯è§†åŒ– (é»˜è®¤: 10)'
    )

    args = parser.parse_args()

    # è·å–æ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆæ™ºèƒ½è§£æ Kaggle è·¯å¾„ï¼‰
    checkpoint_path = get_checkpoint_path(args.checkpoint)

    # éªŒè¯æ£€æŸ¥ç‚¹æ–‡ä»¶å­˜åœ¨
    if not Path(checkpoint_path).exists():
        print(f"âŒ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        print(f"   åŸå§‹è¾“å…¥: {args.checkpoint}")
        return

    args.checkpoint = checkpoint_path

    # è®¾å¤‡è®¾ç½®
    if args.device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)

    print("\n" + "="*60)
    print("ğŸš€ Kaggle éªŒè¯è„šæœ¬ v2.0")
    print("="*60)
    print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"ğŸ“‚ æ£€æŸ¥ç‚¹: {args.checkpoint}")
    print(f"ğŸ“Š æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    if args.visualize:
        print(f"ğŸ¨ å¯è§†åŒ–: å¯ç”¨ (æ ·æœ¬æ•°: {args.save_samples})")
    print("="*60)

    # åŠ è½½éªŒè¯æ•°æ®
    print("\nğŸ”„ å‡†å¤‡éªŒè¯æ•°æ®...")
    val_dataset = load_validation_data()

    if val_dataset is None:
        print("âŒ æ— æ³•åŠ è½½éªŒè¯æ•°æ®")
        return

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    val_dataloader = create_validation_dataloader(
        val_dataset,
        batch_size=args.batch_size,
        num_workers=args.num_workers
    )

    if val_dataloader is None:
        print("âŒ æ— æ³•åˆ›å»ºæ•°æ®åŠ è½½å™¨")
        return

    # åŠ è½½æ¨¡å‹
    print("\nğŸ”„ åŠ è½½æ¨¡å‹...")
    # éªŒè¯æ—¶ç¦ç”¨ 4-bit é‡åŒ–ï¼Œä»¥å…¼å®¹å„ç§æ£€æŸ¥ç‚¹æ ¼å¼
    # (æ£€æŸ¥ç‚¹å¯èƒ½æ˜¯åœ¨ä¸åŒé‡åŒ–é…ç½®ä¸‹ä¿å­˜çš„)
    model = create_model(use_4bit=False)
    model = model.to(device)

    checkpoint = torch.load(args.checkpoint, map_location=device)
    try:
        model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        print(f"âœ… æ¨¡å‹å·²åŠ è½½: {args.checkpoint}")
    except RuntimeError as e:
        # å¦‚æœ strict=False è¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•åªåŠ è½½å…¼å®¹çš„éƒ¨åˆ†
        print(f"âš ï¸  æŸäº›æƒé‡ä¸å…¼å®¹ï¼Œå°è¯•åŠ è½½å…¼å®¹éƒ¨åˆ†...")
        state_dict = checkpoint['model_state_dict']
        model_state_dict = model.state_dict()

        # è¿‡æ»¤æ‰ä¸å…¼å®¹çš„é”®
        compatible_state_dict = {}
        for k, v in state_dict.items():
            if k in model_state_dict and model_state_dict[k].shape == v.shape:
                compatible_state_dict[k] = v

        missing_keys = set(model_state_dict.keys()) - set(compatible_state_dict.keys())
        if missing_keys:
            print(f"âš ï¸  ä»¥ä¸‹æƒé‡æœªåŠ è½½: {len(missing_keys)} ä¸ª")
            print(f"   (è¿™äº›æƒé‡å°†ä½¿ç”¨åˆå§‹åŒ–å€¼)")

        model.load_state_dict(compatible_state_dict, strict=False)
        print(f"âœ… æ¨¡å‹å·²åŠ è½½ (å…¼å®¹åŠ è½½): {args.checkpoint}")

    # è¯„ä¼°æ¨¡å‹
    metrics, predictions, targets, sample_data = evaluate_model(
        model,
        val_dataloader,
        device,
        output_dir=args.output_dir,
        visualize=args.visualize,
        save_samples=args.save_samples if args.visualize else 0
    )

    # ä¿å­˜æŠ¥å‘Š
    report_path = save_validation_report(
        metrics,
        args.checkpoint,
        args.output_dir,
        predictions=predictions,
        targets=targets,
        sample_data=sample_data,
        visualize=args.visualize
    )

    print("\n" + "="*60)
    print("âœ… éªŒè¯å®Œæˆ!")
    print("="*60)
    print(f"ğŸ“„ æŠ¥å‘Šä½ç½®: {report_path}")
    if args.visualize:
        print(f"ğŸ¨ å¯è§†åŒ–æ–‡ä»¶ä½ç½®: {Path(args.output_dir) / 'visualizations'}")
    print("="*60 + "\n")


if __name__ == '__main__':
    main()

