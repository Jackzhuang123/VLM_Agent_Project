#!/usr/bin/env python
"""
åœ¨ Kaggle ä¸Šä½¿ç”¨å·²è®­ç»ƒæ¨¡å‹è¿›è¡ŒéªŒè¯

åŠŸèƒ½ï¼š
    - åŠ è½½å·²è®­ç»ƒçš„æ£€æŸ¥ç‚¹
    - åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹
    - è®¡ç®—éªŒè¯æŒ‡æ ‡
    - ç”ŸæˆéªŒè¯æŠ¥å‘Š
    - å¯è§†åŒ–éªŒè¯ç»“æœï¼ˆå¯é€‰ï¼‰
    - ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½åˆ†æ

ä½¿ç”¨æ–¹å¼ï¼š
    python validation_on_kaggle.py \
        --checkpoint output/checkpoint_20260117_070153/checkpoint_best.pt \
        --batch-size 4 \
        --num-workers 4 \
        --visualize \
        --save-samples 10
"""

import argparse
import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional, List

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm

try:
    import matplotlib.pyplot as plt
    import matplotlib
    matplotlib.use('Agg')  # Use non-interactive backend
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    print("âš ï¸  matplotlib æœªå®‰è£…ï¼Œè·³è¿‡å¯è§†åŒ–åŠŸèƒ½")

from src.config import Config
from src.dataset import LevirCCActionDataset
from src.model import create_model


def load_validation_data():
    """
    åŠ è½½éªŒè¯æ•°æ®

    æ”¯æŒä¸¤ç§æ•°æ®ç»“æ„ï¼š
    1. Arrow æ ¼å¼ï¼ˆé€šè¿‡ datasets åº“ï¼‰
    2. å›¾åƒç›®å½• + JSON æ ‡æ³¨ï¼ˆæœ¬åœ°ç»“æ„ï¼‰
    """
    print("\n" + "="*60)
    print("åŠ è½½éªŒè¯æ•°æ®")
    print("="*60)

    try:
        import datasets

        # å°è¯•ä» Arrow æ ¼å¼åŠ è½½
        dataset_path = Config.DATASET_PATH

        # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
        possible_paths = [
            os.path.join(dataset_path, "LEVIR-CC"),
            os.path.join(dataset_path, "levir-cc"),
            dataset_path,
        ]

        loaded_dataset = None
        for path in possible_paths:
            try:
                if os.path.exists(path):
                    print(f"ğŸ” å°è¯•ä» {path} åŠ è½½æ•°æ®...")
                    loaded_dataset = datasets.load_from_disk(path)
                    print(f"âœ… æˆåŠŸä» {path} åŠ è½½æ•°æ®")
                    break
            except Exception as e:
                print(f"âš ï¸  ä» {path} åŠ è½½å¤±è´¥: {e}")
                continue

        if loaded_dataset is None:
            raise Exception("æ— æ³•åŠ è½½æ•°æ®é›†")

        # è·å–éªŒè¯é›†
        if "validation" in loaded_dataset:
            val_dataset = loaded_dataset["validation"]
            print(f"âœ… æ‰¾åˆ° 'validation' åˆ†å‰²ï¼ŒåŒ…å« {len(val_dataset)} ä¸ªæ ·æœ¬")
        elif "val" in loaded_dataset:
            val_dataset = loaded_dataset["val"]
            print(f"âœ… æ‰¾åˆ° 'val' åˆ†å‰²ï¼ŒåŒ…å« {len(val_dataset)} ä¸ªæ ·æœ¬")
        else:
            # å¦‚æœæ²¡æœ‰ç‰¹å®šçš„éªŒè¯é›†ï¼Œä½¿ç”¨æµ‹è¯•é›†æˆ–å…¨éƒ¨æ•°æ®
            available_splits = list(loaded_dataset.keys())
            print(f"âš ï¸  æ²¡æœ‰æ‰¾åˆ° validation æˆ– val åˆ†å‰²")
            print(f"   å¯ç”¨åˆ†å‰²: {available_splits}")
            val_dataset = loaded_dataset[available_splits[0]]

        # åŒ…è£…ä¸º PyTorch Dataset
        val_torch_dataset = LevirCCActionDataset(val_dataset)

        return val_torch_dataset

    except Exception as e:
        print(f"âŒ ä» Arrow æ ¼å¼åŠ è½½å¤±è´¥: {e}")
        print("âš ï¸  å°è¯•ä»å›¾åƒç›®å½•åŠ è½½...")

        # å¦‚æœ Arrow åŠ è½½å¤±è´¥ï¼Œè¿”å› Noneï¼Œåç»­å¯ä»¥æ‰‹åŠ¨å¤„ç†
        return None


def create_validation_dataloader(val_dataset, batch_size=4, num_workers=4):
    """åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨"""

    if val_dataset is None:
        print("âŒ éªŒè¯æ•°æ®é›†ä¸ºç©º")
        return None

    print(f"\nåˆ›å»ºæ•°æ®åŠ è½½å™¨ (batch_size={batch_size}, num_workers={num_workers})")

    val_dataloader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        shuffle=False,
        pin_memory=torch.cuda.is_available()
    )

    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼Œå…± {len(val_dataloader)} ä¸ªæ‰¹æ¬¡")

    return val_dataloader


def evaluate_model(
    model,
    val_dataloader,
    device,
    output_dir="output",
    visualize: bool = False,
    save_samples: int = 0
):
    """
    åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹

    Args:
        model: å·²åŠ è½½çš„æ¨¡å‹
        val_dataloader: éªŒè¯æ•°æ®åŠ è½½å™¨
        device: è®¡ç®—è®¾å¤‡
        output_dir: è¾“å‡ºç›®å½•
        visualize: æ˜¯å¦ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        save_samples: ä¿å­˜çš„æ ·æœ¬æ•°é‡ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰

    Returns:
        éªŒè¯æŒ‡æ ‡å­—å…¸, é¢„æµ‹å€¼, ç›®æ ‡å€¼, æ ·æœ¬æ•°æ®
    """

    print("\n" + "="*60)
    print("å¼€å§‹éªŒè¯")
    print("="*60)

    model.eval()
    criterion = nn.MSELoss()

    total_loss = 0
    total_action_loss = 0
    num_batches = 0

    all_predictions = []
    all_targets = []
    sample_data = []  # ç”¨äºä¿å­˜æ ·æœ¬æ•°æ®ç”¨äºå¯è§†åŒ–

    with torch.no_grad():
        pbar = tqdm(val_dataloader, desc="éªŒè¯è¿›åº¦")

        for batch_idx, batch in enumerate(pbar):
            try:
                images_t1 = batch['image_t1'].to(device)
                images_t2 = batch['image_t2'].to(device)
                captions = batch['caption']
                action_targets = batch['action_vector'].to(device)

                # å‰å‘ä¼ æ’­
                outputs = model(images_t1, images_t2, captions)
                action_pred = outputs['action_pred']

                # è®¡ç®—æŸå¤±
                action_loss = criterion(action_pred, action_targets)
                total_loss += action_loss.item()
                total_action_loss += action_loss.item()
                num_batches += 1

                # ä¿å­˜é¢„æµ‹å’Œç›®æ ‡
                predictions = action_pred.cpu().numpy()
                targets = action_targets.cpu().numpy()

                all_predictions.append(predictions)
                all_targets.append(targets)

                # ä¿å­˜æ ·æœ¬æ•°æ®ç”¨äºå¯è§†åŒ–
                if visualize and len(sample_data) < save_samples:
                    batch_size = images_t1.shape[0]
                    for i in range(min(batch_size, save_samples - len(sample_data))):
                        sample_data.append({
                            'image_t1': images_t1[i].cpu().numpy(),
                            'image_t2': images_t2[i].cpu().numpy(),
                            'caption': captions[i] if isinstance(captions, list) else captions[i].item(),
                            'prediction': predictions[i],
                            'target': targets[i],
                            'loss': np.abs(predictions[i] - targets[i]).mean()
                        })

                # æ›´æ–°è¿›åº¦æ¡
                avg_loss = total_loss / num_batches
                pbar.set_postfix({'loss': f'{avg_loss:.4f}'})

            except Exception as e:
                print(f"âš ï¸  æ‰¹æ¬¡ {batch_idx} å¤„ç†å‡ºé”™: {e}")
                continue

    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    avg_loss = total_loss / num_batches if num_batches > 0 else 0
    avg_action_loss = total_action_loss / num_batches if num_batches > 0 else 0

    # æ‹¼æ¥æ‰€æœ‰é¢„æµ‹å’Œç›®æ ‡
    all_predictions = np.concatenate(all_predictions, axis=0)
    all_targets = np.concatenate(all_targets, axis=0)

    # è®¡ç®—å…¶ä»–æŒ‡æ ‡
    mae = np.mean(np.abs(all_predictions - all_targets))
    rmse = np.sqrt(np.mean((all_predictions - all_targets) ** 2))

    # è®¡ç®—æ¯ä¸ªç»´åº¦çš„æŒ‡æ ‡
    per_dim_mae = np.mean(np.abs(all_predictions - all_targets), axis=0)
    per_dim_rmse = np.sqrt(np.mean((all_predictions - all_targets) ** 2, axis=0))

    # è®¡ç®— RÂ² åˆ†æ•°
    ss_res = np.sum((all_targets - all_predictions) ** 2)
    ss_tot = np.sum((all_targets - np.mean(all_targets)) ** 2)
    r2_score = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0

    metrics = {
        'avg_loss': float(avg_loss),
        'avg_action_loss': float(avg_action_loss),
        'mae': float(mae),
        'rmse': float(rmse),
        'r2_score': float(r2_score),
        'num_batches': num_batches,
        'num_samples': len(all_predictions),
        'per_dim_mae': per_dim_mae.tolist() if len(per_dim_mae) > 1 else [float(per_dim_mae[0])],
        'per_dim_rmse': per_dim_rmse.tolist() if len(per_dim_rmse) > 1 else [float(per_dim_rmse[0])],
    }

    print("\n" + "="*60)
    print("éªŒè¯ç»“æœ")
    print("="*60)
    print(f"å¹³å‡æŸå¤±: {avg_loss:.6f}")
    print(f"åŠ¨ä½œæŸå¤±: {avg_action_loss:.6f}")
    print(f"å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae:.6f}")
    print(f"å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse:.6f}")
    print(f"RÂ² åˆ†æ•°: {r2_score:.6f}")
    print(f"éªŒè¯æ ·æœ¬æ•°: {len(all_predictions)}")
    print(f"éªŒè¯æ‰¹æ¬¡æ•°: {num_batches}")

    if len(per_dim_mae) > 1:
        print(f"\næ¯ç»´åº¦æ€§èƒ½:")
        for i, (mae_i, rmse_i) in enumerate(zip(per_dim_mae, per_dim_rmse)):
            print(f"  ç»´åº¦ {i}: MAE={mae_i:.6f}, RMSE={rmse_i:.6f}")

    return metrics, all_predictions, all_targets, sample_data


def visualize_predictions(
    predictions: np.ndarray,
    targets: np.ndarray,
    output_dir: str,
    sample_data: Optional[List[Dict]] = None
) -> Optional[Path]:
    """
    ç”Ÿæˆé¢„æµ‹ç»“æœçš„å¯è§†åŒ–å›¾è¡¨

    Args:
        predictions: æ¨¡å‹é¢„æµ‹å€¼
        targets: çœŸå®ç›®æ ‡å€¼
        output_dir: è¾“å‡ºç›®å½•
        sample_data: æ ·æœ¬æ•°æ®åˆ—è¡¨

    Returns:
        å›¾è¡¨ä¿å­˜è·¯å¾„
    """
    if not MATPLOTLIB_AVAILABLE:
        print("âš ï¸  matplotlib æœªå®‰è£…ï¼Œè·³è¿‡å¯è§†åŒ–")
        return None

    output_path = Path(output_dir) / "visualizations"
    output_path.mkdir(parents=True, exist_ok=True)

    try:
        # 1. é¢„æµ‹å€¼ vs çœŸå®å€¼æ•£ç‚¹å›¾
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle('æ¨¡å‹éªŒè¯ç»“æœå¯è§†åŒ–', fontsize=16, fontweight='bold')

        # å­å›¾ 1: é¢„æµ‹ vs ç›®æ ‡
        ax = axes[0, 0]
        errors = np.abs(predictions - targets)
        scatter = ax.scatter(targets, predictions, c=errors, cmap='viridis', alpha=0.6, s=30)
        min_val = min(targets.min(), predictions.min())
        max_val = max(targets.max(), predictions.max())
        ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='å®Œç¾é¢„æµ‹')
        ax.set_xlabel('çœŸå®å€¼')
        ax.set_ylabel('é¢„æµ‹å€¼')
        ax.set_title('é¢„æµ‹å€¼ vs çœŸå®å€¼')
        ax.legend()
        cbar = plt.colorbar(scatter, ax=ax)
        cbar.set_label('ç»å¯¹è¯¯å·®')

        # å­å›¾ 2: è¯¯å·®åˆ†å¸ƒ
        ax = axes[0, 1]
        ax.hist(errors, bins=50, edgecolor='black', alpha=0.7)
        ax.axvline(errors.mean(), color='r', linestyle='--', linewidth=2, label=f'å¹³å‡: {errors.mean():.4f}')
        ax.set_xlabel('ç»å¯¹è¯¯å·®')
        ax.set_ylabel('é¢‘æ¬¡')
        ax.set_title('è¯¯å·®åˆ†å¸ƒ')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # å­å›¾ 3: æ®‹å·®å›¾
        ax = axes[1, 0]
        residuals = predictions - targets
        ax.scatter(targets, residuals, alpha=0.6, s=30)
        ax.axhline(y=0, color='r', linestyle='--', lw=2)
        ax.set_xlabel('çœŸå®å€¼')
        ax.set_ylabel('æ®‹å·® (é¢„æµ‹ - çœŸå®)')
        ax.set_title('æ®‹å·®å›¾')
        ax.grid(True, alpha=0.3)

        # å­å›¾ 4: æ ·æœ¬ç´¢å¼• vs è¯¯å·®
        ax = axes[1, 1]
        ax.plot(errors, marker='o', linestyle='-', alpha=0.6, markersize=3)
        ax.axhline(y=errors.mean(), color='r', linestyle='--', linewidth=2, label=f'å¹³å‡: {errors.mean():.4f}')
        ax.set_xlabel('æ ·æœ¬ç´¢å¼•')
        ax.set_ylabel('ç»å¯¹è¯¯å·®')
        ax.set_title('æ ·æœ¬è¯¯å·®è¶‹åŠ¿')
        ax.legend()
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        viz_path = output_path / "predictions_analysis.png"
        plt.savefig(viz_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"âœ… é¢„æµ‹åˆ†æå›¾è¡¨å·²ä¿å­˜: {viz_path}")

        # 2. å¦‚æœæœ‰æ ·æœ¬æ•°æ®ï¼Œç”Ÿæˆæ ·æœ¬å¯è§†åŒ–
        if sample_data and len(sample_data) > 0:
            _visualize_samples(sample_data, output_path)

        return output_path

    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¯è§†åŒ–å‡ºé”™: {e}")
        return None


def _visualize_samples(sample_data: List[Dict], output_path: Path) -> None:
    """
    å¯è§†åŒ–éªŒè¯æ ·æœ¬

    Args:
        sample_data: æ ·æœ¬æ•°æ®åˆ—è¡¨
        output_path: è¾“å‡ºè·¯å¾„
    """
    try:
        num_samples = len(sample_data)
        cols = min(3, num_samples)
        rows = (num_samples + cols - 1) // cols

        fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))
        if rows == 1 and cols == 1:
            axes = [[axes]]
        elif rows == 1 or cols == 1:
            axes = axes.reshape(rows, cols)

        for idx, sample in enumerate(sample_data):
            row = idx // cols
            col = idx % cols
            ax = axes[row, col]

            # æ˜¾ç¤ºä¸¤å¼ å›¾åƒ
            img_t1 = sample['image_t1']
            img_t2 = sample['image_t2']

            # åå½’ä¸€åŒ–å›¾åƒï¼ˆCLIP normalizationï¼‰
            mean = np.array([0.48145466, 0.4578275, 0.40821073])
            std = np.array([0.26862954, 0.26130258, 0.27577711])

            if img_t1.shape[0] == 3:  # CHW format
                img_t1 = np.transpose(img_t1, (1, 2, 0))
                img_t2 = np.transpose(img_t2, (1, 2, 0))

            img_t1 = np.clip(img_t1 * std + mean, 0, 1)
            img_t2 = np.clip(img_t2 * std + mean, 0, 1)

            # å¹¶æ’æ˜¾ç¤ºä¸¤å¼ å›¾åƒ
            combined = np.hstack([img_t1, img_t2])
            ax.imshow(combined)
            ax.axis('off')

            # è·å–é¢„æµ‹å’ŒçœŸå®å€¼
            pred = sample['prediction']
            target = sample['target']
            loss = sample['loss']
            caption = sample.get('caption', '')[:30]  # æˆªæ–­æ ‡é¢˜

            # æ·»åŠ æ ‡é¢˜
            title = f"æŸå¤±: {loss:.4f}\né¢„æµ‹: {pred}\nçœŸå®: {target}\næ–‡æœ¬: {caption}"
            ax.set_title(title, fontsize=9)

        # éšè—é¢å¤–çš„è½´
        for idx in range(num_samples, rows * cols):
            row = idx // cols
            col = idx % cols
            axes[row, col].axis('off')

        plt.tight_layout()
        sample_path = output_path / "sample_predictions.png"
        plt.savefig(sample_path, dpi=100, bbox_inches='tight')
        plt.close()
        print(f"âœ… æ ·æœ¬å¯è§†åŒ–å·²ä¿å­˜: {sample_path}")

    except Exception as e:
        print(f"âš ï¸  æ ·æœ¬å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")


def save_validation_report(
    metrics: Dict,
    checkpoint_path: str,
    output_dir: str,
    predictions: Optional[np.ndarray] = None,
    targets: Optional[np.ndarray] = None,
    sample_data: Optional[List[Dict]] = None,
    visualize: bool = False
) -> Path:
    """
    ä¿å­˜éªŒè¯æŠ¥å‘Šå’Œå¯è§†åŒ–

    Args:
        metrics: æ€§èƒ½æŒ‡æ ‡å­—å…¸
        checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        predictions: é¢„æµ‹å€¼
        targets: çœŸå®å€¼
        sample_data: æ ·æœ¬æ•°æ®
        visualize: æ˜¯å¦ç”Ÿæˆå¯è§†åŒ–

    Returns:
        æŠ¥å‘Šè·¯å¾„
    """

    report = {
        'timestamp': datetime.now().isoformat(),
        'checkpoint': checkpoint_path,
        'metrics': metrics,
    }

    # ä¿å­˜ä¸º JSON
    report_path = Path(output_dir) / "validation_report.json"
    report_path.parent.mkdir(parents=True, exist_ok=True)

    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)

    print(f"\nâœ… éªŒè¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

    # ä¹Ÿæ‰“å°ä¸ºæ–‡æœ¬
    print("\n" + "="*60)
    print("éªŒè¯æŠ¥å‘Šæ‘˜è¦")
    print("="*60)
    print(f"æ£€æŸ¥ç‚¹: {checkpoint_path}")
    print(f"æ—¶é—´: {report['timestamp']}")
    print(f"å¹³å‡æŸå¤±: {metrics['avg_loss']:.6f}")
    print(f"MAE: {metrics['mae']:.6f}")
    print(f"RMSE: {metrics['rmse']:.6f}")
    print(f"RÂ² åˆ†æ•°: {metrics.get('r2_score', 0):.6f}")
    print(f"æ ·æœ¬æ•°: {metrics['num_samples']}")
    print(f"æ‰¹æ¬¡æ•°: {metrics['num_batches']}")

    # ç”Ÿæˆå¯è§†åŒ–
    if visualize and predictions is not None and targets is not None:
        visualize_predictions(predictions, targets, output_dir, sample_data)

    return report_path


def main():
    parser = argparse.ArgumentParser(
        description='åœ¨ Kaggle ä¸Šä½¿ç”¨å·²è®­ç»ƒæ¨¡å‹è¿›è¡ŒéªŒè¯',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹ç”¨æ³•:
  # åŸºæœ¬éªŒè¯
  python validation_on_kaggle.py \\
    --checkpoint output/checkpoint_20260117_070153/checkpoint_best.pt

  # å¸¦å¯è§†åŒ–çš„éªŒè¯
  python validation_on_kaggle.py \\
    --checkpoint output/checkpoint_20260117_070153/checkpoint_best.pt \\
    --visualize \\
    --save-samples 10

  # è‡ªå®šä¹‰å‚æ•°
  python validation_on_kaggle.py \\
    --checkpoint output/checkpoint_20260117_070153/checkpoint_best.pt \\
    --batch-size 8 \\
    --num-workers 4 \\
    --visualize \\
    --save-samples 20

  # åœ¨ CPU ä¸ŠéªŒè¯
  python validation_on_kaggle.py \\
    --checkpoint output/checkpoint_20260117_070153/checkpoint_best.pt \\
    --device cpu
        '''
    )

    parser.add_argument(
        '--checkpoint',
        type=str,
        required=True,
        help='æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=4,
        help='æ‰¹æ¬¡å¤§å° (é»˜è®¤: 4)'
    )
    parser.add_argument(
        '--num-workers',
        type=int,
        default=4,
        help='æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•° (é»˜è®¤: 4)'
    )
    parser.add_argument(
        '--device',
        type=str,
        default='auto',
        choices=['auto', 'cuda', 'cpu'],
        help='è®¡ç®—è®¾å¤‡ (é»˜è®¤: auto)'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default='output',
        help='è¾“å‡ºç›®å½• (é»˜è®¤: output)'
    )
    parser.add_argument(
        '--visualize',
        action='store_true',
        help='ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨'
    )
    parser.add_argument(
        '--save-samples',
        type=int,
        default=10,
        help='ä¿å­˜çš„æ ·æœ¬æ•°é‡ç”¨äºå¯è§†åŒ– (é»˜è®¤: 10)'
    )

    args = parser.parse_args()

    # éªŒè¯æ£€æŸ¥ç‚¹æ–‡ä»¶å­˜åœ¨
    if not Path(args.checkpoint).exists():
        print(f"âŒ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {args.checkpoint}")
        return

    # è®¾å¤‡è®¾ç½®
    if args.device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)

    print("\n" + "="*60)
    print("ğŸš€ Kaggle éªŒè¯è„šæœ¬ v2.0")
    print("="*60)
    print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"ğŸ“‚ æ£€æŸ¥ç‚¹: {args.checkpoint}")
    print(f"ğŸ“Š æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    if args.visualize:
        print(f"ğŸ¨ å¯è§†åŒ–: å¯ç”¨ (æ ·æœ¬æ•°: {args.save_samples})")
    print("="*60)

    # åŠ è½½éªŒè¯æ•°æ®
    print("\nğŸ”„ å‡†å¤‡éªŒè¯æ•°æ®...")
    val_dataset = load_validation_data()

    if val_dataset is None:
        print("âŒ æ— æ³•åŠ è½½éªŒè¯æ•°æ®")
        return

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    val_dataloader = create_validation_dataloader(
        val_dataset,
        batch_size=args.batch_size,
        num_workers=args.num_workers
    )

    if val_dataloader is None:
        print("âŒ æ— æ³•åˆ›å»ºæ•°æ®åŠ è½½å™¨")
        return

    # åŠ è½½æ¨¡å‹
    print("\nğŸ”„ åŠ è½½æ¨¡å‹...")
    model = create_model()
    model = model.to(device)

    checkpoint = torch.load(args.checkpoint, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    print(f"âœ… æ¨¡å‹å·²åŠ è½½: {args.checkpoint}")

    # è¯„ä¼°æ¨¡å‹
    metrics, predictions, targets, sample_data = evaluate_model(
        model,
        val_dataloader,
        device,
        output_dir=args.output_dir,
        visualize=args.visualize,
        save_samples=args.save_samples if args.visualize else 0
    )

    # ä¿å­˜æŠ¥å‘Š
    report_path = save_validation_report(
        metrics,
        args.checkpoint,
        args.output_dir,
        predictions=predictions,
        targets=targets,
        sample_data=sample_data,
        visualize=args.visualize
    )

    print("\n" + "="*60)
    print("âœ… éªŒè¯å®Œæˆ!")
    print("="*60)
    print(f"ğŸ“„ æŠ¥å‘Šä½ç½®: {report_path}")
    if args.visualize:
        print(f"ğŸ¨ å¯è§†åŒ–æ–‡ä»¶ä½ç½®: {Path(args.output_dir) / 'visualizations'}")
    print("="*60 + "\n")


if __name__ == '__main__':
    main()

